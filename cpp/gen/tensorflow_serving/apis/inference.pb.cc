// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: tensorflow_serving/apis/inference.proto

#define INTERNAL_SUPPRESS_PROTOBUF_FIELD_DEPRECATION
#include "tensorflow_serving/apis/inference.pb.h"

#include <algorithm>

#include <google/protobuf/stubs/common.h>
#include <google/protobuf/stubs/port.h>
#include <google/protobuf/stubs/once.h>
#include <google/protobuf/io/coded_stream.h>
#include <google/protobuf/wire_format_lite_inl.h>
#include <google/protobuf/descriptor.h>
#include <google/protobuf/generated_message_reflection.h>
#include <google/protobuf/reflection_ops.h>
#include <google/protobuf/wire_format.h>
// @@protoc_insertion_point(includes)

namespace tensorflow {
namespace serving {

namespace {

const ::google::protobuf::Descriptor* InferenceTask_descriptor_ = NULL;
const ::google::protobuf::internal::GeneratedMessageReflection*
  InferenceTask_reflection_ = NULL;
const ::google::protobuf::Descriptor* InferenceResult_descriptor_ = NULL;
const ::google::protobuf::internal::GeneratedMessageReflection*
  InferenceResult_reflection_ = NULL;
struct InferenceResultOneofInstance {
  const ::tensorflow::serving::ClassificationResult* classification_result_;
  const ::tensorflow::serving::RegressionResult* regression_result_;
}* InferenceResult_default_oneof_instance_ = NULL;
const ::google::protobuf::Descriptor* MultiInferenceRequest_descriptor_ = NULL;
const ::google::protobuf::internal::GeneratedMessageReflection*
  MultiInferenceRequest_reflection_ = NULL;
const ::google::protobuf::Descriptor* MultiInferenceResponse_descriptor_ = NULL;
const ::google::protobuf::internal::GeneratedMessageReflection*
  MultiInferenceResponse_reflection_ = NULL;

}  // namespace


void protobuf_AssignDesc_tensorflow_5fserving_2fapis_2finference_2eproto() GOOGLE_ATTRIBUTE_COLD;
void protobuf_AssignDesc_tensorflow_5fserving_2fapis_2finference_2eproto() {
  protobuf_AddDesc_tensorflow_5fserving_2fapis_2finference_2eproto();
  const ::google::protobuf::FileDescriptor* file =
    ::google::protobuf::DescriptorPool::generated_pool()->FindFileByName(
      "tensorflow_serving/apis/inference.proto");
  GOOGLE_CHECK(file != NULL);
  InferenceTask_descriptor_ = file->message_type(0);
  static const int InferenceTask_offsets_[2] = {
    GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(InferenceTask, model_spec_),
    GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(InferenceTask, method_name_),
  };
  InferenceTask_reflection_ =
    ::google::protobuf::internal::GeneratedMessageReflection::NewGeneratedMessageReflection(
      InferenceTask_descriptor_,
      InferenceTask::default_instance_,
      InferenceTask_offsets_,
      -1,
      -1,
      -1,
      sizeof(InferenceTask),
      GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(InferenceTask, _internal_metadata_),
      GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(InferenceTask, _is_default_instance_));
  InferenceResult_descriptor_ = file->message_type(1);
  static const int InferenceResult_offsets_[4] = {
    GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(InferenceResult, model_spec_),
    PROTO2_GENERATED_DEFAULT_ONEOF_FIELD_OFFSET(InferenceResult_default_oneof_instance_, classification_result_),
    PROTO2_GENERATED_DEFAULT_ONEOF_FIELD_OFFSET(InferenceResult_default_oneof_instance_, regression_result_),
    GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(InferenceResult, result_),
  };
  InferenceResult_reflection_ =
    ::google::protobuf::internal::GeneratedMessageReflection::NewGeneratedMessageReflection(
      InferenceResult_descriptor_,
      InferenceResult::default_instance_,
      InferenceResult_offsets_,
      -1,
      -1,
      -1,
      InferenceResult_default_oneof_instance_,
      GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(InferenceResult, _oneof_case_[0]),
      sizeof(InferenceResult),
      GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(InferenceResult, _internal_metadata_),
      GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(InferenceResult, _is_default_instance_));
  MultiInferenceRequest_descriptor_ = file->message_type(2);
  static const int MultiInferenceRequest_offsets_[2] = {
    GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(MultiInferenceRequest, tasks_),
    GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(MultiInferenceRequest, input_),
  };
  MultiInferenceRequest_reflection_ =
    ::google::protobuf::internal::GeneratedMessageReflection::NewGeneratedMessageReflection(
      MultiInferenceRequest_descriptor_,
      MultiInferenceRequest::default_instance_,
      MultiInferenceRequest_offsets_,
      -1,
      -1,
      -1,
      sizeof(MultiInferenceRequest),
      GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(MultiInferenceRequest, _internal_metadata_),
      GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(MultiInferenceRequest, _is_default_instance_));
  MultiInferenceResponse_descriptor_ = file->message_type(3);
  static const int MultiInferenceResponse_offsets_[1] = {
    GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(MultiInferenceResponse, results_),
  };
  MultiInferenceResponse_reflection_ =
    ::google::protobuf::internal::GeneratedMessageReflection::NewGeneratedMessageReflection(
      MultiInferenceResponse_descriptor_,
      MultiInferenceResponse::default_instance_,
      MultiInferenceResponse_offsets_,
      -1,
      -1,
      -1,
      sizeof(MultiInferenceResponse),
      GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(MultiInferenceResponse, _internal_metadata_),
      GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(MultiInferenceResponse, _is_default_instance_));
}

namespace {

GOOGLE_PROTOBUF_DECLARE_ONCE(protobuf_AssignDescriptors_once_);
inline void protobuf_AssignDescriptorsOnce() {
  ::google::protobuf::GoogleOnceInit(&protobuf_AssignDescriptors_once_,
                 &protobuf_AssignDesc_tensorflow_5fserving_2fapis_2finference_2eproto);
}

void protobuf_RegisterTypes(const ::std::string&) GOOGLE_ATTRIBUTE_COLD;
void protobuf_RegisterTypes(const ::std::string&) {
  protobuf_AssignDescriptorsOnce();
  ::google::protobuf::MessageFactory::InternalRegisterGeneratedMessage(
      InferenceTask_descriptor_, &InferenceTask::default_instance());
  ::google::protobuf::MessageFactory::InternalRegisterGeneratedMessage(
      InferenceResult_descriptor_, &InferenceResult::default_instance());
  ::google::protobuf::MessageFactory::InternalRegisterGeneratedMessage(
      MultiInferenceRequest_descriptor_, &MultiInferenceRequest::default_instance());
  ::google::protobuf::MessageFactory::InternalRegisterGeneratedMessage(
      MultiInferenceResponse_descriptor_, &MultiInferenceResponse::default_instance());
}

}  // namespace

void protobuf_ShutdownFile_tensorflow_5fserving_2fapis_2finference_2eproto() {
  delete InferenceTask::default_instance_;
  delete InferenceTask_reflection_;
  delete InferenceResult::default_instance_;
  delete InferenceResult_default_oneof_instance_;
  delete InferenceResult_reflection_;
  delete MultiInferenceRequest::default_instance_;
  delete MultiInferenceRequest_reflection_;
  delete MultiInferenceResponse::default_instance_;
  delete MultiInferenceResponse_reflection_;
}

void protobuf_AddDesc_tensorflow_5fserving_2fapis_2finference_2eproto() GOOGLE_ATTRIBUTE_COLD;
void protobuf_AddDesc_tensorflow_5fserving_2fapis_2finference_2eproto() {
  static bool already_here = false;
  if (already_here) return;
  already_here = true;
  GOOGLE_PROTOBUF_VERIFY_VERSION;

  ::tensorflow::serving::protobuf_AddDesc_tensorflow_5fserving_2fapis_2fclassification_2eproto();
  ::tensorflow::serving::protobuf_AddDesc_tensorflow_5fserving_2fapis_2finput_2eproto();
  ::tensorflow::serving::protobuf_AddDesc_tensorflow_5fserving_2fapis_2fmodel_2eproto();
  ::tensorflow::serving::protobuf_AddDesc_tensorflow_5fserving_2fapis_2fregression_2eproto();
  ::google::protobuf::DescriptorPool::InternalAddGeneratedFile(
    "\n\'tensorflow_serving/apis/inference.prot"
    "o\022\022tensorflow.serving\032,tensorflow_servin"
    "g/apis/classification.proto\032#tensorflow_"
    "serving/apis/input.proto\032#tensorflow_ser"
    "ving/apis/model.proto\032(tensorflow_servin"
    "g/apis/regression.proto\"W\n\rInferenceTask"
    "\0221\n\nmodel_spec\030\001 \001(\0132\035.tensorflow.servin"
    "g.ModelSpec\022\023\n\013method_name\030\002 \001(\t\"\334\001\n\017Inf"
    "erenceResult\0221\n\nmodel_spec\030\001 \001(\0132\035.tenso"
    "rflow.serving.ModelSpec\022I\n\025classificatio"
    "n_result\030\002 \001(\0132(.tensorflow.serving.Clas"
    "sificationResultH\000\022A\n\021regression_result\030"
    "\003 \001(\0132$.tensorflow.serving.RegressionRes"
    "ultH\000B\010\n\006result\"s\n\025MultiInferenceRequest"
    "\0220\n\005tasks\030\001 \003(\0132!.tensorflow.serving.Inf"
    "erenceTask\022(\n\005input\030\002 \001(\0132\031.tensorflow.s"
    "erving.Input\"N\n\026MultiInferenceResponse\0224"
    "\n\007results\030\001 \003(\0132#.tensorflow.serving.Inf"
    "erenceResultB\003\370\001\001b\006proto3", 745);
  ::google::protobuf::MessageFactory::InternalRegisterGeneratedFile(
    "tensorflow_serving/apis/inference.proto", &protobuf_RegisterTypes);
  InferenceTask::default_instance_ = new InferenceTask();
  InferenceResult::default_instance_ = new InferenceResult();
  InferenceResult_default_oneof_instance_ = new InferenceResultOneofInstance();
  MultiInferenceRequest::default_instance_ = new MultiInferenceRequest();
  MultiInferenceResponse::default_instance_ = new MultiInferenceResponse();
  InferenceTask::default_instance_->InitAsDefaultInstance();
  InferenceResult::default_instance_->InitAsDefaultInstance();
  MultiInferenceRequest::default_instance_->InitAsDefaultInstance();
  MultiInferenceResponse::default_instance_->InitAsDefaultInstance();
  ::google::protobuf::internal::OnShutdown(&protobuf_ShutdownFile_tensorflow_5fserving_2fapis_2finference_2eproto);
}

// Force AddDescriptors() to be called at static initialization time.
struct StaticDescriptorInitializer_tensorflow_5fserving_2fapis_2finference_2eproto {
  StaticDescriptorInitializer_tensorflow_5fserving_2fapis_2finference_2eproto() {
    protobuf_AddDesc_tensorflow_5fserving_2fapis_2finference_2eproto();
  }
} static_descriptor_initializer_tensorflow_5fserving_2fapis_2finference_2eproto_;

// ===================================================================

void InferenceTask::_slow_mutable_model_spec() {
  model_spec_ = ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::ModelSpec >(
      GetArenaNoVirtual());
}
::tensorflow::serving::ModelSpec* InferenceTask::_slow_release_model_spec() {
  if (model_spec_ == NULL) {
    return NULL;
  } else {
    ::tensorflow::serving::ModelSpec* temp = new ::tensorflow::serving::ModelSpec;
    temp->MergeFrom(*model_spec_);
    model_spec_ = NULL;
    return temp;
  }
}
::tensorflow::serving::ModelSpec* InferenceTask::unsafe_arena_release_model_spec() {
  // @@protoc_insertion_point(field_unsafe_arena_release:tensorflow.serving.InferenceTask.model_spec)
  
  ::tensorflow::serving::ModelSpec* temp = model_spec_;
  model_spec_ = NULL;
  return temp;
}
void InferenceTask::_slow_set_allocated_model_spec(
    ::google::protobuf::Arena* message_arena, ::tensorflow::serving::ModelSpec** model_spec) {
    if (message_arena != NULL && 
        ::google::protobuf::Arena::GetArena(*model_spec) == NULL) {
      message_arena->Own(*model_spec);
    } else if (message_arena !=
               ::google::protobuf::Arena::GetArena(*model_spec)) {
      ::tensorflow::serving::ModelSpec* new_model_spec = 
            ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::ModelSpec >(
            message_arena);
      new_model_spec->CopyFrom(**model_spec);
      *model_spec = new_model_spec;
    }
}
void InferenceTask::unsafe_arena_set_allocated_model_spec(
    ::tensorflow::serving::ModelSpec* model_spec) {
  if (GetArenaNoVirtual() == NULL) {
    delete model_spec_;
  }
  model_spec_ = model_spec;
  if (model_spec) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:tensorflow.serving.InferenceTask.model_spec)
}
#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int InferenceTask::kModelSpecFieldNumber;
const int InferenceTask::kMethodNameFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

InferenceTask::InferenceTask()
  : ::google::protobuf::Message(), _internal_metadata_(NULL) {
  SharedCtor();
  // @@protoc_insertion_point(constructor:tensorflow.serving.InferenceTask)
}

InferenceTask::InferenceTask(::google::protobuf::Arena* arena)
  : ::google::protobuf::Message(),
  _internal_metadata_(arena) {
  SharedCtor();
  RegisterArenaDtor(arena);
  // @@protoc_insertion_point(arena_constructor:tensorflow.serving.InferenceTask)
}

void InferenceTask::InitAsDefaultInstance() {
  _is_default_instance_ = true;
  model_spec_ = const_cast< ::tensorflow::serving::ModelSpec*>(&::tensorflow::serving::ModelSpec::default_instance());
}

InferenceTask::InferenceTask(const InferenceTask& from)
  : ::google::protobuf::Message(),
    _internal_metadata_(NULL) {
  SharedCtor();
  MergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:tensorflow.serving.InferenceTask)
}

void InferenceTask::SharedCtor() {
    _is_default_instance_ = false;
  ::google::protobuf::internal::GetEmptyString();
  _cached_size_ = 0;
  model_spec_ = NULL;
  method_name_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}

InferenceTask::~InferenceTask() {
  // @@protoc_insertion_point(destructor:tensorflow.serving.InferenceTask)
  SharedDtor();
}

void InferenceTask::SharedDtor() {
  if (GetArenaNoVirtual() != NULL) {
    return;
  }

  method_name_.Destroy(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), GetArenaNoVirtual());
  if (this != default_instance_) {
    delete model_spec_;
  }
}

void InferenceTask::ArenaDtor(void* object) {
  InferenceTask* _this = reinterpret_cast< InferenceTask* >(object);
  (void)_this;
}
void InferenceTask::RegisterArenaDtor(::google::protobuf::Arena* arena) {
}
void InferenceTask::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ::google::protobuf::Descriptor* InferenceTask::descriptor() {
  protobuf_AssignDescriptorsOnce();
  return InferenceTask_descriptor_;
}

const InferenceTask& InferenceTask::default_instance() {
  if (default_instance_ == NULL) protobuf_AddDesc_tensorflow_5fserving_2fapis_2finference_2eproto();
  return *default_instance_;
}

InferenceTask* InferenceTask::default_instance_ = NULL;

InferenceTask* InferenceTask::New(::google::protobuf::Arena* arena) const {
  return ::google::protobuf::Arena::CreateMessage<InferenceTask>(arena);
}

void InferenceTask::Clear() {
// @@protoc_insertion_point(message_clear_start:tensorflow.serving.InferenceTask)
  if (GetArenaNoVirtual() == NULL && model_spec_ != NULL) delete model_spec_;
  model_spec_ = NULL;
  method_name_.ClearToEmpty(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), GetArenaNoVirtual());
}

bool InferenceTask::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:tensorflow.serving.InferenceTask)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // optional .tensorflow.serving.ModelSpec model_spec = 1;
      case 1: {
        if (tag == 10) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_model_spec()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(18)) goto parse_method_name;
        break;
      }

      // optional string method_name = 2;
      case 2: {
        if (tag == 18) {
         parse_method_name:
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->mutable_method_name()));
          DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
            this->method_name().data(), this->method_name().length(),
            ::google::protobuf::internal::WireFormatLite::PARSE,
            "tensorflow.serving.InferenceTask.method_name"));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:tensorflow.serving.InferenceTask)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:tensorflow.serving.InferenceTask)
  return false;
#undef DO_
}

void InferenceTask::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:tensorflow.serving.InferenceTask)
  // optional .tensorflow.serving.ModelSpec model_spec = 1;
  if (this->has_model_spec()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      1, *this->model_spec_, output);
  }

  // optional string method_name = 2;
  if (this->method_name().size() > 0) {
    ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
      this->method_name().data(), this->method_name().length(),
      ::google::protobuf::internal::WireFormatLite::SERIALIZE,
      "tensorflow.serving.InferenceTask.method_name");
    ::google::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(
      2, this->method_name(), output);
  }

  // @@protoc_insertion_point(serialize_end:tensorflow.serving.InferenceTask)
}

::google::protobuf::uint8* InferenceTask::InternalSerializeWithCachedSizesToArray(
    bool deterministic, ::google::protobuf::uint8* target) const {
  // @@protoc_insertion_point(serialize_to_array_start:tensorflow.serving.InferenceTask)
  // optional .tensorflow.serving.ModelSpec model_spec = 1;
  if (this->has_model_spec()) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageNoVirtualToArray(
        1, *this->model_spec_, false, target);
  }

  // optional string method_name = 2;
  if (this->method_name().size() > 0) {
    ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
      this->method_name().data(), this->method_name().length(),
      ::google::protobuf::internal::WireFormatLite::SERIALIZE,
      "tensorflow.serving.InferenceTask.method_name");
    target =
      ::google::protobuf::internal::WireFormatLite::WriteStringToArray(
        2, this->method_name(), target);
  }

  // @@protoc_insertion_point(serialize_to_array_end:tensorflow.serving.InferenceTask)
  return target;
}

int InferenceTask::ByteSize() const {
// @@protoc_insertion_point(message_byte_size_start:tensorflow.serving.InferenceTask)
  int total_size = 0;

  // optional .tensorflow.serving.ModelSpec model_spec = 1;
  if (this->has_model_spec()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->model_spec_);
  }

  // optional string method_name = 2;
  if (this->method_name().size() > 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::StringSize(
        this->method_name());
  }

  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = total_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void InferenceTask::MergeFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_merge_from_start:tensorflow.serving.InferenceTask)
  if (GOOGLE_PREDICT_FALSE(&from == this)) {
    ::google::protobuf::internal::MergeFromFail(__FILE__, __LINE__);
  }
  const InferenceTask* source = 
      ::google::protobuf::internal::DynamicCastToGenerated<const InferenceTask>(
          &from);
  if (source == NULL) {
  // @@protoc_insertion_point(generalized_merge_from_cast_fail:tensorflow.serving.InferenceTask)
    ::google::protobuf::internal::ReflectionOps::Merge(from, this);
  } else {
  // @@protoc_insertion_point(generalized_merge_from_cast_success:tensorflow.serving.InferenceTask)
    MergeFrom(*source);
  }
}

void InferenceTask::MergeFrom(const InferenceTask& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:tensorflow.serving.InferenceTask)
  if (GOOGLE_PREDICT_FALSE(&from == this)) {
    ::google::protobuf::internal::MergeFromFail(__FILE__, __LINE__);
  }
  if (from.has_model_spec()) {
    mutable_model_spec()->::tensorflow::serving::ModelSpec::MergeFrom(from.model_spec());
  }
  if (from.method_name().size() > 0) {
    set_method_name(from.method_name());
  }
}

void InferenceTask::CopyFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_copy_from_start:tensorflow.serving.InferenceTask)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

void InferenceTask::CopyFrom(const InferenceTask& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:tensorflow.serving.InferenceTask)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool InferenceTask::IsInitialized() const {

  return true;
}

void InferenceTask::Swap(InferenceTask* other) {
  if (other == this) return;
  if (GetArenaNoVirtual() == other->GetArenaNoVirtual()) {
    InternalSwap(other);
  } else {
    InferenceTask temp;
    temp.MergeFrom(*this);
    CopyFrom(*other);
    other->CopyFrom(temp);
  }
}
void InferenceTask::UnsafeArenaSwap(InferenceTask* other) {
  if (other == this) return;
  GOOGLE_DCHECK(GetArenaNoVirtual() == other->GetArenaNoVirtual());
  InternalSwap(other);
}
void InferenceTask::InternalSwap(InferenceTask* other) {
  std::swap(model_spec_, other->model_spec_);
  method_name_.Swap(&other->method_name_);
  _internal_metadata_.Swap(&other->_internal_metadata_);
  std::swap(_cached_size_, other->_cached_size_);
}

::google::protobuf::Metadata InferenceTask::GetMetadata() const {
  protobuf_AssignDescriptorsOnce();
  ::google::protobuf::Metadata metadata;
  metadata.descriptor = InferenceTask_descriptor_;
  metadata.reflection = InferenceTask_reflection_;
  return metadata;
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// InferenceTask

// optional .tensorflow.serving.ModelSpec model_spec = 1;
bool InferenceTask::has_model_spec() const {
  return !_is_default_instance_ && model_spec_ != NULL;
}
void InferenceTask::clear_model_spec() {
  if (GetArenaNoVirtual() == NULL && model_spec_ != NULL) delete model_spec_;
  model_spec_ = NULL;
}
const ::tensorflow::serving::ModelSpec& InferenceTask::model_spec() const {
  // @@protoc_insertion_point(field_get:tensorflow.serving.InferenceTask.model_spec)
  return model_spec_ != NULL ? *model_spec_ : *default_instance_->model_spec_;
}
::tensorflow::serving::ModelSpec* InferenceTask::mutable_model_spec() {
  
  if (model_spec_ == NULL) {
    _slow_mutable_model_spec();
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.serving.InferenceTask.model_spec)
  return model_spec_;
}
::tensorflow::serving::ModelSpec* InferenceTask::release_model_spec() {
  // @@protoc_insertion_point(field_release:tensorflow.serving.InferenceTask.model_spec)
  
  if (GetArenaNoVirtual() != NULL) {
    return _slow_release_model_spec();
  } else {
    ::tensorflow::serving::ModelSpec* temp = model_spec_;
    model_spec_ = NULL;
    return temp;
  }
}
 void InferenceTask::set_allocated_model_spec(::tensorflow::serving::ModelSpec* model_spec) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete model_spec_;
  }
  if (model_spec != NULL) {
    _slow_set_allocated_model_spec(message_arena, &model_spec);
  }
  model_spec_ = model_spec;
  if (model_spec) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:tensorflow.serving.InferenceTask.model_spec)
}

// optional string method_name = 2;
void InferenceTask::clear_method_name() {
  method_name_.ClearToEmpty(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), GetArenaNoVirtual());
}
 const ::std::string& InferenceTask::method_name() const {
  // @@protoc_insertion_point(field_get:tensorflow.serving.InferenceTask.method_name)
  return method_name_.Get(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
 void InferenceTask::set_method_name(const ::std::string& value) {
  
  method_name_.Set(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value, GetArenaNoVirtual());
  // @@protoc_insertion_point(field_set:tensorflow.serving.InferenceTask.method_name)
}
 void InferenceTask::set_method_name(const char* value) {
  
  method_name_.Set(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value),
              GetArenaNoVirtual());
  // @@protoc_insertion_point(field_set_char:tensorflow.serving.InferenceTask.method_name)
}
 void InferenceTask::set_method_name(const char* value,
    size_t size) {
  
  method_name_.Set(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(
      reinterpret_cast<const char*>(value), size), GetArenaNoVirtual());
  // @@protoc_insertion_point(field_set_pointer:tensorflow.serving.InferenceTask.method_name)
}
 ::std::string* InferenceTask::mutable_method_name() {
  
  // @@protoc_insertion_point(field_mutable:tensorflow.serving.InferenceTask.method_name)
  return method_name_.Mutable(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), GetArenaNoVirtual());
}
 ::std::string* InferenceTask::release_method_name() {
  // @@protoc_insertion_point(field_release:tensorflow.serving.InferenceTask.method_name)
  
  return method_name_.Release(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), GetArenaNoVirtual());
}
 ::std::string* InferenceTask::unsafe_arena_release_method_name() {
  // @@protoc_insertion_point(field_unsafe_arena_release:tensorflow.serving.InferenceTask.method_name)
  GOOGLE_DCHECK(GetArenaNoVirtual() != NULL);
  
  return method_name_.UnsafeArenaRelease(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      GetArenaNoVirtual());
}
 void InferenceTask::set_allocated_method_name(::std::string* method_name) {
  if (method_name != NULL) {
    
  } else {
    
  }
  method_name_.SetAllocated(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), method_name,
      GetArenaNoVirtual());
  // @@protoc_insertion_point(field_set_allocated:tensorflow.serving.InferenceTask.method_name)
}
 void InferenceTask::unsafe_arena_set_allocated_method_name(
    ::std::string* method_name) {
  GOOGLE_DCHECK(GetArenaNoVirtual() != NULL);
  if (method_name != NULL) {
    
  } else {
    
  }
  method_name_.UnsafeArenaSetAllocated(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      method_name, GetArenaNoVirtual());
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:tensorflow.serving.InferenceTask.method_name)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

void InferenceResult::_slow_mutable_model_spec() {
  model_spec_ = ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::ModelSpec >(
      GetArenaNoVirtual());
}
::tensorflow::serving::ModelSpec* InferenceResult::_slow_release_model_spec() {
  if (model_spec_ == NULL) {
    return NULL;
  } else {
    ::tensorflow::serving::ModelSpec* temp = new ::tensorflow::serving::ModelSpec;
    temp->MergeFrom(*model_spec_);
    model_spec_ = NULL;
    return temp;
  }
}
::tensorflow::serving::ModelSpec* InferenceResult::unsafe_arena_release_model_spec() {
  // @@protoc_insertion_point(field_unsafe_arena_release:tensorflow.serving.InferenceResult.model_spec)
  
  ::tensorflow::serving::ModelSpec* temp = model_spec_;
  model_spec_ = NULL;
  return temp;
}
void InferenceResult::_slow_set_allocated_model_spec(
    ::google::protobuf::Arena* message_arena, ::tensorflow::serving::ModelSpec** model_spec) {
    if (message_arena != NULL && 
        ::google::protobuf::Arena::GetArena(*model_spec) == NULL) {
      message_arena->Own(*model_spec);
    } else if (message_arena !=
               ::google::protobuf::Arena::GetArena(*model_spec)) {
      ::tensorflow::serving::ModelSpec* new_model_spec = 
            ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::ModelSpec >(
            message_arena);
      new_model_spec->CopyFrom(**model_spec);
      *model_spec = new_model_spec;
    }
}
void InferenceResult::unsafe_arena_set_allocated_model_spec(
    ::tensorflow::serving::ModelSpec* model_spec) {
  if (GetArenaNoVirtual() == NULL) {
    delete model_spec_;
  }
  model_spec_ = model_spec;
  if (model_spec) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:tensorflow.serving.InferenceResult.model_spec)
}
#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int InferenceResult::kModelSpecFieldNumber;
const int InferenceResult::kClassificationResultFieldNumber;
const int InferenceResult::kRegressionResultFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

InferenceResult::InferenceResult()
  : ::google::protobuf::Message(), _internal_metadata_(NULL) {
  SharedCtor();
  // @@protoc_insertion_point(constructor:tensorflow.serving.InferenceResult)
}

InferenceResult::InferenceResult(::google::protobuf::Arena* arena)
  : ::google::protobuf::Message(),
  _internal_metadata_(arena) {
  SharedCtor();
  RegisterArenaDtor(arena);
  // @@protoc_insertion_point(arena_constructor:tensorflow.serving.InferenceResult)
}

void InferenceResult::InitAsDefaultInstance() {
  _is_default_instance_ = true;
  model_spec_ = const_cast< ::tensorflow::serving::ModelSpec*>(&::tensorflow::serving::ModelSpec::default_instance());
  InferenceResult_default_oneof_instance_->classification_result_ = const_cast< ::tensorflow::serving::ClassificationResult*>(&::tensorflow::serving::ClassificationResult::default_instance());
  InferenceResult_default_oneof_instance_->regression_result_ = const_cast< ::tensorflow::serving::RegressionResult*>(&::tensorflow::serving::RegressionResult::default_instance());
}

InferenceResult::InferenceResult(const InferenceResult& from)
  : ::google::protobuf::Message(),
    _internal_metadata_(NULL) {
  SharedCtor();
  MergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:tensorflow.serving.InferenceResult)
}

void InferenceResult::SharedCtor() {
    _is_default_instance_ = false;
  _cached_size_ = 0;
  model_spec_ = NULL;
  clear_has_result();
}

InferenceResult::~InferenceResult() {
  // @@protoc_insertion_point(destructor:tensorflow.serving.InferenceResult)
  SharedDtor();
}

void InferenceResult::SharedDtor() {
  if (GetArenaNoVirtual() != NULL) {
    return;
  }

  if (has_result()) {
    clear_result();
  }
  if (this != default_instance_) {
    delete model_spec_;
  }
}

void InferenceResult::ArenaDtor(void* object) {
  InferenceResult* _this = reinterpret_cast< InferenceResult* >(object);
  (void)_this;
}
void InferenceResult::RegisterArenaDtor(::google::protobuf::Arena* arena) {
}
void InferenceResult::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ::google::protobuf::Descriptor* InferenceResult::descriptor() {
  protobuf_AssignDescriptorsOnce();
  return InferenceResult_descriptor_;
}

const InferenceResult& InferenceResult::default_instance() {
  if (default_instance_ == NULL) protobuf_AddDesc_tensorflow_5fserving_2fapis_2finference_2eproto();
  return *default_instance_;
}

InferenceResult* InferenceResult::default_instance_ = NULL;

InferenceResult* InferenceResult::New(::google::protobuf::Arena* arena) const {
  return ::google::protobuf::Arena::CreateMessage<InferenceResult>(arena);
}

void InferenceResult::clear_result() {
// @@protoc_insertion_point(one_of_clear_start:tensorflow.serving.InferenceResult)
  switch(result_case()) {
    case kClassificationResult: {
      if (GetArenaNoVirtual() == NULL) {
        delete result_.classification_result_;
      }
      break;
    }
    case kRegressionResult: {
      if (GetArenaNoVirtual() == NULL) {
        delete result_.regression_result_;
      }
      break;
    }
    case RESULT_NOT_SET: {
      break;
    }
  }
  _oneof_case_[0] = RESULT_NOT_SET;
}


void InferenceResult::Clear() {
// @@protoc_insertion_point(message_clear_start:tensorflow.serving.InferenceResult)
  if (GetArenaNoVirtual() == NULL && model_spec_ != NULL) delete model_spec_;
  model_spec_ = NULL;
  clear_result();
}

bool InferenceResult::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:tensorflow.serving.InferenceResult)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // optional .tensorflow.serving.ModelSpec model_spec = 1;
      case 1: {
        if (tag == 10) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_model_spec()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(18)) goto parse_classification_result;
        break;
      }

      // optional .tensorflow.serving.ClassificationResult classification_result = 2;
      case 2: {
        if (tag == 18) {
         parse_classification_result:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_classification_result()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(26)) goto parse_regression_result;
        break;
      }

      // optional .tensorflow.serving.RegressionResult regression_result = 3;
      case 3: {
        if (tag == 26) {
         parse_regression_result:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_regression_result()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:tensorflow.serving.InferenceResult)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:tensorflow.serving.InferenceResult)
  return false;
#undef DO_
}

void InferenceResult::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:tensorflow.serving.InferenceResult)
  // optional .tensorflow.serving.ModelSpec model_spec = 1;
  if (this->has_model_spec()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      1, *this->model_spec_, output);
  }

  // optional .tensorflow.serving.ClassificationResult classification_result = 2;
  if (has_classification_result()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      2, *result_.classification_result_, output);
  }

  // optional .tensorflow.serving.RegressionResult regression_result = 3;
  if (has_regression_result()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      3, *result_.regression_result_, output);
  }

  // @@protoc_insertion_point(serialize_end:tensorflow.serving.InferenceResult)
}

::google::protobuf::uint8* InferenceResult::InternalSerializeWithCachedSizesToArray(
    bool deterministic, ::google::protobuf::uint8* target) const {
  // @@protoc_insertion_point(serialize_to_array_start:tensorflow.serving.InferenceResult)
  // optional .tensorflow.serving.ModelSpec model_spec = 1;
  if (this->has_model_spec()) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageNoVirtualToArray(
        1, *this->model_spec_, false, target);
  }

  // optional .tensorflow.serving.ClassificationResult classification_result = 2;
  if (has_classification_result()) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageNoVirtualToArray(
        2, *result_.classification_result_, false, target);
  }

  // optional .tensorflow.serving.RegressionResult regression_result = 3;
  if (has_regression_result()) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageNoVirtualToArray(
        3, *result_.regression_result_, false, target);
  }

  // @@protoc_insertion_point(serialize_to_array_end:tensorflow.serving.InferenceResult)
  return target;
}

int InferenceResult::ByteSize() const {
// @@protoc_insertion_point(message_byte_size_start:tensorflow.serving.InferenceResult)
  int total_size = 0;

  // optional .tensorflow.serving.ModelSpec model_spec = 1;
  if (this->has_model_spec()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->model_spec_);
  }

  switch (result_case()) {
    // optional .tensorflow.serving.ClassificationResult classification_result = 2;
    case kClassificationResult: {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *result_.classification_result_);
      break;
    }
    // optional .tensorflow.serving.RegressionResult regression_result = 3;
    case kRegressionResult: {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *result_.regression_result_);
      break;
    }
    case RESULT_NOT_SET: {
      break;
    }
  }
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = total_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void InferenceResult::MergeFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_merge_from_start:tensorflow.serving.InferenceResult)
  if (GOOGLE_PREDICT_FALSE(&from == this)) {
    ::google::protobuf::internal::MergeFromFail(__FILE__, __LINE__);
  }
  const InferenceResult* source = 
      ::google::protobuf::internal::DynamicCastToGenerated<const InferenceResult>(
          &from);
  if (source == NULL) {
  // @@protoc_insertion_point(generalized_merge_from_cast_fail:tensorflow.serving.InferenceResult)
    ::google::protobuf::internal::ReflectionOps::Merge(from, this);
  } else {
  // @@protoc_insertion_point(generalized_merge_from_cast_success:tensorflow.serving.InferenceResult)
    MergeFrom(*source);
  }
}

void InferenceResult::MergeFrom(const InferenceResult& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:tensorflow.serving.InferenceResult)
  if (GOOGLE_PREDICT_FALSE(&from == this)) {
    ::google::protobuf::internal::MergeFromFail(__FILE__, __LINE__);
  }
  switch (from.result_case()) {
    case kClassificationResult: {
      mutable_classification_result()->::tensorflow::serving::ClassificationResult::MergeFrom(from.classification_result());
      break;
    }
    case kRegressionResult: {
      mutable_regression_result()->::tensorflow::serving::RegressionResult::MergeFrom(from.regression_result());
      break;
    }
    case RESULT_NOT_SET: {
      break;
    }
  }
  if (from.has_model_spec()) {
    mutable_model_spec()->::tensorflow::serving::ModelSpec::MergeFrom(from.model_spec());
  }
}

void InferenceResult::CopyFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_copy_from_start:tensorflow.serving.InferenceResult)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

void InferenceResult::CopyFrom(const InferenceResult& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:tensorflow.serving.InferenceResult)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool InferenceResult::IsInitialized() const {

  return true;
}

void InferenceResult::Swap(InferenceResult* other) {
  if (other == this) return;
  if (GetArenaNoVirtual() == other->GetArenaNoVirtual()) {
    InternalSwap(other);
  } else {
    InferenceResult temp;
    temp.MergeFrom(*this);
    CopyFrom(*other);
    other->CopyFrom(temp);
  }
}
void InferenceResult::UnsafeArenaSwap(InferenceResult* other) {
  if (other == this) return;
  GOOGLE_DCHECK(GetArenaNoVirtual() == other->GetArenaNoVirtual());
  InternalSwap(other);
}
void InferenceResult::InternalSwap(InferenceResult* other) {
  std::swap(model_spec_, other->model_spec_);
  std::swap(result_, other->result_);
  std::swap(_oneof_case_[0], other->_oneof_case_[0]);
  _internal_metadata_.Swap(&other->_internal_metadata_);
  std::swap(_cached_size_, other->_cached_size_);
}

::google::protobuf::Metadata InferenceResult::GetMetadata() const {
  protobuf_AssignDescriptorsOnce();
  ::google::protobuf::Metadata metadata;
  metadata.descriptor = InferenceResult_descriptor_;
  metadata.reflection = InferenceResult_reflection_;
  return metadata;
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// InferenceResult

// optional .tensorflow.serving.ModelSpec model_spec = 1;
bool InferenceResult::has_model_spec() const {
  return !_is_default_instance_ && model_spec_ != NULL;
}
void InferenceResult::clear_model_spec() {
  if (GetArenaNoVirtual() == NULL && model_spec_ != NULL) delete model_spec_;
  model_spec_ = NULL;
}
const ::tensorflow::serving::ModelSpec& InferenceResult::model_spec() const {
  // @@protoc_insertion_point(field_get:tensorflow.serving.InferenceResult.model_spec)
  return model_spec_ != NULL ? *model_spec_ : *default_instance_->model_spec_;
}
::tensorflow::serving::ModelSpec* InferenceResult::mutable_model_spec() {
  
  if (model_spec_ == NULL) {
    _slow_mutable_model_spec();
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.serving.InferenceResult.model_spec)
  return model_spec_;
}
::tensorflow::serving::ModelSpec* InferenceResult::release_model_spec() {
  // @@protoc_insertion_point(field_release:tensorflow.serving.InferenceResult.model_spec)
  
  if (GetArenaNoVirtual() != NULL) {
    return _slow_release_model_spec();
  } else {
    ::tensorflow::serving::ModelSpec* temp = model_spec_;
    model_spec_ = NULL;
    return temp;
  }
}
 void InferenceResult::set_allocated_model_spec(::tensorflow::serving::ModelSpec* model_spec) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete model_spec_;
  }
  if (model_spec != NULL) {
    _slow_set_allocated_model_spec(message_arena, &model_spec);
  }
  model_spec_ = model_spec;
  if (model_spec) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:tensorflow.serving.InferenceResult.model_spec)
}

// optional .tensorflow.serving.ClassificationResult classification_result = 2;
bool InferenceResult::has_classification_result() const {
  return result_case() == kClassificationResult;
}
void InferenceResult::set_has_classification_result() {
  _oneof_case_[0] = kClassificationResult;
}
void InferenceResult::clear_classification_result() {
  if (has_classification_result()) {
    if (GetArenaNoVirtual() == NULL) {
      delete result_.classification_result_;
    }
    clear_has_result();
  }
}
 const ::tensorflow::serving::ClassificationResult& InferenceResult::classification_result() const {
  // @@protoc_insertion_point(field_get:tensorflow.serving.InferenceResult.classification_result)
  return has_classification_result()
      ? *result_.classification_result_
      : ::tensorflow::serving::ClassificationResult::default_instance();
}
::tensorflow::serving::ClassificationResult* InferenceResult::mutable_classification_result() {
  if (!has_classification_result()) {
    clear_result();
    set_has_classification_result();
    result_.classification_result_ = 
      ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::ClassificationResult >(
      GetArenaNoVirtual());
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.serving.InferenceResult.classification_result)
  return result_.classification_result_;
}
::tensorflow::serving::ClassificationResult* InferenceResult::release_classification_result() {
  // @@protoc_insertion_point(field_release:tensorflow.serving.InferenceResult.classification_result)
  if (has_classification_result()) {
    clear_has_result();
    if (GetArenaNoVirtual() != NULL) {
      ::tensorflow::serving::ClassificationResult* temp = new ::tensorflow::serving::ClassificationResult;
      temp->MergeFrom(*result_.classification_result_);
      result_.classification_result_ = NULL;
      return temp;
    } else {
      ::tensorflow::serving::ClassificationResult* temp = result_.classification_result_;
      result_.classification_result_ = NULL;
      return temp;
    }
  } else {
    return NULL;
  }
}
void InferenceResult::set_allocated_classification_result(::tensorflow::serving::ClassificationResult* classification_result) {
  clear_result();
  if (classification_result) {
    if (GetArenaNoVirtual() != NULL &&
        ::google::protobuf::Arena::GetArena(classification_result) == NULL) {
      GetArenaNoVirtual()->Own(classification_result);
    } else if (GetArenaNoVirtual() !=
               ::google::protobuf::Arena::GetArena(classification_result)) {
      ::tensorflow::serving::ClassificationResult* new_classification_result = 
          ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::ClassificationResult >(
          GetArenaNoVirtual());
      new_classification_result->CopyFrom(*classification_result);
      classification_result = new_classification_result;
    }
    set_has_classification_result();
    result_.classification_result_ = classification_result;
  }
  // @@protoc_insertion_point(field_set_allocated:tensorflow.serving.InferenceResult.classification_result)
}
 ::tensorflow::serving::ClassificationResult* InferenceResult::unsafe_arena_release_classification_result() {
  // @@protoc_insertion_point(field_unsafe_arena_release:tensorflow.serving.InferenceResult.classification_result)
  if (has_classification_result()) {
    clear_has_result();
    ::tensorflow::serving::ClassificationResult* temp = result_.classification_result_;
    result_.classification_result_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
 void InferenceResult::unsafe_arena_set_allocated_classification_result(::tensorflow::serving::ClassificationResult* classification_result) {
  clear_result();
  if (classification_result) {
    set_has_classification_result();
    result_.classification_result_ = classification_result;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:tensorflow.serving.InferenceResult.classification_result)
}

// optional .tensorflow.serving.RegressionResult regression_result = 3;
bool InferenceResult::has_regression_result() const {
  return result_case() == kRegressionResult;
}
void InferenceResult::set_has_regression_result() {
  _oneof_case_[0] = kRegressionResult;
}
void InferenceResult::clear_regression_result() {
  if (has_regression_result()) {
    if (GetArenaNoVirtual() == NULL) {
      delete result_.regression_result_;
    }
    clear_has_result();
  }
}
 const ::tensorflow::serving::RegressionResult& InferenceResult::regression_result() const {
  // @@protoc_insertion_point(field_get:tensorflow.serving.InferenceResult.regression_result)
  return has_regression_result()
      ? *result_.regression_result_
      : ::tensorflow::serving::RegressionResult::default_instance();
}
::tensorflow::serving::RegressionResult* InferenceResult::mutable_regression_result() {
  if (!has_regression_result()) {
    clear_result();
    set_has_regression_result();
    result_.regression_result_ = 
      ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::RegressionResult >(
      GetArenaNoVirtual());
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.serving.InferenceResult.regression_result)
  return result_.regression_result_;
}
::tensorflow::serving::RegressionResult* InferenceResult::release_regression_result() {
  // @@protoc_insertion_point(field_release:tensorflow.serving.InferenceResult.regression_result)
  if (has_regression_result()) {
    clear_has_result();
    if (GetArenaNoVirtual() != NULL) {
      ::tensorflow::serving::RegressionResult* temp = new ::tensorflow::serving::RegressionResult;
      temp->MergeFrom(*result_.regression_result_);
      result_.regression_result_ = NULL;
      return temp;
    } else {
      ::tensorflow::serving::RegressionResult* temp = result_.regression_result_;
      result_.regression_result_ = NULL;
      return temp;
    }
  } else {
    return NULL;
  }
}
void InferenceResult::set_allocated_regression_result(::tensorflow::serving::RegressionResult* regression_result) {
  clear_result();
  if (regression_result) {
    if (GetArenaNoVirtual() != NULL &&
        ::google::protobuf::Arena::GetArena(regression_result) == NULL) {
      GetArenaNoVirtual()->Own(regression_result);
    } else if (GetArenaNoVirtual() !=
               ::google::protobuf::Arena::GetArena(regression_result)) {
      ::tensorflow::serving::RegressionResult* new_regression_result = 
          ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::RegressionResult >(
          GetArenaNoVirtual());
      new_regression_result->CopyFrom(*regression_result);
      regression_result = new_regression_result;
    }
    set_has_regression_result();
    result_.regression_result_ = regression_result;
  }
  // @@protoc_insertion_point(field_set_allocated:tensorflow.serving.InferenceResult.regression_result)
}
 ::tensorflow::serving::RegressionResult* InferenceResult::unsafe_arena_release_regression_result() {
  // @@protoc_insertion_point(field_unsafe_arena_release:tensorflow.serving.InferenceResult.regression_result)
  if (has_regression_result()) {
    clear_has_result();
    ::tensorflow::serving::RegressionResult* temp = result_.regression_result_;
    result_.regression_result_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
 void InferenceResult::unsafe_arena_set_allocated_regression_result(::tensorflow::serving::RegressionResult* regression_result) {
  clear_result();
  if (regression_result) {
    set_has_regression_result();
    result_.regression_result_ = regression_result;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:tensorflow.serving.InferenceResult.regression_result)
}

bool InferenceResult::has_result() const {
  return result_case() != RESULT_NOT_SET;
}
void InferenceResult::clear_has_result() {
  _oneof_case_[0] = RESULT_NOT_SET;
}
InferenceResult::ResultCase InferenceResult::result_case() const {
  return InferenceResult::ResultCase(_oneof_case_[0]);
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

void MultiInferenceRequest::_slow_mutable_input() {
  input_ = ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::Input >(
      GetArenaNoVirtual());
}
::tensorflow::serving::Input* MultiInferenceRequest::_slow_release_input() {
  if (input_ == NULL) {
    return NULL;
  } else {
    ::tensorflow::serving::Input* temp = new ::tensorflow::serving::Input;
    temp->MergeFrom(*input_);
    input_ = NULL;
    return temp;
  }
}
::tensorflow::serving::Input* MultiInferenceRequest::unsafe_arena_release_input() {
  // @@protoc_insertion_point(field_unsafe_arena_release:tensorflow.serving.MultiInferenceRequest.input)
  
  ::tensorflow::serving::Input* temp = input_;
  input_ = NULL;
  return temp;
}
void MultiInferenceRequest::_slow_set_allocated_input(
    ::google::protobuf::Arena* message_arena, ::tensorflow::serving::Input** input) {
    if (message_arena != NULL && 
        ::google::protobuf::Arena::GetArena(*input) == NULL) {
      message_arena->Own(*input);
    } else if (message_arena !=
               ::google::protobuf::Arena::GetArena(*input)) {
      ::tensorflow::serving::Input* new_input = 
            ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::Input >(
            message_arena);
      new_input->CopyFrom(**input);
      *input = new_input;
    }
}
void MultiInferenceRequest::unsafe_arena_set_allocated_input(
    ::tensorflow::serving::Input* input) {
  if (GetArenaNoVirtual() == NULL) {
    delete input_;
  }
  input_ = input;
  if (input) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:tensorflow.serving.MultiInferenceRequest.input)
}
#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int MultiInferenceRequest::kTasksFieldNumber;
const int MultiInferenceRequest::kInputFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

MultiInferenceRequest::MultiInferenceRequest()
  : ::google::protobuf::Message(), _internal_metadata_(NULL) {
  SharedCtor();
  // @@protoc_insertion_point(constructor:tensorflow.serving.MultiInferenceRequest)
}

MultiInferenceRequest::MultiInferenceRequest(::google::protobuf::Arena* arena)
  : ::google::protobuf::Message(),
  _internal_metadata_(arena),
  tasks_(arena) {
  SharedCtor();
  RegisterArenaDtor(arena);
  // @@protoc_insertion_point(arena_constructor:tensorflow.serving.MultiInferenceRequest)
}

void MultiInferenceRequest::InitAsDefaultInstance() {
  _is_default_instance_ = true;
  input_ = const_cast< ::tensorflow::serving::Input*>(&::tensorflow::serving::Input::default_instance());
}

MultiInferenceRequest::MultiInferenceRequest(const MultiInferenceRequest& from)
  : ::google::protobuf::Message(),
    _internal_metadata_(NULL) {
  SharedCtor();
  MergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:tensorflow.serving.MultiInferenceRequest)
}

void MultiInferenceRequest::SharedCtor() {
    _is_default_instance_ = false;
  _cached_size_ = 0;
  input_ = NULL;
}

MultiInferenceRequest::~MultiInferenceRequest() {
  // @@protoc_insertion_point(destructor:tensorflow.serving.MultiInferenceRequest)
  SharedDtor();
}

void MultiInferenceRequest::SharedDtor() {
  if (GetArenaNoVirtual() != NULL) {
    return;
  }

  if (this != default_instance_) {
    delete input_;
  }
}

void MultiInferenceRequest::ArenaDtor(void* object) {
  MultiInferenceRequest* _this = reinterpret_cast< MultiInferenceRequest* >(object);
  (void)_this;
}
void MultiInferenceRequest::RegisterArenaDtor(::google::protobuf::Arena* arena) {
}
void MultiInferenceRequest::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ::google::protobuf::Descriptor* MultiInferenceRequest::descriptor() {
  protobuf_AssignDescriptorsOnce();
  return MultiInferenceRequest_descriptor_;
}

const MultiInferenceRequest& MultiInferenceRequest::default_instance() {
  if (default_instance_ == NULL) protobuf_AddDesc_tensorflow_5fserving_2fapis_2finference_2eproto();
  return *default_instance_;
}

MultiInferenceRequest* MultiInferenceRequest::default_instance_ = NULL;

MultiInferenceRequest* MultiInferenceRequest::New(::google::protobuf::Arena* arena) const {
  return ::google::protobuf::Arena::CreateMessage<MultiInferenceRequest>(arena);
}

void MultiInferenceRequest::Clear() {
// @@protoc_insertion_point(message_clear_start:tensorflow.serving.MultiInferenceRequest)
  if (GetArenaNoVirtual() == NULL && input_ != NULL) delete input_;
  input_ = NULL;
  tasks_.Clear();
}

bool MultiInferenceRequest::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:tensorflow.serving.MultiInferenceRequest)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated .tensorflow.serving.InferenceTask tasks = 1;
      case 1: {
        if (tag == 10) {
          DO_(input->IncrementRecursionDepth());
         parse_loop_tasks:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtualNoRecursionDepth(
                input, add_tasks()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(10)) goto parse_loop_tasks;
        input->UnsafeDecrementRecursionDepth();
        if (input->ExpectTag(18)) goto parse_input;
        break;
      }

      // optional .tensorflow.serving.Input input = 2;
      case 2: {
        if (tag == 18) {
         parse_input:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_input()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:tensorflow.serving.MultiInferenceRequest)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:tensorflow.serving.MultiInferenceRequest)
  return false;
#undef DO_
}

void MultiInferenceRequest::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:tensorflow.serving.MultiInferenceRequest)
  // repeated .tensorflow.serving.InferenceTask tasks = 1;
  for (unsigned int i = 0, n = this->tasks_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      1, this->tasks(i), output);
  }

  // optional .tensorflow.serving.Input input = 2;
  if (this->has_input()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      2, *this->input_, output);
  }

  // @@protoc_insertion_point(serialize_end:tensorflow.serving.MultiInferenceRequest)
}

::google::protobuf::uint8* MultiInferenceRequest::InternalSerializeWithCachedSizesToArray(
    bool deterministic, ::google::protobuf::uint8* target) const {
  // @@protoc_insertion_point(serialize_to_array_start:tensorflow.serving.MultiInferenceRequest)
  // repeated .tensorflow.serving.InferenceTask tasks = 1;
  for (unsigned int i = 0, n = this->tasks_size(); i < n; i++) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageNoVirtualToArray(
        1, this->tasks(i), false, target);
  }

  // optional .tensorflow.serving.Input input = 2;
  if (this->has_input()) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageNoVirtualToArray(
        2, *this->input_, false, target);
  }

  // @@protoc_insertion_point(serialize_to_array_end:tensorflow.serving.MultiInferenceRequest)
  return target;
}

int MultiInferenceRequest::ByteSize() const {
// @@protoc_insertion_point(message_byte_size_start:tensorflow.serving.MultiInferenceRequest)
  int total_size = 0;

  // optional .tensorflow.serving.Input input = 2;
  if (this->has_input()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->input_);
  }

  // repeated .tensorflow.serving.InferenceTask tasks = 1;
  total_size += 1 * this->tasks_size();
  for (int i = 0; i < this->tasks_size(); i++) {
    total_size +=
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        this->tasks(i));
  }

  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = total_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void MultiInferenceRequest::MergeFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_merge_from_start:tensorflow.serving.MultiInferenceRequest)
  if (GOOGLE_PREDICT_FALSE(&from == this)) {
    ::google::protobuf::internal::MergeFromFail(__FILE__, __LINE__);
  }
  const MultiInferenceRequest* source = 
      ::google::protobuf::internal::DynamicCastToGenerated<const MultiInferenceRequest>(
          &from);
  if (source == NULL) {
  // @@protoc_insertion_point(generalized_merge_from_cast_fail:tensorflow.serving.MultiInferenceRequest)
    ::google::protobuf::internal::ReflectionOps::Merge(from, this);
  } else {
  // @@protoc_insertion_point(generalized_merge_from_cast_success:tensorflow.serving.MultiInferenceRequest)
    MergeFrom(*source);
  }
}

void MultiInferenceRequest::MergeFrom(const MultiInferenceRequest& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:tensorflow.serving.MultiInferenceRequest)
  if (GOOGLE_PREDICT_FALSE(&from == this)) {
    ::google::protobuf::internal::MergeFromFail(__FILE__, __LINE__);
  }
  tasks_.MergeFrom(from.tasks_);
  if (from.has_input()) {
    mutable_input()->::tensorflow::serving::Input::MergeFrom(from.input());
  }
}

void MultiInferenceRequest::CopyFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_copy_from_start:tensorflow.serving.MultiInferenceRequest)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

void MultiInferenceRequest::CopyFrom(const MultiInferenceRequest& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:tensorflow.serving.MultiInferenceRequest)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool MultiInferenceRequest::IsInitialized() const {

  return true;
}

void MultiInferenceRequest::Swap(MultiInferenceRequest* other) {
  if (other == this) return;
  if (GetArenaNoVirtual() == other->GetArenaNoVirtual()) {
    InternalSwap(other);
  } else {
    MultiInferenceRequest temp;
    temp.MergeFrom(*this);
    CopyFrom(*other);
    other->CopyFrom(temp);
  }
}
void MultiInferenceRequest::UnsafeArenaSwap(MultiInferenceRequest* other) {
  if (other == this) return;
  GOOGLE_DCHECK(GetArenaNoVirtual() == other->GetArenaNoVirtual());
  InternalSwap(other);
}
void MultiInferenceRequest::InternalSwap(MultiInferenceRequest* other) {
  tasks_.UnsafeArenaSwap(&other->tasks_);
  std::swap(input_, other->input_);
  _internal_metadata_.Swap(&other->_internal_metadata_);
  std::swap(_cached_size_, other->_cached_size_);
}

::google::protobuf::Metadata MultiInferenceRequest::GetMetadata() const {
  protobuf_AssignDescriptorsOnce();
  ::google::protobuf::Metadata metadata;
  metadata.descriptor = MultiInferenceRequest_descriptor_;
  metadata.reflection = MultiInferenceRequest_reflection_;
  return metadata;
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// MultiInferenceRequest

// repeated .tensorflow.serving.InferenceTask tasks = 1;
int MultiInferenceRequest::tasks_size() const {
  return tasks_.size();
}
void MultiInferenceRequest::clear_tasks() {
  tasks_.Clear();
}
const ::tensorflow::serving::InferenceTask& MultiInferenceRequest::tasks(int index) const {
  // @@protoc_insertion_point(field_get:tensorflow.serving.MultiInferenceRequest.tasks)
  return tasks_.Get(index);
}
::tensorflow::serving::InferenceTask* MultiInferenceRequest::mutable_tasks(int index) {
  // @@protoc_insertion_point(field_mutable:tensorflow.serving.MultiInferenceRequest.tasks)
  return tasks_.Mutable(index);
}
::tensorflow::serving::InferenceTask* MultiInferenceRequest::add_tasks() {
  // @@protoc_insertion_point(field_add:tensorflow.serving.MultiInferenceRequest.tasks)
  return tasks_.Add();
}
::google::protobuf::RepeatedPtrField< ::tensorflow::serving::InferenceTask >*
MultiInferenceRequest::mutable_tasks() {
  // @@protoc_insertion_point(field_mutable_list:tensorflow.serving.MultiInferenceRequest.tasks)
  return &tasks_;
}
const ::google::protobuf::RepeatedPtrField< ::tensorflow::serving::InferenceTask >&
MultiInferenceRequest::tasks() const {
  // @@protoc_insertion_point(field_list:tensorflow.serving.MultiInferenceRequest.tasks)
  return tasks_;
}

// optional .tensorflow.serving.Input input = 2;
bool MultiInferenceRequest::has_input() const {
  return !_is_default_instance_ && input_ != NULL;
}
void MultiInferenceRequest::clear_input() {
  if (GetArenaNoVirtual() == NULL && input_ != NULL) delete input_;
  input_ = NULL;
}
const ::tensorflow::serving::Input& MultiInferenceRequest::input() const {
  // @@protoc_insertion_point(field_get:tensorflow.serving.MultiInferenceRequest.input)
  return input_ != NULL ? *input_ : *default_instance_->input_;
}
::tensorflow::serving::Input* MultiInferenceRequest::mutable_input() {
  
  if (input_ == NULL) {
    _slow_mutable_input();
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.serving.MultiInferenceRequest.input)
  return input_;
}
::tensorflow::serving::Input* MultiInferenceRequest::release_input() {
  // @@protoc_insertion_point(field_release:tensorflow.serving.MultiInferenceRequest.input)
  
  if (GetArenaNoVirtual() != NULL) {
    return _slow_release_input();
  } else {
    ::tensorflow::serving::Input* temp = input_;
    input_ = NULL;
    return temp;
  }
}
 void MultiInferenceRequest::set_allocated_input(::tensorflow::serving::Input* input) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete input_;
  }
  if (input != NULL) {
    _slow_set_allocated_input(message_arena, &input);
  }
  input_ = input;
  if (input) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:tensorflow.serving.MultiInferenceRequest.input)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int MultiInferenceResponse::kResultsFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

MultiInferenceResponse::MultiInferenceResponse()
  : ::google::protobuf::Message(), _internal_metadata_(NULL) {
  SharedCtor();
  // @@protoc_insertion_point(constructor:tensorflow.serving.MultiInferenceResponse)
}

MultiInferenceResponse::MultiInferenceResponse(::google::protobuf::Arena* arena)
  : ::google::protobuf::Message(),
  _internal_metadata_(arena),
  results_(arena) {
  SharedCtor();
  RegisterArenaDtor(arena);
  // @@protoc_insertion_point(arena_constructor:tensorflow.serving.MultiInferenceResponse)
}

void MultiInferenceResponse::InitAsDefaultInstance() {
  _is_default_instance_ = true;
}

MultiInferenceResponse::MultiInferenceResponse(const MultiInferenceResponse& from)
  : ::google::protobuf::Message(),
    _internal_metadata_(NULL) {
  SharedCtor();
  MergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:tensorflow.serving.MultiInferenceResponse)
}

void MultiInferenceResponse::SharedCtor() {
    _is_default_instance_ = false;
  _cached_size_ = 0;
}

MultiInferenceResponse::~MultiInferenceResponse() {
  // @@protoc_insertion_point(destructor:tensorflow.serving.MultiInferenceResponse)
  SharedDtor();
}

void MultiInferenceResponse::SharedDtor() {
  if (GetArenaNoVirtual() != NULL) {
    return;
  }

  if (this != default_instance_) {
  }
}

void MultiInferenceResponse::ArenaDtor(void* object) {
  MultiInferenceResponse* _this = reinterpret_cast< MultiInferenceResponse* >(object);
  (void)_this;
}
void MultiInferenceResponse::RegisterArenaDtor(::google::protobuf::Arena* arena) {
}
void MultiInferenceResponse::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ::google::protobuf::Descriptor* MultiInferenceResponse::descriptor() {
  protobuf_AssignDescriptorsOnce();
  return MultiInferenceResponse_descriptor_;
}

const MultiInferenceResponse& MultiInferenceResponse::default_instance() {
  if (default_instance_ == NULL) protobuf_AddDesc_tensorflow_5fserving_2fapis_2finference_2eproto();
  return *default_instance_;
}

MultiInferenceResponse* MultiInferenceResponse::default_instance_ = NULL;

MultiInferenceResponse* MultiInferenceResponse::New(::google::protobuf::Arena* arena) const {
  return ::google::protobuf::Arena::CreateMessage<MultiInferenceResponse>(arena);
}

void MultiInferenceResponse::Clear() {
// @@protoc_insertion_point(message_clear_start:tensorflow.serving.MultiInferenceResponse)
  results_.Clear();
}

bool MultiInferenceResponse::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:tensorflow.serving.MultiInferenceResponse)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated .tensorflow.serving.InferenceResult results = 1;
      case 1: {
        if (tag == 10) {
          DO_(input->IncrementRecursionDepth());
         parse_loop_results:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtualNoRecursionDepth(
                input, add_results()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(10)) goto parse_loop_results;
        input->UnsafeDecrementRecursionDepth();
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:tensorflow.serving.MultiInferenceResponse)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:tensorflow.serving.MultiInferenceResponse)
  return false;
#undef DO_
}

void MultiInferenceResponse::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:tensorflow.serving.MultiInferenceResponse)
  // repeated .tensorflow.serving.InferenceResult results = 1;
  for (unsigned int i = 0, n = this->results_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      1, this->results(i), output);
  }

  // @@protoc_insertion_point(serialize_end:tensorflow.serving.MultiInferenceResponse)
}

::google::protobuf::uint8* MultiInferenceResponse::InternalSerializeWithCachedSizesToArray(
    bool deterministic, ::google::protobuf::uint8* target) const {
  // @@protoc_insertion_point(serialize_to_array_start:tensorflow.serving.MultiInferenceResponse)
  // repeated .tensorflow.serving.InferenceResult results = 1;
  for (unsigned int i = 0, n = this->results_size(); i < n; i++) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageNoVirtualToArray(
        1, this->results(i), false, target);
  }

  // @@protoc_insertion_point(serialize_to_array_end:tensorflow.serving.MultiInferenceResponse)
  return target;
}

int MultiInferenceResponse::ByteSize() const {
// @@protoc_insertion_point(message_byte_size_start:tensorflow.serving.MultiInferenceResponse)
  int total_size = 0;

  // repeated .tensorflow.serving.InferenceResult results = 1;
  total_size += 1 * this->results_size();
  for (int i = 0; i < this->results_size(); i++) {
    total_size +=
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        this->results(i));
  }

  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = total_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void MultiInferenceResponse::MergeFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_merge_from_start:tensorflow.serving.MultiInferenceResponse)
  if (GOOGLE_PREDICT_FALSE(&from == this)) {
    ::google::protobuf::internal::MergeFromFail(__FILE__, __LINE__);
  }
  const MultiInferenceResponse* source = 
      ::google::protobuf::internal::DynamicCastToGenerated<const MultiInferenceResponse>(
          &from);
  if (source == NULL) {
  // @@protoc_insertion_point(generalized_merge_from_cast_fail:tensorflow.serving.MultiInferenceResponse)
    ::google::protobuf::internal::ReflectionOps::Merge(from, this);
  } else {
  // @@protoc_insertion_point(generalized_merge_from_cast_success:tensorflow.serving.MultiInferenceResponse)
    MergeFrom(*source);
  }
}

void MultiInferenceResponse::MergeFrom(const MultiInferenceResponse& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:tensorflow.serving.MultiInferenceResponse)
  if (GOOGLE_PREDICT_FALSE(&from == this)) {
    ::google::protobuf::internal::MergeFromFail(__FILE__, __LINE__);
  }
  results_.MergeFrom(from.results_);
}

void MultiInferenceResponse::CopyFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_copy_from_start:tensorflow.serving.MultiInferenceResponse)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

void MultiInferenceResponse::CopyFrom(const MultiInferenceResponse& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:tensorflow.serving.MultiInferenceResponse)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool MultiInferenceResponse::IsInitialized() const {

  return true;
}

void MultiInferenceResponse::Swap(MultiInferenceResponse* other) {
  if (other == this) return;
  if (GetArenaNoVirtual() == other->GetArenaNoVirtual()) {
    InternalSwap(other);
  } else {
    MultiInferenceResponse temp;
    temp.MergeFrom(*this);
    CopyFrom(*other);
    other->CopyFrom(temp);
  }
}
void MultiInferenceResponse::UnsafeArenaSwap(MultiInferenceResponse* other) {
  if (other == this) return;
  GOOGLE_DCHECK(GetArenaNoVirtual() == other->GetArenaNoVirtual());
  InternalSwap(other);
}
void MultiInferenceResponse::InternalSwap(MultiInferenceResponse* other) {
  results_.UnsafeArenaSwap(&other->results_);
  _internal_metadata_.Swap(&other->_internal_metadata_);
  std::swap(_cached_size_, other->_cached_size_);
}

::google::protobuf::Metadata MultiInferenceResponse::GetMetadata() const {
  protobuf_AssignDescriptorsOnce();
  ::google::protobuf::Metadata metadata;
  metadata.descriptor = MultiInferenceResponse_descriptor_;
  metadata.reflection = MultiInferenceResponse_reflection_;
  return metadata;
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// MultiInferenceResponse

// repeated .tensorflow.serving.InferenceResult results = 1;
int MultiInferenceResponse::results_size() const {
  return results_.size();
}
void MultiInferenceResponse::clear_results() {
  results_.Clear();
}
const ::tensorflow::serving::InferenceResult& MultiInferenceResponse::results(int index) const {
  // @@protoc_insertion_point(field_get:tensorflow.serving.MultiInferenceResponse.results)
  return results_.Get(index);
}
::tensorflow::serving::InferenceResult* MultiInferenceResponse::mutable_results(int index) {
  // @@protoc_insertion_point(field_mutable:tensorflow.serving.MultiInferenceResponse.results)
  return results_.Mutable(index);
}
::tensorflow::serving::InferenceResult* MultiInferenceResponse::add_results() {
  // @@protoc_insertion_point(field_add:tensorflow.serving.MultiInferenceResponse.results)
  return results_.Add();
}
::google::protobuf::RepeatedPtrField< ::tensorflow::serving::InferenceResult >*
MultiInferenceResponse::mutable_results() {
  // @@protoc_insertion_point(field_mutable_list:tensorflow.serving.MultiInferenceResponse.results)
  return &results_;
}
const ::google::protobuf::RepeatedPtrField< ::tensorflow::serving::InferenceResult >&
MultiInferenceResponse::results() const {
  // @@protoc_insertion_point(field_list:tensorflow.serving.MultiInferenceResponse.results)
  return results_;
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// @@protoc_insertion_point(namespace_scope)

}  // namespace serving
}  // namespace tensorflow

// @@protoc_insertion_point(global_scope)
