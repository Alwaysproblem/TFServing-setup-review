// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: tensorflow_serving/apis/prediction_log.proto

#ifndef PROTOBUF_tensorflow_5fserving_2fapis_2fprediction_5flog_2eproto__INCLUDED
#define PROTOBUF_tensorflow_5fserving_2fapis_2fprediction_5flog_2eproto__INCLUDED

#include <string>

#include <google/protobuf/stubs/common.h>

#if GOOGLE_PROTOBUF_VERSION < 3000000
#error This file was generated by a newer version of protoc which is
#error incompatible with your Protocol Buffer headers.  Please update
#error your headers.
#endif
#if 3000000 < GOOGLE_PROTOBUF_MIN_PROTOC_VERSION
#error This file was generated by an older version of protoc which is
#error incompatible with your Protocol Buffer headers.  Please
#error regenerate this file with a newer version of protoc.
#endif

#include <google/protobuf/arena.h>
#include <google/protobuf/arenastring.h>
#include <google/protobuf/generated_message_util.h>
#include <google/protobuf/metadata.h>
#include <google/protobuf/message.h>
#include <google/protobuf/repeated_field.h>
#include <google/protobuf/extension_set.h>
#include <google/protobuf/unknown_field_set.h>
#include "tensorflow_serving/apis/classification.pb.h"
#include "tensorflow_serving/apis/inference.pb.h"
#include "tensorflow_serving/apis/predict.pb.h"
#include "tensorflow_serving/apis/regression.pb.h"
#include "tensorflow_serving/apis/session_service.pb.h"
#include "tensorflow_serving/core/logging.pb.h"
// @@protoc_insertion_point(includes)

namespace tensorflow {
namespace serving {

// Internal implementation detail -- do not call these.
void protobuf_AddDesc_tensorflow_5fserving_2fapis_2fprediction_5flog_2eproto();
void protobuf_AssignDesc_tensorflow_5fserving_2fapis_2fprediction_5flog_2eproto();
void protobuf_ShutdownFile_tensorflow_5fserving_2fapis_2fprediction_5flog_2eproto();

class ClassifyLog;
class MultiInferenceLog;
class PredictLog;
class PredictionLog;
class RegressLog;
class SessionRunLog;

// ===================================================================

class ClassifyLog : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:tensorflow.serving.ClassifyLog) */ {
 public:
  ClassifyLog();
  virtual ~ClassifyLog();

  ClassifyLog(const ClassifyLog& from);

  inline ClassifyLog& operator=(const ClassifyLog& from) {
    CopyFrom(from);
    return *this;
  }

  inline ::google::protobuf::Arena* GetArena() const { return GetArenaNoVirtual(); }
  inline void* GetMaybeArenaPointer() const {
    return MaybeArenaPtr();
  }
  static const ::google::protobuf::Descriptor* descriptor();
  static const ClassifyLog& default_instance();

  void UnsafeArenaSwap(ClassifyLog* other);
  void Swap(ClassifyLog* other);

  // implements Message ----------------------------------------------

  inline ClassifyLog* New() const { return New(NULL); }

  ClassifyLog* New(::google::protobuf::Arena* arena) const;
  void CopyFrom(const ::google::protobuf::Message& from);
  void MergeFrom(const ::google::protobuf::Message& from);
  void CopyFrom(const ClassifyLog& from);
  void MergeFrom(const ClassifyLog& from);
  void Clear();
  bool IsInitialized() const;

  int ByteSize() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* output) const;
  ::google::protobuf::uint8* SerializeWithCachedSizesToArray(::google::protobuf::uint8* output) const {
    return InternalSerializeWithCachedSizesToArray(false, output);
  }
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(ClassifyLog* other);
  protected:
  explicit ClassifyLog(::google::protobuf::Arena* arena);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::google::protobuf::Arena* arena);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _internal_metadata_.arena();
  }
  inline void* MaybeArenaPtr() const {
    return _internal_metadata_.raw_arena_ptr();
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // optional .tensorflow.serving.ClassificationRequest request = 1;
  bool has_request() const;
  void clear_request();
  static const int kRequestFieldNumber = 1;
  private:
  void _slow_mutable_request();
  void _slow_set_allocated_request(
      ::google::protobuf::Arena* message_arena, ::tensorflow::serving::ClassificationRequest** request);
  ::tensorflow::serving::ClassificationRequest* _slow_release_request();
  public:
  const ::tensorflow::serving::ClassificationRequest& request() const;
  ::tensorflow::serving::ClassificationRequest* mutable_request();
  ::tensorflow::serving::ClassificationRequest* release_request();
  void set_allocated_request(::tensorflow::serving::ClassificationRequest* request);
  ::tensorflow::serving::ClassificationRequest* unsafe_arena_release_request();
  void unsafe_arena_set_allocated_request(
      ::tensorflow::serving::ClassificationRequest* request);

  // optional .tensorflow.serving.ClassificationResponse response = 2;
  bool has_response() const;
  void clear_response();
  static const int kResponseFieldNumber = 2;
  private:
  void _slow_mutable_response();
  void _slow_set_allocated_response(
      ::google::protobuf::Arena* message_arena, ::tensorflow::serving::ClassificationResponse** response);
  ::tensorflow::serving::ClassificationResponse* _slow_release_response();
  public:
  const ::tensorflow::serving::ClassificationResponse& response() const;
  ::tensorflow::serving::ClassificationResponse* mutable_response();
  ::tensorflow::serving::ClassificationResponse* release_response();
  void set_allocated_response(::tensorflow::serving::ClassificationResponse* response);
  ::tensorflow::serving::ClassificationResponse* unsafe_arena_release_response();
  void unsafe_arena_set_allocated_response(
      ::tensorflow::serving::ClassificationResponse* response);

  // @@protoc_insertion_point(class_scope:tensorflow.serving.ClassifyLog)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  friend class ::google::protobuf::Arena;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  bool _is_default_instance_;
  ::tensorflow::serving::ClassificationRequest* request_;
  ::tensorflow::serving::ClassificationResponse* response_;
  mutable int _cached_size_;
  friend void  protobuf_AddDesc_tensorflow_5fserving_2fapis_2fprediction_5flog_2eproto();
  friend void protobuf_AssignDesc_tensorflow_5fserving_2fapis_2fprediction_5flog_2eproto();
  friend void protobuf_ShutdownFile_tensorflow_5fserving_2fapis_2fprediction_5flog_2eproto();

  void InitAsDefaultInstance();
  static ClassifyLog* default_instance_;
};
// -------------------------------------------------------------------

class RegressLog : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:tensorflow.serving.RegressLog) */ {
 public:
  RegressLog();
  virtual ~RegressLog();

  RegressLog(const RegressLog& from);

  inline RegressLog& operator=(const RegressLog& from) {
    CopyFrom(from);
    return *this;
  }

  inline ::google::protobuf::Arena* GetArena() const { return GetArenaNoVirtual(); }
  inline void* GetMaybeArenaPointer() const {
    return MaybeArenaPtr();
  }
  static const ::google::protobuf::Descriptor* descriptor();
  static const RegressLog& default_instance();

  void UnsafeArenaSwap(RegressLog* other);
  void Swap(RegressLog* other);

  // implements Message ----------------------------------------------

  inline RegressLog* New() const { return New(NULL); }

  RegressLog* New(::google::protobuf::Arena* arena) const;
  void CopyFrom(const ::google::protobuf::Message& from);
  void MergeFrom(const ::google::protobuf::Message& from);
  void CopyFrom(const RegressLog& from);
  void MergeFrom(const RegressLog& from);
  void Clear();
  bool IsInitialized() const;

  int ByteSize() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* output) const;
  ::google::protobuf::uint8* SerializeWithCachedSizesToArray(::google::protobuf::uint8* output) const {
    return InternalSerializeWithCachedSizesToArray(false, output);
  }
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(RegressLog* other);
  protected:
  explicit RegressLog(::google::protobuf::Arena* arena);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::google::protobuf::Arena* arena);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _internal_metadata_.arena();
  }
  inline void* MaybeArenaPtr() const {
    return _internal_metadata_.raw_arena_ptr();
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // optional .tensorflow.serving.RegressionRequest request = 1;
  bool has_request() const;
  void clear_request();
  static const int kRequestFieldNumber = 1;
  private:
  void _slow_mutable_request();
  void _slow_set_allocated_request(
      ::google::protobuf::Arena* message_arena, ::tensorflow::serving::RegressionRequest** request);
  ::tensorflow::serving::RegressionRequest* _slow_release_request();
  public:
  const ::tensorflow::serving::RegressionRequest& request() const;
  ::tensorflow::serving::RegressionRequest* mutable_request();
  ::tensorflow::serving::RegressionRequest* release_request();
  void set_allocated_request(::tensorflow::serving::RegressionRequest* request);
  ::tensorflow::serving::RegressionRequest* unsafe_arena_release_request();
  void unsafe_arena_set_allocated_request(
      ::tensorflow::serving::RegressionRequest* request);

  // optional .tensorflow.serving.RegressionResponse response = 2;
  bool has_response() const;
  void clear_response();
  static const int kResponseFieldNumber = 2;
  private:
  void _slow_mutable_response();
  void _slow_set_allocated_response(
      ::google::protobuf::Arena* message_arena, ::tensorflow::serving::RegressionResponse** response);
  ::tensorflow::serving::RegressionResponse* _slow_release_response();
  public:
  const ::tensorflow::serving::RegressionResponse& response() const;
  ::tensorflow::serving::RegressionResponse* mutable_response();
  ::tensorflow::serving::RegressionResponse* release_response();
  void set_allocated_response(::tensorflow::serving::RegressionResponse* response);
  ::tensorflow::serving::RegressionResponse* unsafe_arena_release_response();
  void unsafe_arena_set_allocated_response(
      ::tensorflow::serving::RegressionResponse* response);

  // @@protoc_insertion_point(class_scope:tensorflow.serving.RegressLog)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  friend class ::google::protobuf::Arena;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  bool _is_default_instance_;
  ::tensorflow::serving::RegressionRequest* request_;
  ::tensorflow::serving::RegressionResponse* response_;
  mutable int _cached_size_;
  friend void  protobuf_AddDesc_tensorflow_5fserving_2fapis_2fprediction_5flog_2eproto();
  friend void protobuf_AssignDesc_tensorflow_5fserving_2fapis_2fprediction_5flog_2eproto();
  friend void protobuf_ShutdownFile_tensorflow_5fserving_2fapis_2fprediction_5flog_2eproto();

  void InitAsDefaultInstance();
  static RegressLog* default_instance_;
};
// -------------------------------------------------------------------

class PredictLog : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:tensorflow.serving.PredictLog) */ {
 public:
  PredictLog();
  virtual ~PredictLog();

  PredictLog(const PredictLog& from);

  inline PredictLog& operator=(const PredictLog& from) {
    CopyFrom(from);
    return *this;
  }

  inline ::google::protobuf::Arena* GetArena() const { return GetArenaNoVirtual(); }
  inline void* GetMaybeArenaPointer() const {
    return MaybeArenaPtr();
  }
  static const ::google::protobuf::Descriptor* descriptor();
  static const PredictLog& default_instance();

  void UnsafeArenaSwap(PredictLog* other);
  void Swap(PredictLog* other);

  // implements Message ----------------------------------------------

  inline PredictLog* New() const { return New(NULL); }

  PredictLog* New(::google::protobuf::Arena* arena) const;
  void CopyFrom(const ::google::protobuf::Message& from);
  void MergeFrom(const ::google::protobuf::Message& from);
  void CopyFrom(const PredictLog& from);
  void MergeFrom(const PredictLog& from);
  void Clear();
  bool IsInitialized() const;

  int ByteSize() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* output) const;
  ::google::protobuf::uint8* SerializeWithCachedSizesToArray(::google::protobuf::uint8* output) const {
    return InternalSerializeWithCachedSizesToArray(false, output);
  }
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(PredictLog* other);
  protected:
  explicit PredictLog(::google::protobuf::Arena* arena);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::google::protobuf::Arena* arena);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _internal_metadata_.arena();
  }
  inline void* MaybeArenaPtr() const {
    return _internal_metadata_.raw_arena_ptr();
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // optional .tensorflow.serving.PredictRequest request = 1;
  bool has_request() const;
  void clear_request();
  static const int kRequestFieldNumber = 1;
  private:
  void _slow_mutable_request();
  void _slow_set_allocated_request(
      ::google::protobuf::Arena* message_arena, ::tensorflow::serving::PredictRequest** request);
  ::tensorflow::serving::PredictRequest* _slow_release_request();
  public:
  const ::tensorflow::serving::PredictRequest& request() const;
  ::tensorflow::serving::PredictRequest* mutable_request();
  ::tensorflow::serving::PredictRequest* release_request();
  void set_allocated_request(::tensorflow::serving::PredictRequest* request);
  ::tensorflow::serving::PredictRequest* unsafe_arena_release_request();
  void unsafe_arena_set_allocated_request(
      ::tensorflow::serving::PredictRequest* request);

  // optional .tensorflow.serving.PredictResponse response = 2;
  bool has_response() const;
  void clear_response();
  static const int kResponseFieldNumber = 2;
  private:
  void _slow_mutable_response();
  void _slow_set_allocated_response(
      ::google::protobuf::Arena* message_arena, ::tensorflow::serving::PredictResponse** response);
  ::tensorflow::serving::PredictResponse* _slow_release_response();
  public:
  const ::tensorflow::serving::PredictResponse& response() const;
  ::tensorflow::serving::PredictResponse* mutable_response();
  ::tensorflow::serving::PredictResponse* release_response();
  void set_allocated_response(::tensorflow::serving::PredictResponse* response);
  ::tensorflow::serving::PredictResponse* unsafe_arena_release_response();
  void unsafe_arena_set_allocated_response(
      ::tensorflow::serving::PredictResponse* response);

  // @@protoc_insertion_point(class_scope:tensorflow.serving.PredictLog)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  friend class ::google::protobuf::Arena;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  bool _is_default_instance_;
  ::tensorflow::serving::PredictRequest* request_;
  ::tensorflow::serving::PredictResponse* response_;
  mutable int _cached_size_;
  friend void  protobuf_AddDesc_tensorflow_5fserving_2fapis_2fprediction_5flog_2eproto();
  friend void protobuf_AssignDesc_tensorflow_5fserving_2fapis_2fprediction_5flog_2eproto();
  friend void protobuf_ShutdownFile_tensorflow_5fserving_2fapis_2fprediction_5flog_2eproto();

  void InitAsDefaultInstance();
  static PredictLog* default_instance_;
};
// -------------------------------------------------------------------

class MultiInferenceLog : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:tensorflow.serving.MultiInferenceLog) */ {
 public:
  MultiInferenceLog();
  virtual ~MultiInferenceLog();

  MultiInferenceLog(const MultiInferenceLog& from);

  inline MultiInferenceLog& operator=(const MultiInferenceLog& from) {
    CopyFrom(from);
    return *this;
  }

  inline ::google::protobuf::Arena* GetArena() const { return GetArenaNoVirtual(); }
  inline void* GetMaybeArenaPointer() const {
    return MaybeArenaPtr();
  }
  static const ::google::protobuf::Descriptor* descriptor();
  static const MultiInferenceLog& default_instance();

  void UnsafeArenaSwap(MultiInferenceLog* other);
  void Swap(MultiInferenceLog* other);

  // implements Message ----------------------------------------------

  inline MultiInferenceLog* New() const { return New(NULL); }

  MultiInferenceLog* New(::google::protobuf::Arena* arena) const;
  void CopyFrom(const ::google::protobuf::Message& from);
  void MergeFrom(const ::google::protobuf::Message& from);
  void CopyFrom(const MultiInferenceLog& from);
  void MergeFrom(const MultiInferenceLog& from);
  void Clear();
  bool IsInitialized() const;

  int ByteSize() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* output) const;
  ::google::protobuf::uint8* SerializeWithCachedSizesToArray(::google::protobuf::uint8* output) const {
    return InternalSerializeWithCachedSizesToArray(false, output);
  }
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(MultiInferenceLog* other);
  protected:
  explicit MultiInferenceLog(::google::protobuf::Arena* arena);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::google::protobuf::Arena* arena);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _internal_metadata_.arena();
  }
  inline void* MaybeArenaPtr() const {
    return _internal_metadata_.raw_arena_ptr();
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // optional .tensorflow.serving.MultiInferenceRequest request = 1;
  bool has_request() const;
  void clear_request();
  static const int kRequestFieldNumber = 1;
  private:
  void _slow_mutable_request();
  void _slow_set_allocated_request(
      ::google::protobuf::Arena* message_arena, ::tensorflow::serving::MultiInferenceRequest** request);
  ::tensorflow::serving::MultiInferenceRequest* _slow_release_request();
  public:
  const ::tensorflow::serving::MultiInferenceRequest& request() const;
  ::tensorflow::serving::MultiInferenceRequest* mutable_request();
  ::tensorflow::serving::MultiInferenceRequest* release_request();
  void set_allocated_request(::tensorflow::serving::MultiInferenceRequest* request);
  ::tensorflow::serving::MultiInferenceRequest* unsafe_arena_release_request();
  void unsafe_arena_set_allocated_request(
      ::tensorflow::serving::MultiInferenceRequest* request);

  // optional .tensorflow.serving.MultiInferenceResponse response = 2;
  bool has_response() const;
  void clear_response();
  static const int kResponseFieldNumber = 2;
  private:
  void _slow_mutable_response();
  void _slow_set_allocated_response(
      ::google::protobuf::Arena* message_arena, ::tensorflow::serving::MultiInferenceResponse** response);
  ::tensorflow::serving::MultiInferenceResponse* _slow_release_response();
  public:
  const ::tensorflow::serving::MultiInferenceResponse& response() const;
  ::tensorflow::serving::MultiInferenceResponse* mutable_response();
  ::tensorflow::serving::MultiInferenceResponse* release_response();
  void set_allocated_response(::tensorflow::serving::MultiInferenceResponse* response);
  ::tensorflow::serving::MultiInferenceResponse* unsafe_arena_release_response();
  void unsafe_arena_set_allocated_response(
      ::tensorflow::serving::MultiInferenceResponse* response);

  // @@protoc_insertion_point(class_scope:tensorflow.serving.MultiInferenceLog)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  friend class ::google::protobuf::Arena;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  bool _is_default_instance_;
  ::tensorflow::serving::MultiInferenceRequest* request_;
  ::tensorflow::serving::MultiInferenceResponse* response_;
  mutable int _cached_size_;
  friend void  protobuf_AddDesc_tensorflow_5fserving_2fapis_2fprediction_5flog_2eproto();
  friend void protobuf_AssignDesc_tensorflow_5fserving_2fapis_2fprediction_5flog_2eproto();
  friend void protobuf_ShutdownFile_tensorflow_5fserving_2fapis_2fprediction_5flog_2eproto();

  void InitAsDefaultInstance();
  static MultiInferenceLog* default_instance_;
};
// -------------------------------------------------------------------

class SessionRunLog : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:tensorflow.serving.SessionRunLog) */ {
 public:
  SessionRunLog();
  virtual ~SessionRunLog();

  SessionRunLog(const SessionRunLog& from);

  inline SessionRunLog& operator=(const SessionRunLog& from) {
    CopyFrom(from);
    return *this;
  }

  inline ::google::protobuf::Arena* GetArena() const { return GetArenaNoVirtual(); }
  inline void* GetMaybeArenaPointer() const {
    return MaybeArenaPtr();
  }
  static const ::google::protobuf::Descriptor* descriptor();
  static const SessionRunLog& default_instance();

  void UnsafeArenaSwap(SessionRunLog* other);
  void Swap(SessionRunLog* other);

  // implements Message ----------------------------------------------

  inline SessionRunLog* New() const { return New(NULL); }

  SessionRunLog* New(::google::protobuf::Arena* arena) const;
  void CopyFrom(const ::google::protobuf::Message& from);
  void MergeFrom(const ::google::protobuf::Message& from);
  void CopyFrom(const SessionRunLog& from);
  void MergeFrom(const SessionRunLog& from);
  void Clear();
  bool IsInitialized() const;

  int ByteSize() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* output) const;
  ::google::protobuf::uint8* SerializeWithCachedSizesToArray(::google::protobuf::uint8* output) const {
    return InternalSerializeWithCachedSizesToArray(false, output);
  }
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(SessionRunLog* other);
  protected:
  explicit SessionRunLog(::google::protobuf::Arena* arena);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::google::protobuf::Arena* arena);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _internal_metadata_.arena();
  }
  inline void* MaybeArenaPtr() const {
    return _internal_metadata_.raw_arena_ptr();
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // optional .tensorflow.serving.SessionRunRequest request = 1;
  bool has_request() const;
  void clear_request();
  static const int kRequestFieldNumber = 1;
  private:
  void _slow_mutable_request();
  void _slow_set_allocated_request(
      ::google::protobuf::Arena* message_arena, ::tensorflow::serving::SessionRunRequest** request);
  ::tensorflow::serving::SessionRunRequest* _slow_release_request();
  public:
  const ::tensorflow::serving::SessionRunRequest& request() const;
  ::tensorflow::serving::SessionRunRequest* mutable_request();
  ::tensorflow::serving::SessionRunRequest* release_request();
  void set_allocated_request(::tensorflow::serving::SessionRunRequest* request);
  ::tensorflow::serving::SessionRunRequest* unsafe_arena_release_request();
  void unsafe_arena_set_allocated_request(
      ::tensorflow::serving::SessionRunRequest* request);

  // optional .tensorflow.serving.SessionRunResponse response = 2;
  bool has_response() const;
  void clear_response();
  static const int kResponseFieldNumber = 2;
  private:
  void _slow_mutable_response();
  void _slow_set_allocated_response(
      ::google::protobuf::Arena* message_arena, ::tensorflow::serving::SessionRunResponse** response);
  ::tensorflow::serving::SessionRunResponse* _slow_release_response();
  public:
  const ::tensorflow::serving::SessionRunResponse& response() const;
  ::tensorflow::serving::SessionRunResponse* mutable_response();
  ::tensorflow::serving::SessionRunResponse* release_response();
  void set_allocated_response(::tensorflow::serving::SessionRunResponse* response);
  ::tensorflow::serving::SessionRunResponse* unsafe_arena_release_response();
  void unsafe_arena_set_allocated_response(
      ::tensorflow::serving::SessionRunResponse* response);

  // @@protoc_insertion_point(class_scope:tensorflow.serving.SessionRunLog)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  friend class ::google::protobuf::Arena;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  bool _is_default_instance_;
  ::tensorflow::serving::SessionRunRequest* request_;
  ::tensorflow::serving::SessionRunResponse* response_;
  mutable int _cached_size_;
  friend void  protobuf_AddDesc_tensorflow_5fserving_2fapis_2fprediction_5flog_2eproto();
  friend void protobuf_AssignDesc_tensorflow_5fserving_2fapis_2fprediction_5flog_2eproto();
  friend void protobuf_ShutdownFile_tensorflow_5fserving_2fapis_2fprediction_5flog_2eproto();

  void InitAsDefaultInstance();
  static SessionRunLog* default_instance_;
};
// -------------------------------------------------------------------

class PredictionLog : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:tensorflow.serving.PredictionLog) */ {
 public:
  PredictionLog();
  virtual ~PredictionLog();

  PredictionLog(const PredictionLog& from);

  inline PredictionLog& operator=(const PredictionLog& from) {
    CopyFrom(from);
    return *this;
  }

  inline ::google::protobuf::Arena* GetArena() const { return GetArenaNoVirtual(); }
  inline void* GetMaybeArenaPointer() const {
    return MaybeArenaPtr();
  }
  static const ::google::protobuf::Descriptor* descriptor();
  static const PredictionLog& default_instance();

  enum LogTypeCase {
    kClassifyLog = 2,
    kRegressLog = 3,
    kPredictLog = 6,
    kMultiInferenceLog = 4,
    kSessionRunLog = 5,
    LOG_TYPE_NOT_SET = 0,
  };

  void UnsafeArenaSwap(PredictionLog* other);
  void Swap(PredictionLog* other);

  // implements Message ----------------------------------------------

  inline PredictionLog* New() const { return New(NULL); }

  PredictionLog* New(::google::protobuf::Arena* arena) const;
  void CopyFrom(const ::google::protobuf::Message& from);
  void MergeFrom(const ::google::protobuf::Message& from);
  void CopyFrom(const PredictionLog& from);
  void MergeFrom(const PredictionLog& from);
  void Clear();
  bool IsInitialized() const;

  int ByteSize() const;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input);
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* output) const;
  ::google::protobuf::uint8* SerializeWithCachedSizesToArray(::google::protobuf::uint8* output) const {
    return InternalSerializeWithCachedSizesToArray(false, output);
  }
  int GetCachedSize() const { return _cached_size_; }
  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const;
  void InternalSwap(PredictionLog* other);
  protected:
  explicit PredictionLog(::google::protobuf::Arena* arena);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::google::protobuf::Arena* arena);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return _internal_metadata_.arena();
  }
  inline void* MaybeArenaPtr() const {
    return _internal_metadata_.raw_arena_ptr();
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // optional .tensorflow.serving.LogMetadata log_metadata = 1;
  bool has_log_metadata() const;
  void clear_log_metadata();
  static const int kLogMetadataFieldNumber = 1;
  private:
  void _slow_mutable_log_metadata();
  void _slow_set_allocated_log_metadata(
      ::google::protobuf::Arena* message_arena, ::tensorflow::serving::LogMetadata** log_metadata);
  ::tensorflow::serving::LogMetadata* _slow_release_log_metadata();
  public:
  const ::tensorflow::serving::LogMetadata& log_metadata() const;
  ::tensorflow::serving::LogMetadata* mutable_log_metadata();
  ::tensorflow::serving::LogMetadata* release_log_metadata();
  void set_allocated_log_metadata(::tensorflow::serving::LogMetadata* log_metadata);
  ::tensorflow::serving::LogMetadata* unsafe_arena_release_log_metadata();
  void unsafe_arena_set_allocated_log_metadata(
      ::tensorflow::serving::LogMetadata* log_metadata);

  // optional .tensorflow.serving.ClassifyLog classify_log = 2;
  bool has_classify_log() const;
  void clear_classify_log();
  static const int kClassifyLogFieldNumber = 2;
  private:
  void _slow_mutable_classify_log();
  void _slow_set_allocated_classify_log(
      ::google::protobuf::Arena* message_arena, ::tensorflow::serving::ClassifyLog** classify_log);
  ::tensorflow::serving::ClassifyLog* _slow_release_classify_log();
  public:
  const ::tensorflow::serving::ClassifyLog& classify_log() const;
  ::tensorflow::serving::ClassifyLog* mutable_classify_log();
  ::tensorflow::serving::ClassifyLog* release_classify_log();
  void set_allocated_classify_log(::tensorflow::serving::ClassifyLog* classify_log);
  ::tensorflow::serving::ClassifyLog* unsafe_arena_release_classify_log();
  void unsafe_arena_set_allocated_classify_log(
      ::tensorflow::serving::ClassifyLog* classify_log);

  // optional .tensorflow.serving.RegressLog regress_log = 3;
  bool has_regress_log() const;
  void clear_regress_log();
  static const int kRegressLogFieldNumber = 3;
  private:
  void _slow_mutable_regress_log();
  void _slow_set_allocated_regress_log(
      ::google::protobuf::Arena* message_arena, ::tensorflow::serving::RegressLog** regress_log);
  ::tensorflow::serving::RegressLog* _slow_release_regress_log();
  public:
  const ::tensorflow::serving::RegressLog& regress_log() const;
  ::tensorflow::serving::RegressLog* mutable_regress_log();
  ::tensorflow::serving::RegressLog* release_regress_log();
  void set_allocated_regress_log(::tensorflow::serving::RegressLog* regress_log);
  ::tensorflow::serving::RegressLog* unsafe_arena_release_regress_log();
  void unsafe_arena_set_allocated_regress_log(
      ::tensorflow::serving::RegressLog* regress_log);

  // optional .tensorflow.serving.PredictLog predict_log = 6;
  bool has_predict_log() const;
  void clear_predict_log();
  static const int kPredictLogFieldNumber = 6;
  private:
  void _slow_mutable_predict_log();
  void _slow_set_allocated_predict_log(
      ::google::protobuf::Arena* message_arena, ::tensorflow::serving::PredictLog** predict_log);
  ::tensorflow::serving::PredictLog* _slow_release_predict_log();
  public:
  const ::tensorflow::serving::PredictLog& predict_log() const;
  ::tensorflow::serving::PredictLog* mutable_predict_log();
  ::tensorflow::serving::PredictLog* release_predict_log();
  void set_allocated_predict_log(::tensorflow::serving::PredictLog* predict_log);
  ::tensorflow::serving::PredictLog* unsafe_arena_release_predict_log();
  void unsafe_arena_set_allocated_predict_log(
      ::tensorflow::serving::PredictLog* predict_log);

  // optional .tensorflow.serving.MultiInferenceLog multi_inference_log = 4;
  bool has_multi_inference_log() const;
  void clear_multi_inference_log();
  static const int kMultiInferenceLogFieldNumber = 4;
  private:
  void _slow_mutable_multi_inference_log();
  void _slow_set_allocated_multi_inference_log(
      ::google::protobuf::Arena* message_arena, ::tensorflow::serving::MultiInferenceLog** multi_inference_log);
  ::tensorflow::serving::MultiInferenceLog* _slow_release_multi_inference_log();
  public:
  const ::tensorflow::serving::MultiInferenceLog& multi_inference_log() const;
  ::tensorflow::serving::MultiInferenceLog* mutable_multi_inference_log();
  ::tensorflow::serving::MultiInferenceLog* release_multi_inference_log();
  void set_allocated_multi_inference_log(::tensorflow::serving::MultiInferenceLog* multi_inference_log);
  ::tensorflow::serving::MultiInferenceLog* unsafe_arena_release_multi_inference_log();
  void unsafe_arena_set_allocated_multi_inference_log(
      ::tensorflow::serving::MultiInferenceLog* multi_inference_log);

  // optional .tensorflow.serving.SessionRunLog session_run_log = 5;
  bool has_session_run_log() const;
  void clear_session_run_log();
  static const int kSessionRunLogFieldNumber = 5;
  private:
  void _slow_mutable_session_run_log();
  void _slow_set_allocated_session_run_log(
      ::google::protobuf::Arena* message_arena, ::tensorflow::serving::SessionRunLog** session_run_log);
  ::tensorflow::serving::SessionRunLog* _slow_release_session_run_log();
  public:
  const ::tensorflow::serving::SessionRunLog& session_run_log() const;
  ::tensorflow::serving::SessionRunLog* mutable_session_run_log();
  ::tensorflow::serving::SessionRunLog* release_session_run_log();
  void set_allocated_session_run_log(::tensorflow::serving::SessionRunLog* session_run_log);
  ::tensorflow::serving::SessionRunLog* unsafe_arena_release_session_run_log();
  void unsafe_arena_set_allocated_session_run_log(
      ::tensorflow::serving::SessionRunLog* session_run_log);

  LogTypeCase log_type_case() const;
  // @@protoc_insertion_point(class_scope:tensorflow.serving.PredictionLog)
 private:
  inline void set_has_classify_log();
  inline void set_has_regress_log();
  inline void set_has_predict_log();
  inline void set_has_multi_inference_log();
  inline void set_has_session_run_log();

  inline bool has_log_type() const;
  void clear_log_type();
  inline void clear_has_log_type();

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  friend class ::google::protobuf::Arena;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  bool _is_default_instance_;
  ::tensorflow::serving::LogMetadata* log_metadata_;
  union LogTypeUnion {
    LogTypeUnion() {}
    ::tensorflow::serving::ClassifyLog* classify_log_;
    ::tensorflow::serving::RegressLog* regress_log_;
    ::tensorflow::serving::PredictLog* predict_log_;
    ::tensorflow::serving::MultiInferenceLog* multi_inference_log_;
    ::tensorflow::serving::SessionRunLog* session_run_log_;
  } log_type_;
  mutable int _cached_size_;
  ::google::protobuf::uint32 _oneof_case_[1];

  friend void  protobuf_AddDesc_tensorflow_5fserving_2fapis_2fprediction_5flog_2eproto();
  friend void protobuf_AssignDesc_tensorflow_5fserving_2fapis_2fprediction_5flog_2eproto();
  friend void protobuf_ShutdownFile_tensorflow_5fserving_2fapis_2fprediction_5flog_2eproto();

  void InitAsDefaultInstance();
  static PredictionLog* default_instance_;
};
// ===================================================================


// ===================================================================

#if !PROTOBUF_INLINE_NOT_IN_HEADERS
// ClassifyLog

// optional .tensorflow.serving.ClassificationRequest request = 1;
inline bool ClassifyLog::has_request() const {
  return !_is_default_instance_ && request_ != NULL;
}
inline void ClassifyLog::clear_request() {
  if (GetArenaNoVirtual() == NULL && request_ != NULL) delete request_;
  request_ = NULL;
}
inline const ::tensorflow::serving::ClassificationRequest& ClassifyLog::request() const {
  // @@protoc_insertion_point(field_get:tensorflow.serving.ClassifyLog.request)
  return request_ != NULL ? *request_ : *default_instance_->request_;
}
inline ::tensorflow::serving::ClassificationRequest* ClassifyLog::mutable_request() {
  
  if (request_ == NULL) {
    _slow_mutable_request();
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.serving.ClassifyLog.request)
  return request_;
}
inline ::tensorflow::serving::ClassificationRequest* ClassifyLog::release_request() {
  // @@protoc_insertion_point(field_release:tensorflow.serving.ClassifyLog.request)
  
  if (GetArenaNoVirtual() != NULL) {
    return _slow_release_request();
  } else {
    ::tensorflow::serving::ClassificationRequest* temp = request_;
    request_ = NULL;
    return temp;
  }
}
inline  void ClassifyLog::set_allocated_request(::tensorflow::serving::ClassificationRequest* request) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete request_;
  }
  if (request != NULL) {
    _slow_set_allocated_request(message_arena, &request);
  }
  request_ = request;
  if (request) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:tensorflow.serving.ClassifyLog.request)
}

// optional .tensorflow.serving.ClassificationResponse response = 2;
inline bool ClassifyLog::has_response() const {
  return !_is_default_instance_ && response_ != NULL;
}
inline void ClassifyLog::clear_response() {
  if (GetArenaNoVirtual() == NULL && response_ != NULL) delete response_;
  response_ = NULL;
}
inline const ::tensorflow::serving::ClassificationResponse& ClassifyLog::response() const {
  // @@protoc_insertion_point(field_get:tensorflow.serving.ClassifyLog.response)
  return response_ != NULL ? *response_ : *default_instance_->response_;
}
inline ::tensorflow::serving::ClassificationResponse* ClassifyLog::mutable_response() {
  
  if (response_ == NULL) {
    _slow_mutable_response();
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.serving.ClassifyLog.response)
  return response_;
}
inline ::tensorflow::serving::ClassificationResponse* ClassifyLog::release_response() {
  // @@protoc_insertion_point(field_release:tensorflow.serving.ClassifyLog.response)
  
  if (GetArenaNoVirtual() != NULL) {
    return _slow_release_response();
  } else {
    ::tensorflow::serving::ClassificationResponse* temp = response_;
    response_ = NULL;
    return temp;
  }
}
inline  void ClassifyLog::set_allocated_response(::tensorflow::serving::ClassificationResponse* response) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete response_;
  }
  if (response != NULL) {
    _slow_set_allocated_response(message_arena, &response);
  }
  response_ = response;
  if (response) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:tensorflow.serving.ClassifyLog.response)
}

// -------------------------------------------------------------------

// RegressLog

// optional .tensorflow.serving.RegressionRequest request = 1;
inline bool RegressLog::has_request() const {
  return !_is_default_instance_ && request_ != NULL;
}
inline void RegressLog::clear_request() {
  if (GetArenaNoVirtual() == NULL && request_ != NULL) delete request_;
  request_ = NULL;
}
inline const ::tensorflow::serving::RegressionRequest& RegressLog::request() const {
  // @@protoc_insertion_point(field_get:tensorflow.serving.RegressLog.request)
  return request_ != NULL ? *request_ : *default_instance_->request_;
}
inline ::tensorflow::serving::RegressionRequest* RegressLog::mutable_request() {
  
  if (request_ == NULL) {
    _slow_mutable_request();
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.serving.RegressLog.request)
  return request_;
}
inline ::tensorflow::serving::RegressionRequest* RegressLog::release_request() {
  // @@protoc_insertion_point(field_release:tensorflow.serving.RegressLog.request)
  
  if (GetArenaNoVirtual() != NULL) {
    return _slow_release_request();
  } else {
    ::tensorflow::serving::RegressionRequest* temp = request_;
    request_ = NULL;
    return temp;
  }
}
inline  void RegressLog::set_allocated_request(::tensorflow::serving::RegressionRequest* request) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete request_;
  }
  if (request != NULL) {
    _slow_set_allocated_request(message_arena, &request);
  }
  request_ = request;
  if (request) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:tensorflow.serving.RegressLog.request)
}

// optional .tensorflow.serving.RegressionResponse response = 2;
inline bool RegressLog::has_response() const {
  return !_is_default_instance_ && response_ != NULL;
}
inline void RegressLog::clear_response() {
  if (GetArenaNoVirtual() == NULL && response_ != NULL) delete response_;
  response_ = NULL;
}
inline const ::tensorflow::serving::RegressionResponse& RegressLog::response() const {
  // @@protoc_insertion_point(field_get:tensorflow.serving.RegressLog.response)
  return response_ != NULL ? *response_ : *default_instance_->response_;
}
inline ::tensorflow::serving::RegressionResponse* RegressLog::mutable_response() {
  
  if (response_ == NULL) {
    _slow_mutable_response();
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.serving.RegressLog.response)
  return response_;
}
inline ::tensorflow::serving::RegressionResponse* RegressLog::release_response() {
  // @@protoc_insertion_point(field_release:tensorflow.serving.RegressLog.response)
  
  if (GetArenaNoVirtual() != NULL) {
    return _slow_release_response();
  } else {
    ::tensorflow::serving::RegressionResponse* temp = response_;
    response_ = NULL;
    return temp;
  }
}
inline  void RegressLog::set_allocated_response(::tensorflow::serving::RegressionResponse* response) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete response_;
  }
  if (response != NULL) {
    _slow_set_allocated_response(message_arena, &response);
  }
  response_ = response;
  if (response) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:tensorflow.serving.RegressLog.response)
}

// -------------------------------------------------------------------

// PredictLog

// optional .tensorflow.serving.PredictRequest request = 1;
inline bool PredictLog::has_request() const {
  return !_is_default_instance_ && request_ != NULL;
}
inline void PredictLog::clear_request() {
  if (GetArenaNoVirtual() == NULL && request_ != NULL) delete request_;
  request_ = NULL;
}
inline const ::tensorflow::serving::PredictRequest& PredictLog::request() const {
  // @@protoc_insertion_point(field_get:tensorflow.serving.PredictLog.request)
  return request_ != NULL ? *request_ : *default_instance_->request_;
}
inline ::tensorflow::serving::PredictRequest* PredictLog::mutable_request() {
  
  if (request_ == NULL) {
    _slow_mutable_request();
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.serving.PredictLog.request)
  return request_;
}
inline ::tensorflow::serving::PredictRequest* PredictLog::release_request() {
  // @@protoc_insertion_point(field_release:tensorflow.serving.PredictLog.request)
  
  if (GetArenaNoVirtual() != NULL) {
    return _slow_release_request();
  } else {
    ::tensorflow::serving::PredictRequest* temp = request_;
    request_ = NULL;
    return temp;
  }
}
inline  void PredictLog::set_allocated_request(::tensorflow::serving::PredictRequest* request) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete request_;
  }
  if (request != NULL) {
    _slow_set_allocated_request(message_arena, &request);
  }
  request_ = request;
  if (request) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:tensorflow.serving.PredictLog.request)
}

// optional .tensorflow.serving.PredictResponse response = 2;
inline bool PredictLog::has_response() const {
  return !_is_default_instance_ && response_ != NULL;
}
inline void PredictLog::clear_response() {
  if (GetArenaNoVirtual() == NULL && response_ != NULL) delete response_;
  response_ = NULL;
}
inline const ::tensorflow::serving::PredictResponse& PredictLog::response() const {
  // @@protoc_insertion_point(field_get:tensorflow.serving.PredictLog.response)
  return response_ != NULL ? *response_ : *default_instance_->response_;
}
inline ::tensorflow::serving::PredictResponse* PredictLog::mutable_response() {
  
  if (response_ == NULL) {
    _slow_mutable_response();
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.serving.PredictLog.response)
  return response_;
}
inline ::tensorflow::serving::PredictResponse* PredictLog::release_response() {
  // @@protoc_insertion_point(field_release:tensorflow.serving.PredictLog.response)
  
  if (GetArenaNoVirtual() != NULL) {
    return _slow_release_response();
  } else {
    ::tensorflow::serving::PredictResponse* temp = response_;
    response_ = NULL;
    return temp;
  }
}
inline  void PredictLog::set_allocated_response(::tensorflow::serving::PredictResponse* response) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete response_;
  }
  if (response != NULL) {
    _slow_set_allocated_response(message_arena, &response);
  }
  response_ = response;
  if (response) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:tensorflow.serving.PredictLog.response)
}

// -------------------------------------------------------------------

// MultiInferenceLog

// optional .tensorflow.serving.MultiInferenceRequest request = 1;
inline bool MultiInferenceLog::has_request() const {
  return !_is_default_instance_ && request_ != NULL;
}
inline void MultiInferenceLog::clear_request() {
  if (GetArenaNoVirtual() == NULL && request_ != NULL) delete request_;
  request_ = NULL;
}
inline const ::tensorflow::serving::MultiInferenceRequest& MultiInferenceLog::request() const {
  // @@protoc_insertion_point(field_get:tensorflow.serving.MultiInferenceLog.request)
  return request_ != NULL ? *request_ : *default_instance_->request_;
}
inline ::tensorflow::serving::MultiInferenceRequest* MultiInferenceLog::mutable_request() {
  
  if (request_ == NULL) {
    _slow_mutable_request();
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.serving.MultiInferenceLog.request)
  return request_;
}
inline ::tensorflow::serving::MultiInferenceRequest* MultiInferenceLog::release_request() {
  // @@protoc_insertion_point(field_release:tensorflow.serving.MultiInferenceLog.request)
  
  if (GetArenaNoVirtual() != NULL) {
    return _slow_release_request();
  } else {
    ::tensorflow::serving::MultiInferenceRequest* temp = request_;
    request_ = NULL;
    return temp;
  }
}
inline  void MultiInferenceLog::set_allocated_request(::tensorflow::serving::MultiInferenceRequest* request) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete request_;
  }
  if (request != NULL) {
    _slow_set_allocated_request(message_arena, &request);
  }
  request_ = request;
  if (request) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:tensorflow.serving.MultiInferenceLog.request)
}

// optional .tensorflow.serving.MultiInferenceResponse response = 2;
inline bool MultiInferenceLog::has_response() const {
  return !_is_default_instance_ && response_ != NULL;
}
inline void MultiInferenceLog::clear_response() {
  if (GetArenaNoVirtual() == NULL && response_ != NULL) delete response_;
  response_ = NULL;
}
inline const ::tensorflow::serving::MultiInferenceResponse& MultiInferenceLog::response() const {
  // @@protoc_insertion_point(field_get:tensorflow.serving.MultiInferenceLog.response)
  return response_ != NULL ? *response_ : *default_instance_->response_;
}
inline ::tensorflow::serving::MultiInferenceResponse* MultiInferenceLog::mutable_response() {
  
  if (response_ == NULL) {
    _slow_mutable_response();
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.serving.MultiInferenceLog.response)
  return response_;
}
inline ::tensorflow::serving::MultiInferenceResponse* MultiInferenceLog::release_response() {
  // @@protoc_insertion_point(field_release:tensorflow.serving.MultiInferenceLog.response)
  
  if (GetArenaNoVirtual() != NULL) {
    return _slow_release_response();
  } else {
    ::tensorflow::serving::MultiInferenceResponse* temp = response_;
    response_ = NULL;
    return temp;
  }
}
inline  void MultiInferenceLog::set_allocated_response(::tensorflow::serving::MultiInferenceResponse* response) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete response_;
  }
  if (response != NULL) {
    _slow_set_allocated_response(message_arena, &response);
  }
  response_ = response;
  if (response) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:tensorflow.serving.MultiInferenceLog.response)
}

// -------------------------------------------------------------------

// SessionRunLog

// optional .tensorflow.serving.SessionRunRequest request = 1;
inline bool SessionRunLog::has_request() const {
  return !_is_default_instance_ && request_ != NULL;
}
inline void SessionRunLog::clear_request() {
  if (GetArenaNoVirtual() == NULL && request_ != NULL) delete request_;
  request_ = NULL;
}
inline const ::tensorflow::serving::SessionRunRequest& SessionRunLog::request() const {
  // @@protoc_insertion_point(field_get:tensorflow.serving.SessionRunLog.request)
  return request_ != NULL ? *request_ : *default_instance_->request_;
}
inline ::tensorflow::serving::SessionRunRequest* SessionRunLog::mutable_request() {
  
  if (request_ == NULL) {
    _slow_mutable_request();
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.serving.SessionRunLog.request)
  return request_;
}
inline ::tensorflow::serving::SessionRunRequest* SessionRunLog::release_request() {
  // @@protoc_insertion_point(field_release:tensorflow.serving.SessionRunLog.request)
  
  if (GetArenaNoVirtual() != NULL) {
    return _slow_release_request();
  } else {
    ::tensorflow::serving::SessionRunRequest* temp = request_;
    request_ = NULL;
    return temp;
  }
}
inline  void SessionRunLog::set_allocated_request(::tensorflow::serving::SessionRunRequest* request) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete request_;
  }
  if (request != NULL) {
    _slow_set_allocated_request(message_arena, &request);
  }
  request_ = request;
  if (request) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:tensorflow.serving.SessionRunLog.request)
}

// optional .tensorflow.serving.SessionRunResponse response = 2;
inline bool SessionRunLog::has_response() const {
  return !_is_default_instance_ && response_ != NULL;
}
inline void SessionRunLog::clear_response() {
  if (GetArenaNoVirtual() == NULL && response_ != NULL) delete response_;
  response_ = NULL;
}
inline const ::tensorflow::serving::SessionRunResponse& SessionRunLog::response() const {
  // @@protoc_insertion_point(field_get:tensorflow.serving.SessionRunLog.response)
  return response_ != NULL ? *response_ : *default_instance_->response_;
}
inline ::tensorflow::serving::SessionRunResponse* SessionRunLog::mutable_response() {
  
  if (response_ == NULL) {
    _slow_mutable_response();
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.serving.SessionRunLog.response)
  return response_;
}
inline ::tensorflow::serving::SessionRunResponse* SessionRunLog::release_response() {
  // @@protoc_insertion_point(field_release:tensorflow.serving.SessionRunLog.response)
  
  if (GetArenaNoVirtual() != NULL) {
    return _slow_release_response();
  } else {
    ::tensorflow::serving::SessionRunResponse* temp = response_;
    response_ = NULL;
    return temp;
  }
}
inline  void SessionRunLog::set_allocated_response(::tensorflow::serving::SessionRunResponse* response) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete response_;
  }
  if (response != NULL) {
    _slow_set_allocated_response(message_arena, &response);
  }
  response_ = response;
  if (response) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:tensorflow.serving.SessionRunLog.response)
}

// -------------------------------------------------------------------

// PredictionLog

// optional .tensorflow.serving.LogMetadata log_metadata = 1;
inline bool PredictionLog::has_log_metadata() const {
  return !_is_default_instance_ && log_metadata_ != NULL;
}
inline void PredictionLog::clear_log_metadata() {
  if (GetArenaNoVirtual() == NULL && log_metadata_ != NULL) delete log_metadata_;
  log_metadata_ = NULL;
}
inline const ::tensorflow::serving::LogMetadata& PredictionLog::log_metadata() const {
  // @@protoc_insertion_point(field_get:tensorflow.serving.PredictionLog.log_metadata)
  return log_metadata_ != NULL ? *log_metadata_ : *default_instance_->log_metadata_;
}
inline ::tensorflow::serving::LogMetadata* PredictionLog::mutable_log_metadata() {
  
  if (log_metadata_ == NULL) {
    _slow_mutable_log_metadata();
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.serving.PredictionLog.log_metadata)
  return log_metadata_;
}
inline ::tensorflow::serving::LogMetadata* PredictionLog::release_log_metadata() {
  // @@protoc_insertion_point(field_release:tensorflow.serving.PredictionLog.log_metadata)
  
  if (GetArenaNoVirtual() != NULL) {
    return _slow_release_log_metadata();
  } else {
    ::tensorflow::serving::LogMetadata* temp = log_metadata_;
    log_metadata_ = NULL;
    return temp;
  }
}
inline  void PredictionLog::set_allocated_log_metadata(::tensorflow::serving::LogMetadata* log_metadata) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete log_metadata_;
  }
  if (log_metadata != NULL) {
    _slow_set_allocated_log_metadata(message_arena, &log_metadata);
  }
  log_metadata_ = log_metadata;
  if (log_metadata) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:tensorflow.serving.PredictionLog.log_metadata)
}

// optional .tensorflow.serving.ClassifyLog classify_log = 2;
inline bool PredictionLog::has_classify_log() const {
  return log_type_case() == kClassifyLog;
}
inline void PredictionLog::set_has_classify_log() {
  _oneof_case_[0] = kClassifyLog;
}
inline void PredictionLog::clear_classify_log() {
  if (has_classify_log()) {
    if (GetArenaNoVirtual() == NULL) {
      delete log_type_.classify_log_;
    }
    clear_has_log_type();
  }
}
inline  const ::tensorflow::serving::ClassifyLog& PredictionLog::classify_log() const {
  // @@protoc_insertion_point(field_get:tensorflow.serving.PredictionLog.classify_log)
  return has_classify_log()
      ? *log_type_.classify_log_
      : ::tensorflow::serving::ClassifyLog::default_instance();
}
inline ::tensorflow::serving::ClassifyLog* PredictionLog::mutable_classify_log() {
  if (!has_classify_log()) {
    clear_log_type();
    set_has_classify_log();
    log_type_.classify_log_ = 
      ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::ClassifyLog >(
      GetArenaNoVirtual());
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.serving.PredictionLog.classify_log)
  return log_type_.classify_log_;
}
inline ::tensorflow::serving::ClassifyLog* PredictionLog::release_classify_log() {
  // @@protoc_insertion_point(field_release:tensorflow.serving.PredictionLog.classify_log)
  if (has_classify_log()) {
    clear_has_log_type();
    if (GetArenaNoVirtual() != NULL) {
      ::tensorflow::serving::ClassifyLog* temp = new ::tensorflow::serving::ClassifyLog;
      temp->MergeFrom(*log_type_.classify_log_);
      log_type_.classify_log_ = NULL;
      return temp;
    } else {
      ::tensorflow::serving::ClassifyLog* temp = log_type_.classify_log_;
      log_type_.classify_log_ = NULL;
      return temp;
    }
  } else {
    return NULL;
  }
}
inline void PredictionLog::set_allocated_classify_log(::tensorflow::serving::ClassifyLog* classify_log) {
  clear_log_type();
  if (classify_log) {
    if (GetArenaNoVirtual() != NULL &&
        ::google::protobuf::Arena::GetArena(classify_log) == NULL) {
      GetArenaNoVirtual()->Own(classify_log);
    } else if (GetArenaNoVirtual() !=
               ::google::protobuf::Arena::GetArena(classify_log)) {
      ::tensorflow::serving::ClassifyLog* new_classify_log = 
          ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::ClassifyLog >(
          GetArenaNoVirtual());
      new_classify_log->CopyFrom(*classify_log);
      classify_log = new_classify_log;
    }
    set_has_classify_log();
    log_type_.classify_log_ = classify_log;
  }
  // @@protoc_insertion_point(field_set_allocated:tensorflow.serving.PredictionLog.classify_log)
}
inline  ::tensorflow::serving::ClassifyLog* PredictionLog::unsafe_arena_release_classify_log() {
  // @@protoc_insertion_point(field_unsafe_arena_release:tensorflow.serving.PredictionLog.classify_log)
  if (has_classify_log()) {
    clear_has_log_type();
    ::tensorflow::serving::ClassifyLog* temp = log_type_.classify_log_;
    log_type_.classify_log_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline  void PredictionLog::unsafe_arena_set_allocated_classify_log(::tensorflow::serving::ClassifyLog* classify_log) {
  clear_log_type();
  if (classify_log) {
    set_has_classify_log();
    log_type_.classify_log_ = classify_log;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:tensorflow.serving.PredictionLog.classify_log)
}

// optional .tensorflow.serving.RegressLog regress_log = 3;
inline bool PredictionLog::has_regress_log() const {
  return log_type_case() == kRegressLog;
}
inline void PredictionLog::set_has_regress_log() {
  _oneof_case_[0] = kRegressLog;
}
inline void PredictionLog::clear_regress_log() {
  if (has_regress_log()) {
    if (GetArenaNoVirtual() == NULL) {
      delete log_type_.regress_log_;
    }
    clear_has_log_type();
  }
}
inline  const ::tensorflow::serving::RegressLog& PredictionLog::regress_log() const {
  // @@protoc_insertion_point(field_get:tensorflow.serving.PredictionLog.regress_log)
  return has_regress_log()
      ? *log_type_.regress_log_
      : ::tensorflow::serving::RegressLog::default_instance();
}
inline ::tensorflow::serving::RegressLog* PredictionLog::mutable_regress_log() {
  if (!has_regress_log()) {
    clear_log_type();
    set_has_regress_log();
    log_type_.regress_log_ = 
      ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::RegressLog >(
      GetArenaNoVirtual());
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.serving.PredictionLog.regress_log)
  return log_type_.regress_log_;
}
inline ::tensorflow::serving::RegressLog* PredictionLog::release_regress_log() {
  // @@protoc_insertion_point(field_release:tensorflow.serving.PredictionLog.regress_log)
  if (has_regress_log()) {
    clear_has_log_type();
    if (GetArenaNoVirtual() != NULL) {
      ::tensorflow::serving::RegressLog* temp = new ::tensorflow::serving::RegressLog;
      temp->MergeFrom(*log_type_.regress_log_);
      log_type_.regress_log_ = NULL;
      return temp;
    } else {
      ::tensorflow::serving::RegressLog* temp = log_type_.regress_log_;
      log_type_.regress_log_ = NULL;
      return temp;
    }
  } else {
    return NULL;
  }
}
inline void PredictionLog::set_allocated_regress_log(::tensorflow::serving::RegressLog* regress_log) {
  clear_log_type();
  if (regress_log) {
    if (GetArenaNoVirtual() != NULL &&
        ::google::protobuf::Arena::GetArena(regress_log) == NULL) {
      GetArenaNoVirtual()->Own(regress_log);
    } else if (GetArenaNoVirtual() !=
               ::google::protobuf::Arena::GetArena(regress_log)) {
      ::tensorflow::serving::RegressLog* new_regress_log = 
          ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::RegressLog >(
          GetArenaNoVirtual());
      new_regress_log->CopyFrom(*regress_log);
      regress_log = new_regress_log;
    }
    set_has_regress_log();
    log_type_.regress_log_ = regress_log;
  }
  // @@protoc_insertion_point(field_set_allocated:tensorflow.serving.PredictionLog.regress_log)
}
inline  ::tensorflow::serving::RegressLog* PredictionLog::unsafe_arena_release_regress_log() {
  // @@protoc_insertion_point(field_unsafe_arena_release:tensorflow.serving.PredictionLog.regress_log)
  if (has_regress_log()) {
    clear_has_log_type();
    ::tensorflow::serving::RegressLog* temp = log_type_.regress_log_;
    log_type_.regress_log_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline  void PredictionLog::unsafe_arena_set_allocated_regress_log(::tensorflow::serving::RegressLog* regress_log) {
  clear_log_type();
  if (regress_log) {
    set_has_regress_log();
    log_type_.regress_log_ = regress_log;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:tensorflow.serving.PredictionLog.regress_log)
}

// optional .tensorflow.serving.PredictLog predict_log = 6;
inline bool PredictionLog::has_predict_log() const {
  return log_type_case() == kPredictLog;
}
inline void PredictionLog::set_has_predict_log() {
  _oneof_case_[0] = kPredictLog;
}
inline void PredictionLog::clear_predict_log() {
  if (has_predict_log()) {
    if (GetArenaNoVirtual() == NULL) {
      delete log_type_.predict_log_;
    }
    clear_has_log_type();
  }
}
inline  const ::tensorflow::serving::PredictLog& PredictionLog::predict_log() const {
  // @@protoc_insertion_point(field_get:tensorflow.serving.PredictionLog.predict_log)
  return has_predict_log()
      ? *log_type_.predict_log_
      : ::tensorflow::serving::PredictLog::default_instance();
}
inline ::tensorflow::serving::PredictLog* PredictionLog::mutable_predict_log() {
  if (!has_predict_log()) {
    clear_log_type();
    set_has_predict_log();
    log_type_.predict_log_ = 
      ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::PredictLog >(
      GetArenaNoVirtual());
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.serving.PredictionLog.predict_log)
  return log_type_.predict_log_;
}
inline ::tensorflow::serving::PredictLog* PredictionLog::release_predict_log() {
  // @@protoc_insertion_point(field_release:tensorflow.serving.PredictionLog.predict_log)
  if (has_predict_log()) {
    clear_has_log_type();
    if (GetArenaNoVirtual() != NULL) {
      ::tensorflow::serving::PredictLog* temp = new ::tensorflow::serving::PredictLog;
      temp->MergeFrom(*log_type_.predict_log_);
      log_type_.predict_log_ = NULL;
      return temp;
    } else {
      ::tensorflow::serving::PredictLog* temp = log_type_.predict_log_;
      log_type_.predict_log_ = NULL;
      return temp;
    }
  } else {
    return NULL;
  }
}
inline void PredictionLog::set_allocated_predict_log(::tensorflow::serving::PredictLog* predict_log) {
  clear_log_type();
  if (predict_log) {
    if (GetArenaNoVirtual() != NULL &&
        ::google::protobuf::Arena::GetArena(predict_log) == NULL) {
      GetArenaNoVirtual()->Own(predict_log);
    } else if (GetArenaNoVirtual() !=
               ::google::protobuf::Arena::GetArena(predict_log)) {
      ::tensorflow::serving::PredictLog* new_predict_log = 
          ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::PredictLog >(
          GetArenaNoVirtual());
      new_predict_log->CopyFrom(*predict_log);
      predict_log = new_predict_log;
    }
    set_has_predict_log();
    log_type_.predict_log_ = predict_log;
  }
  // @@protoc_insertion_point(field_set_allocated:tensorflow.serving.PredictionLog.predict_log)
}
inline  ::tensorflow::serving::PredictLog* PredictionLog::unsafe_arena_release_predict_log() {
  // @@protoc_insertion_point(field_unsafe_arena_release:tensorflow.serving.PredictionLog.predict_log)
  if (has_predict_log()) {
    clear_has_log_type();
    ::tensorflow::serving::PredictLog* temp = log_type_.predict_log_;
    log_type_.predict_log_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline  void PredictionLog::unsafe_arena_set_allocated_predict_log(::tensorflow::serving::PredictLog* predict_log) {
  clear_log_type();
  if (predict_log) {
    set_has_predict_log();
    log_type_.predict_log_ = predict_log;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:tensorflow.serving.PredictionLog.predict_log)
}

// optional .tensorflow.serving.MultiInferenceLog multi_inference_log = 4;
inline bool PredictionLog::has_multi_inference_log() const {
  return log_type_case() == kMultiInferenceLog;
}
inline void PredictionLog::set_has_multi_inference_log() {
  _oneof_case_[0] = kMultiInferenceLog;
}
inline void PredictionLog::clear_multi_inference_log() {
  if (has_multi_inference_log()) {
    if (GetArenaNoVirtual() == NULL) {
      delete log_type_.multi_inference_log_;
    }
    clear_has_log_type();
  }
}
inline  const ::tensorflow::serving::MultiInferenceLog& PredictionLog::multi_inference_log() const {
  // @@protoc_insertion_point(field_get:tensorflow.serving.PredictionLog.multi_inference_log)
  return has_multi_inference_log()
      ? *log_type_.multi_inference_log_
      : ::tensorflow::serving::MultiInferenceLog::default_instance();
}
inline ::tensorflow::serving::MultiInferenceLog* PredictionLog::mutable_multi_inference_log() {
  if (!has_multi_inference_log()) {
    clear_log_type();
    set_has_multi_inference_log();
    log_type_.multi_inference_log_ = 
      ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::MultiInferenceLog >(
      GetArenaNoVirtual());
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.serving.PredictionLog.multi_inference_log)
  return log_type_.multi_inference_log_;
}
inline ::tensorflow::serving::MultiInferenceLog* PredictionLog::release_multi_inference_log() {
  // @@protoc_insertion_point(field_release:tensorflow.serving.PredictionLog.multi_inference_log)
  if (has_multi_inference_log()) {
    clear_has_log_type();
    if (GetArenaNoVirtual() != NULL) {
      ::tensorflow::serving::MultiInferenceLog* temp = new ::tensorflow::serving::MultiInferenceLog;
      temp->MergeFrom(*log_type_.multi_inference_log_);
      log_type_.multi_inference_log_ = NULL;
      return temp;
    } else {
      ::tensorflow::serving::MultiInferenceLog* temp = log_type_.multi_inference_log_;
      log_type_.multi_inference_log_ = NULL;
      return temp;
    }
  } else {
    return NULL;
  }
}
inline void PredictionLog::set_allocated_multi_inference_log(::tensorflow::serving::MultiInferenceLog* multi_inference_log) {
  clear_log_type();
  if (multi_inference_log) {
    if (GetArenaNoVirtual() != NULL &&
        ::google::protobuf::Arena::GetArena(multi_inference_log) == NULL) {
      GetArenaNoVirtual()->Own(multi_inference_log);
    } else if (GetArenaNoVirtual() !=
               ::google::protobuf::Arena::GetArena(multi_inference_log)) {
      ::tensorflow::serving::MultiInferenceLog* new_multi_inference_log = 
          ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::MultiInferenceLog >(
          GetArenaNoVirtual());
      new_multi_inference_log->CopyFrom(*multi_inference_log);
      multi_inference_log = new_multi_inference_log;
    }
    set_has_multi_inference_log();
    log_type_.multi_inference_log_ = multi_inference_log;
  }
  // @@protoc_insertion_point(field_set_allocated:tensorflow.serving.PredictionLog.multi_inference_log)
}
inline  ::tensorflow::serving::MultiInferenceLog* PredictionLog::unsafe_arena_release_multi_inference_log() {
  // @@protoc_insertion_point(field_unsafe_arena_release:tensorflow.serving.PredictionLog.multi_inference_log)
  if (has_multi_inference_log()) {
    clear_has_log_type();
    ::tensorflow::serving::MultiInferenceLog* temp = log_type_.multi_inference_log_;
    log_type_.multi_inference_log_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline  void PredictionLog::unsafe_arena_set_allocated_multi_inference_log(::tensorflow::serving::MultiInferenceLog* multi_inference_log) {
  clear_log_type();
  if (multi_inference_log) {
    set_has_multi_inference_log();
    log_type_.multi_inference_log_ = multi_inference_log;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:tensorflow.serving.PredictionLog.multi_inference_log)
}

// optional .tensorflow.serving.SessionRunLog session_run_log = 5;
inline bool PredictionLog::has_session_run_log() const {
  return log_type_case() == kSessionRunLog;
}
inline void PredictionLog::set_has_session_run_log() {
  _oneof_case_[0] = kSessionRunLog;
}
inline void PredictionLog::clear_session_run_log() {
  if (has_session_run_log()) {
    if (GetArenaNoVirtual() == NULL) {
      delete log_type_.session_run_log_;
    }
    clear_has_log_type();
  }
}
inline  const ::tensorflow::serving::SessionRunLog& PredictionLog::session_run_log() const {
  // @@protoc_insertion_point(field_get:tensorflow.serving.PredictionLog.session_run_log)
  return has_session_run_log()
      ? *log_type_.session_run_log_
      : ::tensorflow::serving::SessionRunLog::default_instance();
}
inline ::tensorflow::serving::SessionRunLog* PredictionLog::mutable_session_run_log() {
  if (!has_session_run_log()) {
    clear_log_type();
    set_has_session_run_log();
    log_type_.session_run_log_ = 
      ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::SessionRunLog >(
      GetArenaNoVirtual());
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.serving.PredictionLog.session_run_log)
  return log_type_.session_run_log_;
}
inline ::tensorflow::serving::SessionRunLog* PredictionLog::release_session_run_log() {
  // @@protoc_insertion_point(field_release:tensorflow.serving.PredictionLog.session_run_log)
  if (has_session_run_log()) {
    clear_has_log_type();
    if (GetArenaNoVirtual() != NULL) {
      ::tensorflow::serving::SessionRunLog* temp = new ::tensorflow::serving::SessionRunLog;
      temp->MergeFrom(*log_type_.session_run_log_);
      log_type_.session_run_log_ = NULL;
      return temp;
    } else {
      ::tensorflow::serving::SessionRunLog* temp = log_type_.session_run_log_;
      log_type_.session_run_log_ = NULL;
      return temp;
    }
  } else {
    return NULL;
  }
}
inline void PredictionLog::set_allocated_session_run_log(::tensorflow::serving::SessionRunLog* session_run_log) {
  clear_log_type();
  if (session_run_log) {
    if (GetArenaNoVirtual() != NULL &&
        ::google::protobuf::Arena::GetArena(session_run_log) == NULL) {
      GetArenaNoVirtual()->Own(session_run_log);
    } else if (GetArenaNoVirtual() !=
               ::google::protobuf::Arena::GetArena(session_run_log)) {
      ::tensorflow::serving::SessionRunLog* new_session_run_log = 
          ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::SessionRunLog >(
          GetArenaNoVirtual());
      new_session_run_log->CopyFrom(*session_run_log);
      session_run_log = new_session_run_log;
    }
    set_has_session_run_log();
    log_type_.session_run_log_ = session_run_log;
  }
  // @@protoc_insertion_point(field_set_allocated:tensorflow.serving.PredictionLog.session_run_log)
}
inline  ::tensorflow::serving::SessionRunLog* PredictionLog::unsafe_arena_release_session_run_log() {
  // @@protoc_insertion_point(field_unsafe_arena_release:tensorflow.serving.PredictionLog.session_run_log)
  if (has_session_run_log()) {
    clear_has_log_type();
    ::tensorflow::serving::SessionRunLog* temp = log_type_.session_run_log_;
    log_type_.session_run_log_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
inline  void PredictionLog::unsafe_arena_set_allocated_session_run_log(::tensorflow::serving::SessionRunLog* session_run_log) {
  clear_log_type();
  if (session_run_log) {
    set_has_session_run_log();
    log_type_.session_run_log_ = session_run_log;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:tensorflow.serving.PredictionLog.session_run_log)
}

inline bool PredictionLog::has_log_type() const {
  return log_type_case() != LOG_TYPE_NOT_SET;
}
inline void PredictionLog::clear_has_log_type() {
  _oneof_case_[0] = LOG_TYPE_NOT_SET;
}
inline PredictionLog::LogTypeCase PredictionLog::log_type_case() const {
  return PredictionLog::LogTypeCase(_oneof_case_[0]);
}
#endif  // !PROTOBUF_INLINE_NOT_IN_HEADERS
// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------


// @@protoc_insertion_point(namespace_scope)

}  // namespace serving
}  // namespace tensorflow

// @@protoc_insertion_point(global_scope)

#endif  // PROTOBUF_tensorflow_5fserving_2fapis_2fprediction_5flog_2eproto__INCLUDED
