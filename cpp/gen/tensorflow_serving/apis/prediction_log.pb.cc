// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: tensorflow_serving/apis/prediction_log.proto

#define INTERNAL_SUPPRESS_PROTOBUF_FIELD_DEPRECATION
#include "tensorflow_serving/apis/prediction_log.pb.h"

#include <algorithm>

#include <google/protobuf/stubs/common.h>
#include <google/protobuf/stubs/port.h>
#include <google/protobuf/stubs/once.h>
#include <google/protobuf/io/coded_stream.h>
#include <google/protobuf/wire_format_lite_inl.h>
#include <google/protobuf/descriptor.h>
#include <google/protobuf/generated_message_reflection.h>
#include <google/protobuf/reflection_ops.h>
#include <google/protobuf/wire_format.h>
// @@protoc_insertion_point(includes)

namespace tensorflow {
namespace serving {

namespace {

const ::google::protobuf::Descriptor* ClassifyLog_descriptor_ = NULL;
const ::google::protobuf::internal::GeneratedMessageReflection*
  ClassifyLog_reflection_ = NULL;
const ::google::protobuf::Descriptor* RegressLog_descriptor_ = NULL;
const ::google::protobuf::internal::GeneratedMessageReflection*
  RegressLog_reflection_ = NULL;
const ::google::protobuf::Descriptor* PredictLog_descriptor_ = NULL;
const ::google::protobuf::internal::GeneratedMessageReflection*
  PredictLog_reflection_ = NULL;
const ::google::protobuf::Descriptor* MultiInferenceLog_descriptor_ = NULL;
const ::google::protobuf::internal::GeneratedMessageReflection*
  MultiInferenceLog_reflection_ = NULL;
const ::google::protobuf::Descriptor* SessionRunLog_descriptor_ = NULL;
const ::google::protobuf::internal::GeneratedMessageReflection*
  SessionRunLog_reflection_ = NULL;
const ::google::protobuf::Descriptor* PredictionLog_descriptor_ = NULL;
const ::google::protobuf::internal::GeneratedMessageReflection*
  PredictionLog_reflection_ = NULL;
struct PredictionLogOneofInstance {
  const ::tensorflow::serving::ClassifyLog* classify_log_;
  const ::tensorflow::serving::RegressLog* regress_log_;
  const ::tensorflow::serving::PredictLog* predict_log_;
  const ::tensorflow::serving::MultiInferenceLog* multi_inference_log_;
  const ::tensorflow::serving::SessionRunLog* session_run_log_;
}* PredictionLog_default_oneof_instance_ = NULL;

}  // namespace


void protobuf_AssignDesc_tensorflow_5fserving_2fapis_2fprediction_5flog_2eproto() GOOGLE_ATTRIBUTE_COLD;
void protobuf_AssignDesc_tensorflow_5fserving_2fapis_2fprediction_5flog_2eproto() {
  protobuf_AddDesc_tensorflow_5fserving_2fapis_2fprediction_5flog_2eproto();
  const ::google::protobuf::FileDescriptor* file =
    ::google::protobuf::DescriptorPool::generated_pool()->FindFileByName(
      "tensorflow_serving/apis/prediction_log.proto");
  GOOGLE_CHECK(file != NULL);
  ClassifyLog_descriptor_ = file->message_type(0);
  static const int ClassifyLog_offsets_[2] = {
    GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(ClassifyLog, request_),
    GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(ClassifyLog, response_),
  };
  ClassifyLog_reflection_ =
    ::google::protobuf::internal::GeneratedMessageReflection::NewGeneratedMessageReflection(
      ClassifyLog_descriptor_,
      ClassifyLog::default_instance_,
      ClassifyLog_offsets_,
      -1,
      -1,
      -1,
      sizeof(ClassifyLog),
      GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(ClassifyLog, _internal_metadata_),
      GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(ClassifyLog, _is_default_instance_));
  RegressLog_descriptor_ = file->message_type(1);
  static const int RegressLog_offsets_[2] = {
    GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(RegressLog, request_),
    GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(RegressLog, response_),
  };
  RegressLog_reflection_ =
    ::google::protobuf::internal::GeneratedMessageReflection::NewGeneratedMessageReflection(
      RegressLog_descriptor_,
      RegressLog::default_instance_,
      RegressLog_offsets_,
      -1,
      -1,
      -1,
      sizeof(RegressLog),
      GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(RegressLog, _internal_metadata_),
      GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(RegressLog, _is_default_instance_));
  PredictLog_descriptor_ = file->message_type(2);
  static const int PredictLog_offsets_[2] = {
    GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(PredictLog, request_),
    GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(PredictLog, response_),
  };
  PredictLog_reflection_ =
    ::google::protobuf::internal::GeneratedMessageReflection::NewGeneratedMessageReflection(
      PredictLog_descriptor_,
      PredictLog::default_instance_,
      PredictLog_offsets_,
      -1,
      -1,
      -1,
      sizeof(PredictLog),
      GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(PredictLog, _internal_metadata_),
      GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(PredictLog, _is_default_instance_));
  MultiInferenceLog_descriptor_ = file->message_type(3);
  static const int MultiInferenceLog_offsets_[2] = {
    GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(MultiInferenceLog, request_),
    GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(MultiInferenceLog, response_),
  };
  MultiInferenceLog_reflection_ =
    ::google::protobuf::internal::GeneratedMessageReflection::NewGeneratedMessageReflection(
      MultiInferenceLog_descriptor_,
      MultiInferenceLog::default_instance_,
      MultiInferenceLog_offsets_,
      -1,
      -1,
      -1,
      sizeof(MultiInferenceLog),
      GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(MultiInferenceLog, _internal_metadata_),
      GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(MultiInferenceLog, _is_default_instance_));
  SessionRunLog_descriptor_ = file->message_type(4);
  static const int SessionRunLog_offsets_[2] = {
    GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(SessionRunLog, request_),
    GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(SessionRunLog, response_),
  };
  SessionRunLog_reflection_ =
    ::google::protobuf::internal::GeneratedMessageReflection::NewGeneratedMessageReflection(
      SessionRunLog_descriptor_,
      SessionRunLog::default_instance_,
      SessionRunLog_offsets_,
      -1,
      -1,
      -1,
      sizeof(SessionRunLog),
      GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(SessionRunLog, _internal_metadata_),
      GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(SessionRunLog, _is_default_instance_));
  PredictionLog_descriptor_ = file->message_type(5);
  static const int PredictionLog_offsets_[7] = {
    GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(PredictionLog, log_metadata_),
    PROTO2_GENERATED_DEFAULT_ONEOF_FIELD_OFFSET(PredictionLog_default_oneof_instance_, classify_log_),
    PROTO2_GENERATED_DEFAULT_ONEOF_FIELD_OFFSET(PredictionLog_default_oneof_instance_, regress_log_),
    PROTO2_GENERATED_DEFAULT_ONEOF_FIELD_OFFSET(PredictionLog_default_oneof_instance_, predict_log_),
    PROTO2_GENERATED_DEFAULT_ONEOF_FIELD_OFFSET(PredictionLog_default_oneof_instance_, multi_inference_log_),
    PROTO2_GENERATED_DEFAULT_ONEOF_FIELD_OFFSET(PredictionLog_default_oneof_instance_, session_run_log_),
    GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(PredictionLog, log_type_),
  };
  PredictionLog_reflection_ =
    ::google::protobuf::internal::GeneratedMessageReflection::NewGeneratedMessageReflection(
      PredictionLog_descriptor_,
      PredictionLog::default_instance_,
      PredictionLog_offsets_,
      -1,
      -1,
      -1,
      PredictionLog_default_oneof_instance_,
      GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(PredictionLog, _oneof_case_[0]),
      sizeof(PredictionLog),
      GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(PredictionLog, _internal_metadata_),
      GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(PredictionLog, _is_default_instance_));
}

namespace {

GOOGLE_PROTOBUF_DECLARE_ONCE(protobuf_AssignDescriptors_once_);
inline void protobuf_AssignDescriptorsOnce() {
  ::google::protobuf::GoogleOnceInit(&protobuf_AssignDescriptors_once_,
                 &protobuf_AssignDesc_tensorflow_5fserving_2fapis_2fprediction_5flog_2eproto);
}

void protobuf_RegisterTypes(const ::std::string&) GOOGLE_ATTRIBUTE_COLD;
void protobuf_RegisterTypes(const ::std::string&) {
  protobuf_AssignDescriptorsOnce();
  ::google::protobuf::MessageFactory::InternalRegisterGeneratedMessage(
      ClassifyLog_descriptor_, &ClassifyLog::default_instance());
  ::google::protobuf::MessageFactory::InternalRegisterGeneratedMessage(
      RegressLog_descriptor_, &RegressLog::default_instance());
  ::google::protobuf::MessageFactory::InternalRegisterGeneratedMessage(
      PredictLog_descriptor_, &PredictLog::default_instance());
  ::google::protobuf::MessageFactory::InternalRegisterGeneratedMessage(
      MultiInferenceLog_descriptor_, &MultiInferenceLog::default_instance());
  ::google::protobuf::MessageFactory::InternalRegisterGeneratedMessage(
      SessionRunLog_descriptor_, &SessionRunLog::default_instance());
  ::google::protobuf::MessageFactory::InternalRegisterGeneratedMessage(
      PredictionLog_descriptor_, &PredictionLog::default_instance());
}

}  // namespace

void protobuf_ShutdownFile_tensorflow_5fserving_2fapis_2fprediction_5flog_2eproto() {
  delete ClassifyLog::default_instance_;
  delete ClassifyLog_reflection_;
  delete RegressLog::default_instance_;
  delete RegressLog_reflection_;
  delete PredictLog::default_instance_;
  delete PredictLog_reflection_;
  delete MultiInferenceLog::default_instance_;
  delete MultiInferenceLog_reflection_;
  delete SessionRunLog::default_instance_;
  delete SessionRunLog_reflection_;
  delete PredictionLog::default_instance_;
  delete PredictionLog_default_oneof_instance_;
  delete PredictionLog_reflection_;
}

void protobuf_AddDesc_tensorflow_5fserving_2fapis_2fprediction_5flog_2eproto() GOOGLE_ATTRIBUTE_COLD;
void protobuf_AddDesc_tensorflow_5fserving_2fapis_2fprediction_5flog_2eproto() {
  static bool already_here = false;
  if (already_here) return;
  already_here = true;
  GOOGLE_PROTOBUF_VERIFY_VERSION;

  ::tensorflow::serving::protobuf_AddDesc_tensorflow_5fserving_2fapis_2fclassification_2eproto();
  ::tensorflow::serving::protobuf_AddDesc_tensorflow_5fserving_2fapis_2finference_2eproto();
  ::tensorflow::serving::protobuf_AddDesc_tensorflow_5fserving_2fapis_2fpredict_2eproto();
  ::tensorflow::serving::protobuf_AddDesc_tensorflow_5fserving_2fapis_2fregression_2eproto();
  ::tensorflow::serving::protobuf_AddDesc_tensorflow_5fserving_2fapis_2fsession_5fservice_2eproto();
  ::tensorflow::serving::protobuf_AddDesc_tensorflow_5fserving_2fcore_2flogging_2eproto();
  ::google::protobuf::DescriptorPool::InternalAddGeneratedFile(
    "\n,tensorflow_serving/apis/prediction_log"
    ".proto\022\022tensorflow.serving\032,tensorflow_s"
    "erving/apis/classification.proto\032\'tensor"
    "flow_serving/apis/inference.proto\032%tenso"
    "rflow_serving/apis/predict.proto\032(tensor"
    "flow_serving/apis/regression.proto\032-tens"
    "orflow_serving/apis/session_service.prot"
    "o\032%tensorflow_serving/core/logging.proto"
    "\"\207\001\n\013ClassifyLog\022:\n\007request\030\001 \001(\0132).tens"
    "orflow.serving.ClassificationRequest\022<\n\010"
    "response\030\002 \001(\0132*.tensorflow.serving.Clas"
    "sificationResponse\"~\n\nRegressLog\0226\n\007requ"
    "est\030\001 \001(\0132%.tensorflow.serving.Regressio"
    "nRequest\0228\n\010response\030\002 \001(\0132&.tensorflow."
    "serving.RegressionResponse\"x\n\nPredictLog"
    "\0223\n\007request\030\001 \001(\0132\".tensorflow.serving.P"
    "redictRequest\0225\n\010response\030\002 \001(\0132#.tensor"
    "flow.serving.PredictResponse\"\215\001\n\021MultiIn"
    "ferenceLog\022:\n\007request\030\001 \001(\0132).tensorflow"
    ".serving.MultiInferenceRequest\022<\n\010respon"
    "se\030\002 \001(\0132*.tensorflow.serving.MultiInfer"
    "enceResponse\"\201\001\n\rSessionRunLog\0226\n\007reques"
    "t\030\001 \001(\0132%.tensorflow.serving.SessionRunR"
    "equest\0228\n\010response\030\002 \001(\0132&.tensorflow.se"
    "rving.SessionRunResponse\"\375\002\n\rPredictionL"
    "og\0225\n\014log_metadata\030\001 \001(\0132\037.tensorflow.se"
    "rving.LogMetadata\0227\n\014classify_log\030\002 \001(\0132"
    "\037.tensorflow.serving.ClassifyLogH\000\0225\n\013re"
    "gress_log\030\003 \001(\0132\036.tensorflow.serving.Reg"
    "ressLogH\000\0225\n\013predict_log\030\006 \001(\0132\036.tensorf"
    "low.serving.PredictLogH\000\022D\n\023multi_infere"
    "nce_log\030\004 \001(\0132%.tensorflow.serving.Multi"
    "InferenceLogH\000\022<\n\017session_run_log\030\005 \001(\0132"
    "!.tensorflow.serving.SessionRunLogH\000B\n\n\010"
    "log_typeB\003\370\001\001b\006proto3", 1381);
  ::google::protobuf::MessageFactory::InternalRegisterGeneratedFile(
    "tensorflow_serving/apis/prediction_log.proto", &protobuf_RegisterTypes);
  ClassifyLog::default_instance_ = new ClassifyLog();
  RegressLog::default_instance_ = new RegressLog();
  PredictLog::default_instance_ = new PredictLog();
  MultiInferenceLog::default_instance_ = new MultiInferenceLog();
  SessionRunLog::default_instance_ = new SessionRunLog();
  PredictionLog::default_instance_ = new PredictionLog();
  PredictionLog_default_oneof_instance_ = new PredictionLogOneofInstance();
  ClassifyLog::default_instance_->InitAsDefaultInstance();
  RegressLog::default_instance_->InitAsDefaultInstance();
  PredictLog::default_instance_->InitAsDefaultInstance();
  MultiInferenceLog::default_instance_->InitAsDefaultInstance();
  SessionRunLog::default_instance_->InitAsDefaultInstance();
  PredictionLog::default_instance_->InitAsDefaultInstance();
  ::google::protobuf::internal::OnShutdown(&protobuf_ShutdownFile_tensorflow_5fserving_2fapis_2fprediction_5flog_2eproto);
}

// Force AddDescriptors() to be called at static initialization time.
struct StaticDescriptorInitializer_tensorflow_5fserving_2fapis_2fprediction_5flog_2eproto {
  StaticDescriptorInitializer_tensorflow_5fserving_2fapis_2fprediction_5flog_2eproto() {
    protobuf_AddDesc_tensorflow_5fserving_2fapis_2fprediction_5flog_2eproto();
  }
} static_descriptor_initializer_tensorflow_5fserving_2fapis_2fprediction_5flog_2eproto_;

// ===================================================================

void ClassifyLog::_slow_mutable_request() {
  request_ = ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::ClassificationRequest >(
      GetArenaNoVirtual());
}
::tensorflow::serving::ClassificationRequest* ClassifyLog::_slow_release_request() {
  if (request_ == NULL) {
    return NULL;
  } else {
    ::tensorflow::serving::ClassificationRequest* temp = new ::tensorflow::serving::ClassificationRequest;
    temp->MergeFrom(*request_);
    request_ = NULL;
    return temp;
  }
}
::tensorflow::serving::ClassificationRequest* ClassifyLog::unsafe_arena_release_request() {
  // @@protoc_insertion_point(field_unsafe_arena_release:tensorflow.serving.ClassifyLog.request)
  
  ::tensorflow::serving::ClassificationRequest* temp = request_;
  request_ = NULL;
  return temp;
}
void ClassifyLog::_slow_set_allocated_request(
    ::google::protobuf::Arena* message_arena, ::tensorflow::serving::ClassificationRequest** request) {
    if (message_arena != NULL && 
        ::google::protobuf::Arena::GetArena(*request) == NULL) {
      message_arena->Own(*request);
    } else if (message_arena !=
               ::google::protobuf::Arena::GetArena(*request)) {
      ::tensorflow::serving::ClassificationRequest* new_request = 
            ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::ClassificationRequest >(
            message_arena);
      new_request->CopyFrom(**request);
      *request = new_request;
    }
}
void ClassifyLog::unsafe_arena_set_allocated_request(
    ::tensorflow::serving::ClassificationRequest* request) {
  if (GetArenaNoVirtual() == NULL) {
    delete request_;
  }
  request_ = request;
  if (request) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:tensorflow.serving.ClassifyLog.request)
}
void ClassifyLog::_slow_mutable_response() {
  response_ = ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::ClassificationResponse >(
      GetArenaNoVirtual());
}
::tensorflow::serving::ClassificationResponse* ClassifyLog::_slow_release_response() {
  if (response_ == NULL) {
    return NULL;
  } else {
    ::tensorflow::serving::ClassificationResponse* temp = new ::tensorflow::serving::ClassificationResponse;
    temp->MergeFrom(*response_);
    response_ = NULL;
    return temp;
  }
}
::tensorflow::serving::ClassificationResponse* ClassifyLog::unsafe_arena_release_response() {
  // @@protoc_insertion_point(field_unsafe_arena_release:tensorflow.serving.ClassifyLog.response)
  
  ::tensorflow::serving::ClassificationResponse* temp = response_;
  response_ = NULL;
  return temp;
}
void ClassifyLog::_slow_set_allocated_response(
    ::google::protobuf::Arena* message_arena, ::tensorflow::serving::ClassificationResponse** response) {
    if (message_arena != NULL && 
        ::google::protobuf::Arena::GetArena(*response) == NULL) {
      message_arena->Own(*response);
    } else if (message_arena !=
               ::google::protobuf::Arena::GetArena(*response)) {
      ::tensorflow::serving::ClassificationResponse* new_response = 
            ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::ClassificationResponse >(
            message_arena);
      new_response->CopyFrom(**response);
      *response = new_response;
    }
}
void ClassifyLog::unsafe_arena_set_allocated_response(
    ::tensorflow::serving::ClassificationResponse* response) {
  if (GetArenaNoVirtual() == NULL) {
    delete response_;
  }
  response_ = response;
  if (response) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:tensorflow.serving.ClassifyLog.response)
}
#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ClassifyLog::kRequestFieldNumber;
const int ClassifyLog::kResponseFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ClassifyLog::ClassifyLog()
  : ::google::protobuf::Message(), _internal_metadata_(NULL) {
  SharedCtor();
  // @@protoc_insertion_point(constructor:tensorflow.serving.ClassifyLog)
}

ClassifyLog::ClassifyLog(::google::protobuf::Arena* arena)
  : ::google::protobuf::Message(),
  _internal_metadata_(arena) {
  SharedCtor();
  RegisterArenaDtor(arena);
  // @@protoc_insertion_point(arena_constructor:tensorflow.serving.ClassifyLog)
}

void ClassifyLog::InitAsDefaultInstance() {
  _is_default_instance_ = true;
  request_ = const_cast< ::tensorflow::serving::ClassificationRequest*>(&::tensorflow::serving::ClassificationRequest::default_instance());
  response_ = const_cast< ::tensorflow::serving::ClassificationResponse*>(&::tensorflow::serving::ClassificationResponse::default_instance());
}

ClassifyLog::ClassifyLog(const ClassifyLog& from)
  : ::google::protobuf::Message(),
    _internal_metadata_(NULL) {
  SharedCtor();
  MergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:tensorflow.serving.ClassifyLog)
}

void ClassifyLog::SharedCtor() {
    _is_default_instance_ = false;
  _cached_size_ = 0;
  request_ = NULL;
  response_ = NULL;
}

ClassifyLog::~ClassifyLog() {
  // @@protoc_insertion_point(destructor:tensorflow.serving.ClassifyLog)
  SharedDtor();
}

void ClassifyLog::SharedDtor() {
  if (GetArenaNoVirtual() != NULL) {
    return;
  }

  if (this != default_instance_) {
    delete request_;
    delete response_;
  }
}

void ClassifyLog::ArenaDtor(void* object) {
  ClassifyLog* _this = reinterpret_cast< ClassifyLog* >(object);
  (void)_this;
}
void ClassifyLog::RegisterArenaDtor(::google::protobuf::Arena* arena) {
}
void ClassifyLog::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ::google::protobuf::Descriptor* ClassifyLog::descriptor() {
  protobuf_AssignDescriptorsOnce();
  return ClassifyLog_descriptor_;
}

const ClassifyLog& ClassifyLog::default_instance() {
  if (default_instance_ == NULL) protobuf_AddDesc_tensorflow_5fserving_2fapis_2fprediction_5flog_2eproto();
  return *default_instance_;
}

ClassifyLog* ClassifyLog::default_instance_ = NULL;

ClassifyLog* ClassifyLog::New(::google::protobuf::Arena* arena) const {
  return ::google::protobuf::Arena::CreateMessage<ClassifyLog>(arena);
}

void ClassifyLog::Clear() {
// @@protoc_insertion_point(message_clear_start:tensorflow.serving.ClassifyLog)
  if (GetArenaNoVirtual() == NULL && request_ != NULL) delete request_;
  request_ = NULL;
  if (GetArenaNoVirtual() == NULL && response_ != NULL) delete response_;
  response_ = NULL;
}

bool ClassifyLog::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:tensorflow.serving.ClassifyLog)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // optional .tensorflow.serving.ClassificationRequest request = 1;
      case 1: {
        if (tag == 10) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_request()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(18)) goto parse_response;
        break;
      }

      // optional .tensorflow.serving.ClassificationResponse response = 2;
      case 2: {
        if (tag == 18) {
         parse_response:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_response()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:tensorflow.serving.ClassifyLog)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:tensorflow.serving.ClassifyLog)
  return false;
#undef DO_
}

void ClassifyLog::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:tensorflow.serving.ClassifyLog)
  // optional .tensorflow.serving.ClassificationRequest request = 1;
  if (this->has_request()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      1, *this->request_, output);
  }

  // optional .tensorflow.serving.ClassificationResponse response = 2;
  if (this->has_response()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      2, *this->response_, output);
  }

  // @@protoc_insertion_point(serialize_end:tensorflow.serving.ClassifyLog)
}

::google::protobuf::uint8* ClassifyLog::InternalSerializeWithCachedSizesToArray(
    bool deterministic, ::google::protobuf::uint8* target) const {
  // @@protoc_insertion_point(serialize_to_array_start:tensorflow.serving.ClassifyLog)
  // optional .tensorflow.serving.ClassificationRequest request = 1;
  if (this->has_request()) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageNoVirtualToArray(
        1, *this->request_, false, target);
  }

  // optional .tensorflow.serving.ClassificationResponse response = 2;
  if (this->has_response()) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageNoVirtualToArray(
        2, *this->response_, false, target);
  }

  // @@protoc_insertion_point(serialize_to_array_end:tensorflow.serving.ClassifyLog)
  return target;
}

int ClassifyLog::ByteSize() const {
// @@protoc_insertion_point(message_byte_size_start:tensorflow.serving.ClassifyLog)
  int total_size = 0;

  // optional .tensorflow.serving.ClassificationRequest request = 1;
  if (this->has_request()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->request_);
  }

  // optional .tensorflow.serving.ClassificationResponse response = 2;
  if (this->has_response()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->response_);
  }

  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = total_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void ClassifyLog::MergeFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_merge_from_start:tensorflow.serving.ClassifyLog)
  if (GOOGLE_PREDICT_FALSE(&from == this)) {
    ::google::protobuf::internal::MergeFromFail(__FILE__, __LINE__);
  }
  const ClassifyLog* source = 
      ::google::protobuf::internal::DynamicCastToGenerated<const ClassifyLog>(
          &from);
  if (source == NULL) {
  // @@protoc_insertion_point(generalized_merge_from_cast_fail:tensorflow.serving.ClassifyLog)
    ::google::protobuf::internal::ReflectionOps::Merge(from, this);
  } else {
  // @@protoc_insertion_point(generalized_merge_from_cast_success:tensorflow.serving.ClassifyLog)
    MergeFrom(*source);
  }
}

void ClassifyLog::MergeFrom(const ClassifyLog& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:tensorflow.serving.ClassifyLog)
  if (GOOGLE_PREDICT_FALSE(&from == this)) {
    ::google::protobuf::internal::MergeFromFail(__FILE__, __LINE__);
  }
  if (from.has_request()) {
    mutable_request()->::tensorflow::serving::ClassificationRequest::MergeFrom(from.request());
  }
  if (from.has_response()) {
    mutable_response()->::tensorflow::serving::ClassificationResponse::MergeFrom(from.response());
  }
}

void ClassifyLog::CopyFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_copy_from_start:tensorflow.serving.ClassifyLog)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

void ClassifyLog::CopyFrom(const ClassifyLog& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:tensorflow.serving.ClassifyLog)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ClassifyLog::IsInitialized() const {

  return true;
}

void ClassifyLog::Swap(ClassifyLog* other) {
  if (other == this) return;
  if (GetArenaNoVirtual() == other->GetArenaNoVirtual()) {
    InternalSwap(other);
  } else {
    ClassifyLog temp;
    temp.MergeFrom(*this);
    CopyFrom(*other);
    other->CopyFrom(temp);
  }
}
void ClassifyLog::UnsafeArenaSwap(ClassifyLog* other) {
  if (other == this) return;
  GOOGLE_DCHECK(GetArenaNoVirtual() == other->GetArenaNoVirtual());
  InternalSwap(other);
}
void ClassifyLog::InternalSwap(ClassifyLog* other) {
  std::swap(request_, other->request_);
  std::swap(response_, other->response_);
  _internal_metadata_.Swap(&other->_internal_metadata_);
  std::swap(_cached_size_, other->_cached_size_);
}

::google::protobuf::Metadata ClassifyLog::GetMetadata() const {
  protobuf_AssignDescriptorsOnce();
  ::google::protobuf::Metadata metadata;
  metadata.descriptor = ClassifyLog_descriptor_;
  metadata.reflection = ClassifyLog_reflection_;
  return metadata;
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// ClassifyLog

// optional .tensorflow.serving.ClassificationRequest request = 1;
bool ClassifyLog::has_request() const {
  return !_is_default_instance_ && request_ != NULL;
}
void ClassifyLog::clear_request() {
  if (GetArenaNoVirtual() == NULL && request_ != NULL) delete request_;
  request_ = NULL;
}
const ::tensorflow::serving::ClassificationRequest& ClassifyLog::request() const {
  // @@protoc_insertion_point(field_get:tensorflow.serving.ClassifyLog.request)
  return request_ != NULL ? *request_ : *default_instance_->request_;
}
::tensorflow::serving::ClassificationRequest* ClassifyLog::mutable_request() {
  
  if (request_ == NULL) {
    _slow_mutable_request();
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.serving.ClassifyLog.request)
  return request_;
}
::tensorflow::serving::ClassificationRequest* ClassifyLog::release_request() {
  // @@protoc_insertion_point(field_release:tensorflow.serving.ClassifyLog.request)
  
  if (GetArenaNoVirtual() != NULL) {
    return _slow_release_request();
  } else {
    ::tensorflow::serving::ClassificationRequest* temp = request_;
    request_ = NULL;
    return temp;
  }
}
 void ClassifyLog::set_allocated_request(::tensorflow::serving::ClassificationRequest* request) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete request_;
  }
  if (request != NULL) {
    _slow_set_allocated_request(message_arena, &request);
  }
  request_ = request;
  if (request) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:tensorflow.serving.ClassifyLog.request)
}

// optional .tensorflow.serving.ClassificationResponse response = 2;
bool ClassifyLog::has_response() const {
  return !_is_default_instance_ && response_ != NULL;
}
void ClassifyLog::clear_response() {
  if (GetArenaNoVirtual() == NULL && response_ != NULL) delete response_;
  response_ = NULL;
}
const ::tensorflow::serving::ClassificationResponse& ClassifyLog::response() const {
  // @@protoc_insertion_point(field_get:tensorflow.serving.ClassifyLog.response)
  return response_ != NULL ? *response_ : *default_instance_->response_;
}
::tensorflow::serving::ClassificationResponse* ClassifyLog::mutable_response() {
  
  if (response_ == NULL) {
    _slow_mutable_response();
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.serving.ClassifyLog.response)
  return response_;
}
::tensorflow::serving::ClassificationResponse* ClassifyLog::release_response() {
  // @@protoc_insertion_point(field_release:tensorflow.serving.ClassifyLog.response)
  
  if (GetArenaNoVirtual() != NULL) {
    return _slow_release_response();
  } else {
    ::tensorflow::serving::ClassificationResponse* temp = response_;
    response_ = NULL;
    return temp;
  }
}
 void ClassifyLog::set_allocated_response(::tensorflow::serving::ClassificationResponse* response) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete response_;
  }
  if (response != NULL) {
    _slow_set_allocated_response(message_arena, &response);
  }
  response_ = response;
  if (response) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:tensorflow.serving.ClassifyLog.response)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

void RegressLog::_slow_mutable_request() {
  request_ = ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::RegressionRequest >(
      GetArenaNoVirtual());
}
::tensorflow::serving::RegressionRequest* RegressLog::_slow_release_request() {
  if (request_ == NULL) {
    return NULL;
  } else {
    ::tensorflow::serving::RegressionRequest* temp = new ::tensorflow::serving::RegressionRequest;
    temp->MergeFrom(*request_);
    request_ = NULL;
    return temp;
  }
}
::tensorflow::serving::RegressionRequest* RegressLog::unsafe_arena_release_request() {
  // @@protoc_insertion_point(field_unsafe_arena_release:tensorflow.serving.RegressLog.request)
  
  ::tensorflow::serving::RegressionRequest* temp = request_;
  request_ = NULL;
  return temp;
}
void RegressLog::_slow_set_allocated_request(
    ::google::protobuf::Arena* message_arena, ::tensorflow::serving::RegressionRequest** request) {
    if (message_arena != NULL && 
        ::google::protobuf::Arena::GetArena(*request) == NULL) {
      message_arena->Own(*request);
    } else if (message_arena !=
               ::google::protobuf::Arena::GetArena(*request)) {
      ::tensorflow::serving::RegressionRequest* new_request = 
            ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::RegressionRequest >(
            message_arena);
      new_request->CopyFrom(**request);
      *request = new_request;
    }
}
void RegressLog::unsafe_arena_set_allocated_request(
    ::tensorflow::serving::RegressionRequest* request) {
  if (GetArenaNoVirtual() == NULL) {
    delete request_;
  }
  request_ = request;
  if (request) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:tensorflow.serving.RegressLog.request)
}
void RegressLog::_slow_mutable_response() {
  response_ = ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::RegressionResponse >(
      GetArenaNoVirtual());
}
::tensorflow::serving::RegressionResponse* RegressLog::_slow_release_response() {
  if (response_ == NULL) {
    return NULL;
  } else {
    ::tensorflow::serving::RegressionResponse* temp = new ::tensorflow::serving::RegressionResponse;
    temp->MergeFrom(*response_);
    response_ = NULL;
    return temp;
  }
}
::tensorflow::serving::RegressionResponse* RegressLog::unsafe_arena_release_response() {
  // @@protoc_insertion_point(field_unsafe_arena_release:tensorflow.serving.RegressLog.response)
  
  ::tensorflow::serving::RegressionResponse* temp = response_;
  response_ = NULL;
  return temp;
}
void RegressLog::_slow_set_allocated_response(
    ::google::protobuf::Arena* message_arena, ::tensorflow::serving::RegressionResponse** response) {
    if (message_arena != NULL && 
        ::google::protobuf::Arena::GetArena(*response) == NULL) {
      message_arena->Own(*response);
    } else if (message_arena !=
               ::google::protobuf::Arena::GetArena(*response)) {
      ::tensorflow::serving::RegressionResponse* new_response = 
            ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::RegressionResponse >(
            message_arena);
      new_response->CopyFrom(**response);
      *response = new_response;
    }
}
void RegressLog::unsafe_arena_set_allocated_response(
    ::tensorflow::serving::RegressionResponse* response) {
  if (GetArenaNoVirtual() == NULL) {
    delete response_;
  }
  response_ = response;
  if (response) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:tensorflow.serving.RegressLog.response)
}
#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int RegressLog::kRequestFieldNumber;
const int RegressLog::kResponseFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

RegressLog::RegressLog()
  : ::google::protobuf::Message(), _internal_metadata_(NULL) {
  SharedCtor();
  // @@protoc_insertion_point(constructor:tensorflow.serving.RegressLog)
}

RegressLog::RegressLog(::google::protobuf::Arena* arena)
  : ::google::protobuf::Message(),
  _internal_metadata_(arena) {
  SharedCtor();
  RegisterArenaDtor(arena);
  // @@protoc_insertion_point(arena_constructor:tensorflow.serving.RegressLog)
}

void RegressLog::InitAsDefaultInstance() {
  _is_default_instance_ = true;
  request_ = const_cast< ::tensorflow::serving::RegressionRequest*>(&::tensorflow::serving::RegressionRequest::default_instance());
  response_ = const_cast< ::tensorflow::serving::RegressionResponse*>(&::tensorflow::serving::RegressionResponse::default_instance());
}

RegressLog::RegressLog(const RegressLog& from)
  : ::google::protobuf::Message(),
    _internal_metadata_(NULL) {
  SharedCtor();
  MergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:tensorflow.serving.RegressLog)
}

void RegressLog::SharedCtor() {
    _is_default_instance_ = false;
  _cached_size_ = 0;
  request_ = NULL;
  response_ = NULL;
}

RegressLog::~RegressLog() {
  // @@protoc_insertion_point(destructor:tensorflow.serving.RegressLog)
  SharedDtor();
}

void RegressLog::SharedDtor() {
  if (GetArenaNoVirtual() != NULL) {
    return;
  }

  if (this != default_instance_) {
    delete request_;
    delete response_;
  }
}

void RegressLog::ArenaDtor(void* object) {
  RegressLog* _this = reinterpret_cast< RegressLog* >(object);
  (void)_this;
}
void RegressLog::RegisterArenaDtor(::google::protobuf::Arena* arena) {
}
void RegressLog::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ::google::protobuf::Descriptor* RegressLog::descriptor() {
  protobuf_AssignDescriptorsOnce();
  return RegressLog_descriptor_;
}

const RegressLog& RegressLog::default_instance() {
  if (default_instance_ == NULL) protobuf_AddDesc_tensorflow_5fserving_2fapis_2fprediction_5flog_2eproto();
  return *default_instance_;
}

RegressLog* RegressLog::default_instance_ = NULL;

RegressLog* RegressLog::New(::google::protobuf::Arena* arena) const {
  return ::google::protobuf::Arena::CreateMessage<RegressLog>(arena);
}

void RegressLog::Clear() {
// @@protoc_insertion_point(message_clear_start:tensorflow.serving.RegressLog)
  if (GetArenaNoVirtual() == NULL && request_ != NULL) delete request_;
  request_ = NULL;
  if (GetArenaNoVirtual() == NULL && response_ != NULL) delete response_;
  response_ = NULL;
}

bool RegressLog::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:tensorflow.serving.RegressLog)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // optional .tensorflow.serving.RegressionRequest request = 1;
      case 1: {
        if (tag == 10) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_request()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(18)) goto parse_response;
        break;
      }

      // optional .tensorflow.serving.RegressionResponse response = 2;
      case 2: {
        if (tag == 18) {
         parse_response:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_response()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:tensorflow.serving.RegressLog)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:tensorflow.serving.RegressLog)
  return false;
#undef DO_
}

void RegressLog::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:tensorflow.serving.RegressLog)
  // optional .tensorflow.serving.RegressionRequest request = 1;
  if (this->has_request()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      1, *this->request_, output);
  }

  // optional .tensorflow.serving.RegressionResponse response = 2;
  if (this->has_response()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      2, *this->response_, output);
  }

  // @@protoc_insertion_point(serialize_end:tensorflow.serving.RegressLog)
}

::google::protobuf::uint8* RegressLog::InternalSerializeWithCachedSizesToArray(
    bool deterministic, ::google::protobuf::uint8* target) const {
  // @@protoc_insertion_point(serialize_to_array_start:tensorflow.serving.RegressLog)
  // optional .tensorflow.serving.RegressionRequest request = 1;
  if (this->has_request()) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageNoVirtualToArray(
        1, *this->request_, false, target);
  }

  // optional .tensorflow.serving.RegressionResponse response = 2;
  if (this->has_response()) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageNoVirtualToArray(
        2, *this->response_, false, target);
  }

  // @@protoc_insertion_point(serialize_to_array_end:tensorflow.serving.RegressLog)
  return target;
}

int RegressLog::ByteSize() const {
// @@protoc_insertion_point(message_byte_size_start:tensorflow.serving.RegressLog)
  int total_size = 0;

  // optional .tensorflow.serving.RegressionRequest request = 1;
  if (this->has_request()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->request_);
  }

  // optional .tensorflow.serving.RegressionResponse response = 2;
  if (this->has_response()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->response_);
  }

  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = total_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void RegressLog::MergeFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_merge_from_start:tensorflow.serving.RegressLog)
  if (GOOGLE_PREDICT_FALSE(&from == this)) {
    ::google::protobuf::internal::MergeFromFail(__FILE__, __LINE__);
  }
  const RegressLog* source = 
      ::google::protobuf::internal::DynamicCastToGenerated<const RegressLog>(
          &from);
  if (source == NULL) {
  // @@protoc_insertion_point(generalized_merge_from_cast_fail:tensorflow.serving.RegressLog)
    ::google::protobuf::internal::ReflectionOps::Merge(from, this);
  } else {
  // @@protoc_insertion_point(generalized_merge_from_cast_success:tensorflow.serving.RegressLog)
    MergeFrom(*source);
  }
}

void RegressLog::MergeFrom(const RegressLog& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:tensorflow.serving.RegressLog)
  if (GOOGLE_PREDICT_FALSE(&from == this)) {
    ::google::protobuf::internal::MergeFromFail(__FILE__, __LINE__);
  }
  if (from.has_request()) {
    mutable_request()->::tensorflow::serving::RegressionRequest::MergeFrom(from.request());
  }
  if (from.has_response()) {
    mutable_response()->::tensorflow::serving::RegressionResponse::MergeFrom(from.response());
  }
}

void RegressLog::CopyFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_copy_from_start:tensorflow.serving.RegressLog)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

void RegressLog::CopyFrom(const RegressLog& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:tensorflow.serving.RegressLog)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool RegressLog::IsInitialized() const {

  return true;
}

void RegressLog::Swap(RegressLog* other) {
  if (other == this) return;
  if (GetArenaNoVirtual() == other->GetArenaNoVirtual()) {
    InternalSwap(other);
  } else {
    RegressLog temp;
    temp.MergeFrom(*this);
    CopyFrom(*other);
    other->CopyFrom(temp);
  }
}
void RegressLog::UnsafeArenaSwap(RegressLog* other) {
  if (other == this) return;
  GOOGLE_DCHECK(GetArenaNoVirtual() == other->GetArenaNoVirtual());
  InternalSwap(other);
}
void RegressLog::InternalSwap(RegressLog* other) {
  std::swap(request_, other->request_);
  std::swap(response_, other->response_);
  _internal_metadata_.Swap(&other->_internal_metadata_);
  std::swap(_cached_size_, other->_cached_size_);
}

::google::protobuf::Metadata RegressLog::GetMetadata() const {
  protobuf_AssignDescriptorsOnce();
  ::google::protobuf::Metadata metadata;
  metadata.descriptor = RegressLog_descriptor_;
  metadata.reflection = RegressLog_reflection_;
  return metadata;
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// RegressLog

// optional .tensorflow.serving.RegressionRequest request = 1;
bool RegressLog::has_request() const {
  return !_is_default_instance_ && request_ != NULL;
}
void RegressLog::clear_request() {
  if (GetArenaNoVirtual() == NULL && request_ != NULL) delete request_;
  request_ = NULL;
}
const ::tensorflow::serving::RegressionRequest& RegressLog::request() const {
  // @@protoc_insertion_point(field_get:tensorflow.serving.RegressLog.request)
  return request_ != NULL ? *request_ : *default_instance_->request_;
}
::tensorflow::serving::RegressionRequest* RegressLog::mutable_request() {
  
  if (request_ == NULL) {
    _slow_mutable_request();
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.serving.RegressLog.request)
  return request_;
}
::tensorflow::serving::RegressionRequest* RegressLog::release_request() {
  // @@protoc_insertion_point(field_release:tensorflow.serving.RegressLog.request)
  
  if (GetArenaNoVirtual() != NULL) {
    return _slow_release_request();
  } else {
    ::tensorflow::serving::RegressionRequest* temp = request_;
    request_ = NULL;
    return temp;
  }
}
 void RegressLog::set_allocated_request(::tensorflow::serving::RegressionRequest* request) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete request_;
  }
  if (request != NULL) {
    _slow_set_allocated_request(message_arena, &request);
  }
  request_ = request;
  if (request) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:tensorflow.serving.RegressLog.request)
}

// optional .tensorflow.serving.RegressionResponse response = 2;
bool RegressLog::has_response() const {
  return !_is_default_instance_ && response_ != NULL;
}
void RegressLog::clear_response() {
  if (GetArenaNoVirtual() == NULL && response_ != NULL) delete response_;
  response_ = NULL;
}
const ::tensorflow::serving::RegressionResponse& RegressLog::response() const {
  // @@protoc_insertion_point(field_get:tensorflow.serving.RegressLog.response)
  return response_ != NULL ? *response_ : *default_instance_->response_;
}
::tensorflow::serving::RegressionResponse* RegressLog::mutable_response() {
  
  if (response_ == NULL) {
    _slow_mutable_response();
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.serving.RegressLog.response)
  return response_;
}
::tensorflow::serving::RegressionResponse* RegressLog::release_response() {
  // @@protoc_insertion_point(field_release:tensorflow.serving.RegressLog.response)
  
  if (GetArenaNoVirtual() != NULL) {
    return _slow_release_response();
  } else {
    ::tensorflow::serving::RegressionResponse* temp = response_;
    response_ = NULL;
    return temp;
  }
}
 void RegressLog::set_allocated_response(::tensorflow::serving::RegressionResponse* response) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete response_;
  }
  if (response != NULL) {
    _slow_set_allocated_response(message_arena, &response);
  }
  response_ = response;
  if (response) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:tensorflow.serving.RegressLog.response)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

void PredictLog::_slow_mutable_request() {
  request_ = ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::PredictRequest >(
      GetArenaNoVirtual());
}
::tensorflow::serving::PredictRequest* PredictLog::_slow_release_request() {
  if (request_ == NULL) {
    return NULL;
  } else {
    ::tensorflow::serving::PredictRequest* temp = new ::tensorflow::serving::PredictRequest;
    temp->MergeFrom(*request_);
    request_ = NULL;
    return temp;
  }
}
::tensorflow::serving::PredictRequest* PredictLog::unsafe_arena_release_request() {
  // @@protoc_insertion_point(field_unsafe_arena_release:tensorflow.serving.PredictLog.request)
  
  ::tensorflow::serving::PredictRequest* temp = request_;
  request_ = NULL;
  return temp;
}
void PredictLog::_slow_set_allocated_request(
    ::google::protobuf::Arena* message_arena, ::tensorflow::serving::PredictRequest** request) {
    if (message_arena != NULL && 
        ::google::protobuf::Arena::GetArena(*request) == NULL) {
      message_arena->Own(*request);
    } else if (message_arena !=
               ::google::protobuf::Arena::GetArena(*request)) {
      ::tensorflow::serving::PredictRequest* new_request = 
            ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::PredictRequest >(
            message_arena);
      new_request->CopyFrom(**request);
      *request = new_request;
    }
}
void PredictLog::unsafe_arena_set_allocated_request(
    ::tensorflow::serving::PredictRequest* request) {
  if (GetArenaNoVirtual() == NULL) {
    delete request_;
  }
  request_ = request;
  if (request) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:tensorflow.serving.PredictLog.request)
}
void PredictLog::_slow_mutable_response() {
  response_ = ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::PredictResponse >(
      GetArenaNoVirtual());
}
::tensorflow::serving::PredictResponse* PredictLog::_slow_release_response() {
  if (response_ == NULL) {
    return NULL;
  } else {
    ::tensorflow::serving::PredictResponse* temp = new ::tensorflow::serving::PredictResponse;
    temp->MergeFrom(*response_);
    response_ = NULL;
    return temp;
  }
}
::tensorflow::serving::PredictResponse* PredictLog::unsafe_arena_release_response() {
  // @@protoc_insertion_point(field_unsafe_arena_release:tensorflow.serving.PredictLog.response)
  
  ::tensorflow::serving::PredictResponse* temp = response_;
  response_ = NULL;
  return temp;
}
void PredictLog::_slow_set_allocated_response(
    ::google::protobuf::Arena* message_arena, ::tensorflow::serving::PredictResponse** response) {
    if (message_arena != NULL && 
        ::google::protobuf::Arena::GetArena(*response) == NULL) {
      message_arena->Own(*response);
    } else if (message_arena !=
               ::google::protobuf::Arena::GetArena(*response)) {
      ::tensorflow::serving::PredictResponse* new_response = 
            ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::PredictResponse >(
            message_arena);
      new_response->CopyFrom(**response);
      *response = new_response;
    }
}
void PredictLog::unsafe_arena_set_allocated_response(
    ::tensorflow::serving::PredictResponse* response) {
  if (GetArenaNoVirtual() == NULL) {
    delete response_;
  }
  response_ = response;
  if (response) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:tensorflow.serving.PredictLog.response)
}
#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int PredictLog::kRequestFieldNumber;
const int PredictLog::kResponseFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

PredictLog::PredictLog()
  : ::google::protobuf::Message(), _internal_metadata_(NULL) {
  SharedCtor();
  // @@protoc_insertion_point(constructor:tensorflow.serving.PredictLog)
}

PredictLog::PredictLog(::google::protobuf::Arena* arena)
  : ::google::protobuf::Message(),
  _internal_metadata_(arena) {
  SharedCtor();
  RegisterArenaDtor(arena);
  // @@protoc_insertion_point(arena_constructor:tensorflow.serving.PredictLog)
}

void PredictLog::InitAsDefaultInstance() {
  _is_default_instance_ = true;
  request_ = const_cast< ::tensorflow::serving::PredictRequest*>(&::tensorflow::serving::PredictRequest::default_instance());
  response_ = const_cast< ::tensorflow::serving::PredictResponse*>(&::tensorflow::serving::PredictResponse::default_instance());
}

PredictLog::PredictLog(const PredictLog& from)
  : ::google::protobuf::Message(),
    _internal_metadata_(NULL) {
  SharedCtor();
  MergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:tensorflow.serving.PredictLog)
}

void PredictLog::SharedCtor() {
    _is_default_instance_ = false;
  _cached_size_ = 0;
  request_ = NULL;
  response_ = NULL;
}

PredictLog::~PredictLog() {
  // @@protoc_insertion_point(destructor:tensorflow.serving.PredictLog)
  SharedDtor();
}

void PredictLog::SharedDtor() {
  if (GetArenaNoVirtual() != NULL) {
    return;
  }

  if (this != default_instance_) {
    delete request_;
    delete response_;
  }
}

void PredictLog::ArenaDtor(void* object) {
  PredictLog* _this = reinterpret_cast< PredictLog* >(object);
  (void)_this;
}
void PredictLog::RegisterArenaDtor(::google::protobuf::Arena* arena) {
}
void PredictLog::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ::google::protobuf::Descriptor* PredictLog::descriptor() {
  protobuf_AssignDescriptorsOnce();
  return PredictLog_descriptor_;
}

const PredictLog& PredictLog::default_instance() {
  if (default_instance_ == NULL) protobuf_AddDesc_tensorflow_5fserving_2fapis_2fprediction_5flog_2eproto();
  return *default_instance_;
}

PredictLog* PredictLog::default_instance_ = NULL;

PredictLog* PredictLog::New(::google::protobuf::Arena* arena) const {
  return ::google::protobuf::Arena::CreateMessage<PredictLog>(arena);
}

void PredictLog::Clear() {
// @@protoc_insertion_point(message_clear_start:tensorflow.serving.PredictLog)
  if (GetArenaNoVirtual() == NULL && request_ != NULL) delete request_;
  request_ = NULL;
  if (GetArenaNoVirtual() == NULL && response_ != NULL) delete response_;
  response_ = NULL;
}

bool PredictLog::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:tensorflow.serving.PredictLog)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // optional .tensorflow.serving.PredictRequest request = 1;
      case 1: {
        if (tag == 10) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_request()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(18)) goto parse_response;
        break;
      }

      // optional .tensorflow.serving.PredictResponse response = 2;
      case 2: {
        if (tag == 18) {
         parse_response:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_response()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:tensorflow.serving.PredictLog)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:tensorflow.serving.PredictLog)
  return false;
#undef DO_
}

void PredictLog::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:tensorflow.serving.PredictLog)
  // optional .tensorflow.serving.PredictRequest request = 1;
  if (this->has_request()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      1, *this->request_, output);
  }

  // optional .tensorflow.serving.PredictResponse response = 2;
  if (this->has_response()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      2, *this->response_, output);
  }

  // @@protoc_insertion_point(serialize_end:tensorflow.serving.PredictLog)
}

::google::protobuf::uint8* PredictLog::InternalSerializeWithCachedSizesToArray(
    bool deterministic, ::google::protobuf::uint8* target) const {
  // @@protoc_insertion_point(serialize_to_array_start:tensorflow.serving.PredictLog)
  // optional .tensorflow.serving.PredictRequest request = 1;
  if (this->has_request()) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageNoVirtualToArray(
        1, *this->request_, false, target);
  }

  // optional .tensorflow.serving.PredictResponse response = 2;
  if (this->has_response()) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageNoVirtualToArray(
        2, *this->response_, false, target);
  }

  // @@protoc_insertion_point(serialize_to_array_end:tensorflow.serving.PredictLog)
  return target;
}

int PredictLog::ByteSize() const {
// @@protoc_insertion_point(message_byte_size_start:tensorflow.serving.PredictLog)
  int total_size = 0;

  // optional .tensorflow.serving.PredictRequest request = 1;
  if (this->has_request()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->request_);
  }

  // optional .tensorflow.serving.PredictResponse response = 2;
  if (this->has_response()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->response_);
  }

  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = total_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void PredictLog::MergeFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_merge_from_start:tensorflow.serving.PredictLog)
  if (GOOGLE_PREDICT_FALSE(&from == this)) {
    ::google::protobuf::internal::MergeFromFail(__FILE__, __LINE__);
  }
  const PredictLog* source = 
      ::google::protobuf::internal::DynamicCastToGenerated<const PredictLog>(
          &from);
  if (source == NULL) {
  // @@protoc_insertion_point(generalized_merge_from_cast_fail:tensorflow.serving.PredictLog)
    ::google::protobuf::internal::ReflectionOps::Merge(from, this);
  } else {
  // @@protoc_insertion_point(generalized_merge_from_cast_success:tensorflow.serving.PredictLog)
    MergeFrom(*source);
  }
}

void PredictLog::MergeFrom(const PredictLog& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:tensorflow.serving.PredictLog)
  if (GOOGLE_PREDICT_FALSE(&from == this)) {
    ::google::protobuf::internal::MergeFromFail(__FILE__, __LINE__);
  }
  if (from.has_request()) {
    mutable_request()->::tensorflow::serving::PredictRequest::MergeFrom(from.request());
  }
  if (from.has_response()) {
    mutable_response()->::tensorflow::serving::PredictResponse::MergeFrom(from.response());
  }
}

void PredictLog::CopyFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_copy_from_start:tensorflow.serving.PredictLog)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

void PredictLog::CopyFrom(const PredictLog& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:tensorflow.serving.PredictLog)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool PredictLog::IsInitialized() const {

  return true;
}

void PredictLog::Swap(PredictLog* other) {
  if (other == this) return;
  if (GetArenaNoVirtual() == other->GetArenaNoVirtual()) {
    InternalSwap(other);
  } else {
    PredictLog temp;
    temp.MergeFrom(*this);
    CopyFrom(*other);
    other->CopyFrom(temp);
  }
}
void PredictLog::UnsafeArenaSwap(PredictLog* other) {
  if (other == this) return;
  GOOGLE_DCHECK(GetArenaNoVirtual() == other->GetArenaNoVirtual());
  InternalSwap(other);
}
void PredictLog::InternalSwap(PredictLog* other) {
  std::swap(request_, other->request_);
  std::swap(response_, other->response_);
  _internal_metadata_.Swap(&other->_internal_metadata_);
  std::swap(_cached_size_, other->_cached_size_);
}

::google::protobuf::Metadata PredictLog::GetMetadata() const {
  protobuf_AssignDescriptorsOnce();
  ::google::protobuf::Metadata metadata;
  metadata.descriptor = PredictLog_descriptor_;
  metadata.reflection = PredictLog_reflection_;
  return metadata;
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// PredictLog

// optional .tensorflow.serving.PredictRequest request = 1;
bool PredictLog::has_request() const {
  return !_is_default_instance_ && request_ != NULL;
}
void PredictLog::clear_request() {
  if (GetArenaNoVirtual() == NULL && request_ != NULL) delete request_;
  request_ = NULL;
}
const ::tensorflow::serving::PredictRequest& PredictLog::request() const {
  // @@protoc_insertion_point(field_get:tensorflow.serving.PredictLog.request)
  return request_ != NULL ? *request_ : *default_instance_->request_;
}
::tensorflow::serving::PredictRequest* PredictLog::mutable_request() {
  
  if (request_ == NULL) {
    _slow_mutable_request();
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.serving.PredictLog.request)
  return request_;
}
::tensorflow::serving::PredictRequest* PredictLog::release_request() {
  // @@protoc_insertion_point(field_release:tensorflow.serving.PredictLog.request)
  
  if (GetArenaNoVirtual() != NULL) {
    return _slow_release_request();
  } else {
    ::tensorflow::serving::PredictRequest* temp = request_;
    request_ = NULL;
    return temp;
  }
}
 void PredictLog::set_allocated_request(::tensorflow::serving::PredictRequest* request) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete request_;
  }
  if (request != NULL) {
    _slow_set_allocated_request(message_arena, &request);
  }
  request_ = request;
  if (request) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:tensorflow.serving.PredictLog.request)
}

// optional .tensorflow.serving.PredictResponse response = 2;
bool PredictLog::has_response() const {
  return !_is_default_instance_ && response_ != NULL;
}
void PredictLog::clear_response() {
  if (GetArenaNoVirtual() == NULL && response_ != NULL) delete response_;
  response_ = NULL;
}
const ::tensorflow::serving::PredictResponse& PredictLog::response() const {
  // @@protoc_insertion_point(field_get:tensorflow.serving.PredictLog.response)
  return response_ != NULL ? *response_ : *default_instance_->response_;
}
::tensorflow::serving::PredictResponse* PredictLog::mutable_response() {
  
  if (response_ == NULL) {
    _slow_mutable_response();
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.serving.PredictLog.response)
  return response_;
}
::tensorflow::serving::PredictResponse* PredictLog::release_response() {
  // @@protoc_insertion_point(field_release:tensorflow.serving.PredictLog.response)
  
  if (GetArenaNoVirtual() != NULL) {
    return _slow_release_response();
  } else {
    ::tensorflow::serving::PredictResponse* temp = response_;
    response_ = NULL;
    return temp;
  }
}
 void PredictLog::set_allocated_response(::tensorflow::serving::PredictResponse* response) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete response_;
  }
  if (response != NULL) {
    _slow_set_allocated_response(message_arena, &response);
  }
  response_ = response;
  if (response) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:tensorflow.serving.PredictLog.response)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

void MultiInferenceLog::_slow_mutable_request() {
  request_ = ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::MultiInferenceRequest >(
      GetArenaNoVirtual());
}
::tensorflow::serving::MultiInferenceRequest* MultiInferenceLog::_slow_release_request() {
  if (request_ == NULL) {
    return NULL;
  } else {
    ::tensorflow::serving::MultiInferenceRequest* temp = new ::tensorflow::serving::MultiInferenceRequest;
    temp->MergeFrom(*request_);
    request_ = NULL;
    return temp;
  }
}
::tensorflow::serving::MultiInferenceRequest* MultiInferenceLog::unsafe_arena_release_request() {
  // @@protoc_insertion_point(field_unsafe_arena_release:tensorflow.serving.MultiInferenceLog.request)
  
  ::tensorflow::serving::MultiInferenceRequest* temp = request_;
  request_ = NULL;
  return temp;
}
void MultiInferenceLog::_slow_set_allocated_request(
    ::google::protobuf::Arena* message_arena, ::tensorflow::serving::MultiInferenceRequest** request) {
    if (message_arena != NULL && 
        ::google::protobuf::Arena::GetArena(*request) == NULL) {
      message_arena->Own(*request);
    } else if (message_arena !=
               ::google::protobuf::Arena::GetArena(*request)) {
      ::tensorflow::serving::MultiInferenceRequest* new_request = 
            ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::MultiInferenceRequest >(
            message_arena);
      new_request->CopyFrom(**request);
      *request = new_request;
    }
}
void MultiInferenceLog::unsafe_arena_set_allocated_request(
    ::tensorflow::serving::MultiInferenceRequest* request) {
  if (GetArenaNoVirtual() == NULL) {
    delete request_;
  }
  request_ = request;
  if (request) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:tensorflow.serving.MultiInferenceLog.request)
}
void MultiInferenceLog::_slow_mutable_response() {
  response_ = ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::MultiInferenceResponse >(
      GetArenaNoVirtual());
}
::tensorflow::serving::MultiInferenceResponse* MultiInferenceLog::_slow_release_response() {
  if (response_ == NULL) {
    return NULL;
  } else {
    ::tensorflow::serving::MultiInferenceResponse* temp = new ::tensorflow::serving::MultiInferenceResponse;
    temp->MergeFrom(*response_);
    response_ = NULL;
    return temp;
  }
}
::tensorflow::serving::MultiInferenceResponse* MultiInferenceLog::unsafe_arena_release_response() {
  // @@protoc_insertion_point(field_unsafe_arena_release:tensorflow.serving.MultiInferenceLog.response)
  
  ::tensorflow::serving::MultiInferenceResponse* temp = response_;
  response_ = NULL;
  return temp;
}
void MultiInferenceLog::_slow_set_allocated_response(
    ::google::protobuf::Arena* message_arena, ::tensorflow::serving::MultiInferenceResponse** response) {
    if (message_arena != NULL && 
        ::google::protobuf::Arena::GetArena(*response) == NULL) {
      message_arena->Own(*response);
    } else if (message_arena !=
               ::google::protobuf::Arena::GetArena(*response)) {
      ::tensorflow::serving::MultiInferenceResponse* new_response = 
            ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::MultiInferenceResponse >(
            message_arena);
      new_response->CopyFrom(**response);
      *response = new_response;
    }
}
void MultiInferenceLog::unsafe_arena_set_allocated_response(
    ::tensorflow::serving::MultiInferenceResponse* response) {
  if (GetArenaNoVirtual() == NULL) {
    delete response_;
  }
  response_ = response;
  if (response) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:tensorflow.serving.MultiInferenceLog.response)
}
#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int MultiInferenceLog::kRequestFieldNumber;
const int MultiInferenceLog::kResponseFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

MultiInferenceLog::MultiInferenceLog()
  : ::google::protobuf::Message(), _internal_metadata_(NULL) {
  SharedCtor();
  // @@protoc_insertion_point(constructor:tensorflow.serving.MultiInferenceLog)
}

MultiInferenceLog::MultiInferenceLog(::google::protobuf::Arena* arena)
  : ::google::protobuf::Message(),
  _internal_metadata_(arena) {
  SharedCtor();
  RegisterArenaDtor(arena);
  // @@protoc_insertion_point(arena_constructor:tensorflow.serving.MultiInferenceLog)
}

void MultiInferenceLog::InitAsDefaultInstance() {
  _is_default_instance_ = true;
  request_ = const_cast< ::tensorflow::serving::MultiInferenceRequest*>(&::tensorflow::serving::MultiInferenceRequest::default_instance());
  response_ = const_cast< ::tensorflow::serving::MultiInferenceResponse*>(&::tensorflow::serving::MultiInferenceResponse::default_instance());
}

MultiInferenceLog::MultiInferenceLog(const MultiInferenceLog& from)
  : ::google::protobuf::Message(),
    _internal_metadata_(NULL) {
  SharedCtor();
  MergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:tensorflow.serving.MultiInferenceLog)
}

void MultiInferenceLog::SharedCtor() {
    _is_default_instance_ = false;
  _cached_size_ = 0;
  request_ = NULL;
  response_ = NULL;
}

MultiInferenceLog::~MultiInferenceLog() {
  // @@protoc_insertion_point(destructor:tensorflow.serving.MultiInferenceLog)
  SharedDtor();
}

void MultiInferenceLog::SharedDtor() {
  if (GetArenaNoVirtual() != NULL) {
    return;
  }

  if (this != default_instance_) {
    delete request_;
    delete response_;
  }
}

void MultiInferenceLog::ArenaDtor(void* object) {
  MultiInferenceLog* _this = reinterpret_cast< MultiInferenceLog* >(object);
  (void)_this;
}
void MultiInferenceLog::RegisterArenaDtor(::google::protobuf::Arena* arena) {
}
void MultiInferenceLog::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ::google::protobuf::Descriptor* MultiInferenceLog::descriptor() {
  protobuf_AssignDescriptorsOnce();
  return MultiInferenceLog_descriptor_;
}

const MultiInferenceLog& MultiInferenceLog::default_instance() {
  if (default_instance_ == NULL) protobuf_AddDesc_tensorflow_5fserving_2fapis_2fprediction_5flog_2eproto();
  return *default_instance_;
}

MultiInferenceLog* MultiInferenceLog::default_instance_ = NULL;

MultiInferenceLog* MultiInferenceLog::New(::google::protobuf::Arena* arena) const {
  return ::google::protobuf::Arena::CreateMessage<MultiInferenceLog>(arena);
}

void MultiInferenceLog::Clear() {
// @@protoc_insertion_point(message_clear_start:tensorflow.serving.MultiInferenceLog)
  if (GetArenaNoVirtual() == NULL && request_ != NULL) delete request_;
  request_ = NULL;
  if (GetArenaNoVirtual() == NULL && response_ != NULL) delete response_;
  response_ = NULL;
}

bool MultiInferenceLog::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:tensorflow.serving.MultiInferenceLog)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // optional .tensorflow.serving.MultiInferenceRequest request = 1;
      case 1: {
        if (tag == 10) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_request()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(18)) goto parse_response;
        break;
      }

      // optional .tensorflow.serving.MultiInferenceResponse response = 2;
      case 2: {
        if (tag == 18) {
         parse_response:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_response()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:tensorflow.serving.MultiInferenceLog)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:tensorflow.serving.MultiInferenceLog)
  return false;
#undef DO_
}

void MultiInferenceLog::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:tensorflow.serving.MultiInferenceLog)
  // optional .tensorflow.serving.MultiInferenceRequest request = 1;
  if (this->has_request()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      1, *this->request_, output);
  }

  // optional .tensorflow.serving.MultiInferenceResponse response = 2;
  if (this->has_response()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      2, *this->response_, output);
  }

  // @@protoc_insertion_point(serialize_end:tensorflow.serving.MultiInferenceLog)
}

::google::protobuf::uint8* MultiInferenceLog::InternalSerializeWithCachedSizesToArray(
    bool deterministic, ::google::protobuf::uint8* target) const {
  // @@protoc_insertion_point(serialize_to_array_start:tensorflow.serving.MultiInferenceLog)
  // optional .tensorflow.serving.MultiInferenceRequest request = 1;
  if (this->has_request()) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageNoVirtualToArray(
        1, *this->request_, false, target);
  }

  // optional .tensorflow.serving.MultiInferenceResponse response = 2;
  if (this->has_response()) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageNoVirtualToArray(
        2, *this->response_, false, target);
  }

  // @@protoc_insertion_point(serialize_to_array_end:tensorflow.serving.MultiInferenceLog)
  return target;
}

int MultiInferenceLog::ByteSize() const {
// @@protoc_insertion_point(message_byte_size_start:tensorflow.serving.MultiInferenceLog)
  int total_size = 0;

  // optional .tensorflow.serving.MultiInferenceRequest request = 1;
  if (this->has_request()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->request_);
  }

  // optional .tensorflow.serving.MultiInferenceResponse response = 2;
  if (this->has_response()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->response_);
  }

  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = total_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void MultiInferenceLog::MergeFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_merge_from_start:tensorflow.serving.MultiInferenceLog)
  if (GOOGLE_PREDICT_FALSE(&from == this)) {
    ::google::protobuf::internal::MergeFromFail(__FILE__, __LINE__);
  }
  const MultiInferenceLog* source = 
      ::google::protobuf::internal::DynamicCastToGenerated<const MultiInferenceLog>(
          &from);
  if (source == NULL) {
  // @@protoc_insertion_point(generalized_merge_from_cast_fail:tensorflow.serving.MultiInferenceLog)
    ::google::protobuf::internal::ReflectionOps::Merge(from, this);
  } else {
  // @@protoc_insertion_point(generalized_merge_from_cast_success:tensorflow.serving.MultiInferenceLog)
    MergeFrom(*source);
  }
}

void MultiInferenceLog::MergeFrom(const MultiInferenceLog& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:tensorflow.serving.MultiInferenceLog)
  if (GOOGLE_PREDICT_FALSE(&from == this)) {
    ::google::protobuf::internal::MergeFromFail(__FILE__, __LINE__);
  }
  if (from.has_request()) {
    mutable_request()->::tensorflow::serving::MultiInferenceRequest::MergeFrom(from.request());
  }
  if (from.has_response()) {
    mutable_response()->::tensorflow::serving::MultiInferenceResponse::MergeFrom(from.response());
  }
}

void MultiInferenceLog::CopyFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_copy_from_start:tensorflow.serving.MultiInferenceLog)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

void MultiInferenceLog::CopyFrom(const MultiInferenceLog& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:tensorflow.serving.MultiInferenceLog)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool MultiInferenceLog::IsInitialized() const {

  return true;
}

void MultiInferenceLog::Swap(MultiInferenceLog* other) {
  if (other == this) return;
  if (GetArenaNoVirtual() == other->GetArenaNoVirtual()) {
    InternalSwap(other);
  } else {
    MultiInferenceLog temp;
    temp.MergeFrom(*this);
    CopyFrom(*other);
    other->CopyFrom(temp);
  }
}
void MultiInferenceLog::UnsafeArenaSwap(MultiInferenceLog* other) {
  if (other == this) return;
  GOOGLE_DCHECK(GetArenaNoVirtual() == other->GetArenaNoVirtual());
  InternalSwap(other);
}
void MultiInferenceLog::InternalSwap(MultiInferenceLog* other) {
  std::swap(request_, other->request_);
  std::swap(response_, other->response_);
  _internal_metadata_.Swap(&other->_internal_metadata_);
  std::swap(_cached_size_, other->_cached_size_);
}

::google::protobuf::Metadata MultiInferenceLog::GetMetadata() const {
  protobuf_AssignDescriptorsOnce();
  ::google::protobuf::Metadata metadata;
  metadata.descriptor = MultiInferenceLog_descriptor_;
  metadata.reflection = MultiInferenceLog_reflection_;
  return metadata;
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// MultiInferenceLog

// optional .tensorflow.serving.MultiInferenceRequest request = 1;
bool MultiInferenceLog::has_request() const {
  return !_is_default_instance_ && request_ != NULL;
}
void MultiInferenceLog::clear_request() {
  if (GetArenaNoVirtual() == NULL && request_ != NULL) delete request_;
  request_ = NULL;
}
const ::tensorflow::serving::MultiInferenceRequest& MultiInferenceLog::request() const {
  // @@protoc_insertion_point(field_get:tensorflow.serving.MultiInferenceLog.request)
  return request_ != NULL ? *request_ : *default_instance_->request_;
}
::tensorflow::serving::MultiInferenceRequest* MultiInferenceLog::mutable_request() {
  
  if (request_ == NULL) {
    _slow_mutable_request();
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.serving.MultiInferenceLog.request)
  return request_;
}
::tensorflow::serving::MultiInferenceRequest* MultiInferenceLog::release_request() {
  // @@protoc_insertion_point(field_release:tensorflow.serving.MultiInferenceLog.request)
  
  if (GetArenaNoVirtual() != NULL) {
    return _slow_release_request();
  } else {
    ::tensorflow::serving::MultiInferenceRequest* temp = request_;
    request_ = NULL;
    return temp;
  }
}
 void MultiInferenceLog::set_allocated_request(::tensorflow::serving::MultiInferenceRequest* request) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete request_;
  }
  if (request != NULL) {
    _slow_set_allocated_request(message_arena, &request);
  }
  request_ = request;
  if (request) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:tensorflow.serving.MultiInferenceLog.request)
}

// optional .tensorflow.serving.MultiInferenceResponse response = 2;
bool MultiInferenceLog::has_response() const {
  return !_is_default_instance_ && response_ != NULL;
}
void MultiInferenceLog::clear_response() {
  if (GetArenaNoVirtual() == NULL && response_ != NULL) delete response_;
  response_ = NULL;
}
const ::tensorflow::serving::MultiInferenceResponse& MultiInferenceLog::response() const {
  // @@protoc_insertion_point(field_get:tensorflow.serving.MultiInferenceLog.response)
  return response_ != NULL ? *response_ : *default_instance_->response_;
}
::tensorflow::serving::MultiInferenceResponse* MultiInferenceLog::mutable_response() {
  
  if (response_ == NULL) {
    _slow_mutable_response();
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.serving.MultiInferenceLog.response)
  return response_;
}
::tensorflow::serving::MultiInferenceResponse* MultiInferenceLog::release_response() {
  // @@protoc_insertion_point(field_release:tensorflow.serving.MultiInferenceLog.response)
  
  if (GetArenaNoVirtual() != NULL) {
    return _slow_release_response();
  } else {
    ::tensorflow::serving::MultiInferenceResponse* temp = response_;
    response_ = NULL;
    return temp;
  }
}
 void MultiInferenceLog::set_allocated_response(::tensorflow::serving::MultiInferenceResponse* response) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete response_;
  }
  if (response != NULL) {
    _slow_set_allocated_response(message_arena, &response);
  }
  response_ = response;
  if (response) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:tensorflow.serving.MultiInferenceLog.response)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

void SessionRunLog::_slow_mutable_request() {
  request_ = ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::SessionRunRequest >(
      GetArenaNoVirtual());
}
::tensorflow::serving::SessionRunRequest* SessionRunLog::_slow_release_request() {
  if (request_ == NULL) {
    return NULL;
  } else {
    ::tensorflow::serving::SessionRunRequest* temp = new ::tensorflow::serving::SessionRunRequest;
    temp->MergeFrom(*request_);
    request_ = NULL;
    return temp;
  }
}
::tensorflow::serving::SessionRunRequest* SessionRunLog::unsafe_arena_release_request() {
  // @@protoc_insertion_point(field_unsafe_arena_release:tensorflow.serving.SessionRunLog.request)
  
  ::tensorflow::serving::SessionRunRequest* temp = request_;
  request_ = NULL;
  return temp;
}
void SessionRunLog::_slow_set_allocated_request(
    ::google::protobuf::Arena* message_arena, ::tensorflow::serving::SessionRunRequest** request) {
    if (message_arena != NULL && 
        ::google::protobuf::Arena::GetArena(*request) == NULL) {
      message_arena->Own(*request);
    } else if (message_arena !=
               ::google::protobuf::Arena::GetArena(*request)) {
      ::tensorflow::serving::SessionRunRequest* new_request = 
            ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::SessionRunRequest >(
            message_arena);
      new_request->CopyFrom(**request);
      *request = new_request;
    }
}
void SessionRunLog::unsafe_arena_set_allocated_request(
    ::tensorflow::serving::SessionRunRequest* request) {
  if (GetArenaNoVirtual() == NULL) {
    delete request_;
  }
  request_ = request;
  if (request) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:tensorflow.serving.SessionRunLog.request)
}
void SessionRunLog::_slow_mutable_response() {
  response_ = ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::SessionRunResponse >(
      GetArenaNoVirtual());
}
::tensorflow::serving::SessionRunResponse* SessionRunLog::_slow_release_response() {
  if (response_ == NULL) {
    return NULL;
  } else {
    ::tensorflow::serving::SessionRunResponse* temp = new ::tensorflow::serving::SessionRunResponse;
    temp->MergeFrom(*response_);
    response_ = NULL;
    return temp;
  }
}
::tensorflow::serving::SessionRunResponse* SessionRunLog::unsafe_arena_release_response() {
  // @@protoc_insertion_point(field_unsafe_arena_release:tensorflow.serving.SessionRunLog.response)
  
  ::tensorflow::serving::SessionRunResponse* temp = response_;
  response_ = NULL;
  return temp;
}
void SessionRunLog::_slow_set_allocated_response(
    ::google::protobuf::Arena* message_arena, ::tensorflow::serving::SessionRunResponse** response) {
    if (message_arena != NULL && 
        ::google::protobuf::Arena::GetArena(*response) == NULL) {
      message_arena->Own(*response);
    } else if (message_arena !=
               ::google::protobuf::Arena::GetArena(*response)) {
      ::tensorflow::serving::SessionRunResponse* new_response = 
            ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::SessionRunResponse >(
            message_arena);
      new_response->CopyFrom(**response);
      *response = new_response;
    }
}
void SessionRunLog::unsafe_arena_set_allocated_response(
    ::tensorflow::serving::SessionRunResponse* response) {
  if (GetArenaNoVirtual() == NULL) {
    delete response_;
  }
  response_ = response;
  if (response) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:tensorflow.serving.SessionRunLog.response)
}
#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int SessionRunLog::kRequestFieldNumber;
const int SessionRunLog::kResponseFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

SessionRunLog::SessionRunLog()
  : ::google::protobuf::Message(), _internal_metadata_(NULL) {
  SharedCtor();
  // @@protoc_insertion_point(constructor:tensorflow.serving.SessionRunLog)
}

SessionRunLog::SessionRunLog(::google::protobuf::Arena* arena)
  : ::google::protobuf::Message(),
  _internal_metadata_(arena) {
  SharedCtor();
  RegisterArenaDtor(arena);
  // @@protoc_insertion_point(arena_constructor:tensorflow.serving.SessionRunLog)
}

void SessionRunLog::InitAsDefaultInstance() {
  _is_default_instance_ = true;
  request_ = const_cast< ::tensorflow::serving::SessionRunRequest*>(&::tensorflow::serving::SessionRunRequest::default_instance());
  response_ = const_cast< ::tensorflow::serving::SessionRunResponse*>(&::tensorflow::serving::SessionRunResponse::default_instance());
}

SessionRunLog::SessionRunLog(const SessionRunLog& from)
  : ::google::protobuf::Message(),
    _internal_metadata_(NULL) {
  SharedCtor();
  MergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:tensorflow.serving.SessionRunLog)
}

void SessionRunLog::SharedCtor() {
    _is_default_instance_ = false;
  _cached_size_ = 0;
  request_ = NULL;
  response_ = NULL;
}

SessionRunLog::~SessionRunLog() {
  // @@protoc_insertion_point(destructor:tensorflow.serving.SessionRunLog)
  SharedDtor();
}

void SessionRunLog::SharedDtor() {
  if (GetArenaNoVirtual() != NULL) {
    return;
  }

  if (this != default_instance_) {
    delete request_;
    delete response_;
  }
}

void SessionRunLog::ArenaDtor(void* object) {
  SessionRunLog* _this = reinterpret_cast< SessionRunLog* >(object);
  (void)_this;
}
void SessionRunLog::RegisterArenaDtor(::google::protobuf::Arena* arena) {
}
void SessionRunLog::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ::google::protobuf::Descriptor* SessionRunLog::descriptor() {
  protobuf_AssignDescriptorsOnce();
  return SessionRunLog_descriptor_;
}

const SessionRunLog& SessionRunLog::default_instance() {
  if (default_instance_ == NULL) protobuf_AddDesc_tensorflow_5fserving_2fapis_2fprediction_5flog_2eproto();
  return *default_instance_;
}

SessionRunLog* SessionRunLog::default_instance_ = NULL;

SessionRunLog* SessionRunLog::New(::google::protobuf::Arena* arena) const {
  return ::google::protobuf::Arena::CreateMessage<SessionRunLog>(arena);
}

void SessionRunLog::Clear() {
// @@protoc_insertion_point(message_clear_start:tensorflow.serving.SessionRunLog)
  if (GetArenaNoVirtual() == NULL && request_ != NULL) delete request_;
  request_ = NULL;
  if (GetArenaNoVirtual() == NULL && response_ != NULL) delete response_;
  response_ = NULL;
}

bool SessionRunLog::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:tensorflow.serving.SessionRunLog)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // optional .tensorflow.serving.SessionRunRequest request = 1;
      case 1: {
        if (tag == 10) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_request()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(18)) goto parse_response;
        break;
      }

      // optional .tensorflow.serving.SessionRunResponse response = 2;
      case 2: {
        if (tag == 18) {
         parse_response:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_response()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:tensorflow.serving.SessionRunLog)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:tensorflow.serving.SessionRunLog)
  return false;
#undef DO_
}

void SessionRunLog::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:tensorflow.serving.SessionRunLog)
  // optional .tensorflow.serving.SessionRunRequest request = 1;
  if (this->has_request()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      1, *this->request_, output);
  }

  // optional .tensorflow.serving.SessionRunResponse response = 2;
  if (this->has_response()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      2, *this->response_, output);
  }

  // @@protoc_insertion_point(serialize_end:tensorflow.serving.SessionRunLog)
}

::google::protobuf::uint8* SessionRunLog::InternalSerializeWithCachedSizesToArray(
    bool deterministic, ::google::protobuf::uint8* target) const {
  // @@protoc_insertion_point(serialize_to_array_start:tensorflow.serving.SessionRunLog)
  // optional .tensorflow.serving.SessionRunRequest request = 1;
  if (this->has_request()) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageNoVirtualToArray(
        1, *this->request_, false, target);
  }

  // optional .tensorflow.serving.SessionRunResponse response = 2;
  if (this->has_response()) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageNoVirtualToArray(
        2, *this->response_, false, target);
  }

  // @@protoc_insertion_point(serialize_to_array_end:tensorflow.serving.SessionRunLog)
  return target;
}

int SessionRunLog::ByteSize() const {
// @@protoc_insertion_point(message_byte_size_start:tensorflow.serving.SessionRunLog)
  int total_size = 0;

  // optional .tensorflow.serving.SessionRunRequest request = 1;
  if (this->has_request()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->request_);
  }

  // optional .tensorflow.serving.SessionRunResponse response = 2;
  if (this->has_response()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->response_);
  }

  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = total_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void SessionRunLog::MergeFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_merge_from_start:tensorflow.serving.SessionRunLog)
  if (GOOGLE_PREDICT_FALSE(&from == this)) {
    ::google::protobuf::internal::MergeFromFail(__FILE__, __LINE__);
  }
  const SessionRunLog* source = 
      ::google::protobuf::internal::DynamicCastToGenerated<const SessionRunLog>(
          &from);
  if (source == NULL) {
  // @@protoc_insertion_point(generalized_merge_from_cast_fail:tensorflow.serving.SessionRunLog)
    ::google::protobuf::internal::ReflectionOps::Merge(from, this);
  } else {
  // @@protoc_insertion_point(generalized_merge_from_cast_success:tensorflow.serving.SessionRunLog)
    MergeFrom(*source);
  }
}

void SessionRunLog::MergeFrom(const SessionRunLog& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:tensorflow.serving.SessionRunLog)
  if (GOOGLE_PREDICT_FALSE(&from == this)) {
    ::google::protobuf::internal::MergeFromFail(__FILE__, __LINE__);
  }
  if (from.has_request()) {
    mutable_request()->::tensorflow::serving::SessionRunRequest::MergeFrom(from.request());
  }
  if (from.has_response()) {
    mutable_response()->::tensorflow::serving::SessionRunResponse::MergeFrom(from.response());
  }
}

void SessionRunLog::CopyFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_copy_from_start:tensorflow.serving.SessionRunLog)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

void SessionRunLog::CopyFrom(const SessionRunLog& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:tensorflow.serving.SessionRunLog)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool SessionRunLog::IsInitialized() const {

  return true;
}

void SessionRunLog::Swap(SessionRunLog* other) {
  if (other == this) return;
  if (GetArenaNoVirtual() == other->GetArenaNoVirtual()) {
    InternalSwap(other);
  } else {
    SessionRunLog temp;
    temp.MergeFrom(*this);
    CopyFrom(*other);
    other->CopyFrom(temp);
  }
}
void SessionRunLog::UnsafeArenaSwap(SessionRunLog* other) {
  if (other == this) return;
  GOOGLE_DCHECK(GetArenaNoVirtual() == other->GetArenaNoVirtual());
  InternalSwap(other);
}
void SessionRunLog::InternalSwap(SessionRunLog* other) {
  std::swap(request_, other->request_);
  std::swap(response_, other->response_);
  _internal_metadata_.Swap(&other->_internal_metadata_);
  std::swap(_cached_size_, other->_cached_size_);
}

::google::protobuf::Metadata SessionRunLog::GetMetadata() const {
  protobuf_AssignDescriptorsOnce();
  ::google::protobuf::Metadata metadata;
  metadata.descriptor = SessionRunLog_descriptor_;
  metadata.reflection = SessionRunLog_reflection_;
  return metadata;
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// SessionRunLog

// optional .tensorflow.serving.SessionRunRequest request = 1;
bool SessionRunLog::has_request() const {
  return !_is_default_instance_ && request_ != NULL;
}
void SessionRunLog::clear_request() {
  if (GetArenaNoVirtual() == NULL && request_ != NULL) delete request_;
  request_ = NULL;
}
const ::tensorflow::serving::SessionRunRequest& SessionRunLog::request() const {
  // @@protoc_insertion_point(field_get:tensorflow.serving.SessionRunLog.request)
  return request_ != NULL ? *request_ : *default_instance_->request_;
}
::tensorflow::serving::SessionRunRequest* SessionRunLog::mutable_request() {
  
  if (request_ == NULL) {
    _slow_mutable_request();
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.serving.SessionRunLog.request)
  return request_;
}
::tensorflow::serving::SessionRunRequest* SessionRunLog::release_request() {
  // @@protoc_insertion_point(field_release:tensorflow.serving.SessionRunLog.request)
  
  if (GetArenaNoVirtual() != NULL) {
    return _slow_release_request();
  } else {
    ::tensorflow::serving::SessionRunRequest* temp = request_;
    request_ = NULL;
    return temp;
  }
}
 void SessionRunLog::set_allocated_request(::tensorflow::serving::SessionRunRequest* request) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete request_;
  }
  if (request != NULL) {
    _slow_set_allocated_request(message_arena, &request);
  }
  request_ = request;
  if (request) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:tensorflow.serving.SessionRunLog.request)
}

// optional .tensorflow.serving.SessionRunResponse response = 2;
bool SessionRunLog::has_response() const {
  return !_is_default_instance_ && response_ != NULL;
}
void SessionRunLog::clear_response() {
  if (GetArenaNoVirtual() == NULL && response_ != NULL) delete response_;
  response_ = NULL;
}
const ::tensorflow::serving::SessionRunResponse& SessionRunLog::response() const {
  // @@protoc_insertion_point(field_get:tensorflow.serving.SessionRunLog.response)
  return response_ != NULL ? *response_ : *default_instance_->response_;
}
::tensorflow::serving::SessionRunResponse* SessionRunLog::mutable_response() {
  
  if (response_ == NULL) {
    _slow_mutable_response();
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.serving.SessionRunLog.response)
  return response_;
}
::tensorflow::serving::SessionRunResponse* SessionRunLog::release_response() {
  // @@protoc_insertion_point(field_release:tensorflow.serving.SessionRunLog.response)
  
  if (GetArenaNoVirtual() != NULL) {
    return _slow_release_response();
  } else {
    ::tensorflow::serving::SessionRunResponse* temp = response_;
    response_ = NULL;
    return temp;
  }
}
 void SessionRunLog::set_allocated_response(::tensorflow::serving::SessionRunResponse* response) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete response_;
  }
  if (response != NULL) {
    _slow_set_allocated_response(message_arena, &response);
  }
  response_ = response;
  if (response) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:tensorflow.serving.SessionRunLog.response)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

void PredictionLog::_slow_mutable_log_metadata() {
  log_metadata_ = ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::LogMetadata >(
      GetArenaNoVirtual());
}
::tensorflow::serving::LogMetadata* PredictionLog::_slow_release_log_metadata() {
  if (log_metadata_ == NULL) {
    return NULL;
  } else {
    ::tensorflow::serving::LogMetadata* temp = new ::tensorflow::serving::LogMetadata;
    temp->MergeFrom(*log_metadata_);
    log_metadata_ = NULL;
    return temp;
  }
}
::tensorflow::serving::LogMetadata* PredictionLog::unsafe_arena_release_log_metadata() {
  // @@protoc_insertion_point(field_unsafe_arena_release:tensorflow.serving.PredictionLog.log_metadata)
  
  ::tensorflow::serving::LogMetadata* temp = log_metadata_;
  log_metadata_ = NULL;
  return temp;
}
void PredictionLog::_slow_set_allocated_log_metadata(
    ::google::protobuf::Arena* message_arena, ::tensorflow::serving::LogMetadata** log_metadata) {
    if (message_arena != NULL && 
        ::google::protobuf::Arena::GetArena(*log_metadata) == NULL) {
      message_arena->Own(*log_metadata);
    } else if (message_arena !=
               ::google::protobuf::Arena::GetArena(*log_metadata)) {
      ::tensorflow::serving::LogMetadata* new_log_metadata = 
            ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::LogMetadata >(
            message_arena);
      new_log_metadata->CopyFrom(**log_metadata);
      *log_metadata = new_log_metadata;
    }
}
void PredictionLog::unsafe_arena_set_allocated_log_metadata(
    ::tensorflow::serving::LogMetadata* log_metadata) {
  if (GetArenaNoVirtual() == NULL) {
    delete log_metadata_;
  }
  log_metadata_ = log_metadata;
  if (log_metadata) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:tensorflow.serving.PredictionLog.log_metadata)
}
#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int PredictionLog::kLogMetadataFieldNumber;
const int PredictionLog::kClassifyLogFieldNumber;
const int PredictionLog::kRegressLogFieldNumber;
const int PredictionLog::kPredictLogFieldNumber;
const int PredictionLog::kMultiInferenceLogFieldNumber;
const int PredictionLog::kSessionRunLogFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

PredictionLog::PredictionLog()
  : ::google::protobuf::Message(), _internal_metadata_(NULL) {
  SharedCtor();
  // @@protoc_insertion_point(constructor:tensorflow.serving.PredictionLog)
}

PredictionLog::PredictionLog(::google::protobuf::Arena* arena)
  : ::google::protobuf::Message(),
  _internal_metadata_(arena) {
  SharedCtor();
  RegisterArenaDtor(arena);
  // @@protoc_insertion_point(arena_constructor:tensorflow.serving.PredictionLog)
}

void PredictionLog::InitAsDefaultInstance() {
  _is_default_instance_ = true;
  log_metadata_ = const_cast< ::tensorflow::serving::LogMetadata*>(&::tensorflow::serving::LogMetadata::default_instance());
  PredictionLog_default_oneof_instance_->classify_log_ = const_cast< ::tensorflow::serving::ClassifyLog*>(&::tensorflow::serving::ClassifyLog::default_instance());
  PredictionLog_default_oneof_instance_->regress_log_ = const_cast< ::tensorflow::serving::RegressLog*>(&::tensorflow::serving::RegressLog::default_instance());
  PredictionLog_default_oneof_instance_->predict_log_ = const_cast< ::tensorflow::serving::PredictLog*>(&::tensorflow::serving::PredictLog::default_instance());
  PredictionLog_default_oneof_instance_->multi_inference_log_ = const_cast< ::tensorflow::serving::MultiInferenceLog*>(&::tensorflow::serving::MultiInferenceLog::default_instance());
  PredictionLog_default_oneof_instance_->session_run_log_ = const_cast< ::tensorflow::serving::SessionRunLog*>(&::tensorflow::serving::SessionRunLog::default_instance());
}

PredictionLog::PredictionLog(const PredictionLog& from)
  : ::google::protobuf::Message(),
    _internal_metadata_(NULL) {
  SharedCtor();
  MergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:tensorflow.serving.PredictionLog)
}

void PredictionLog::SharedCtor() {
    _is_default_instance_ = false;
  _cached_size_ = 0;
  log_metadata_ = NULL;
  clear_has_log_type();
}

PredictionLog::~PredictionLog() {
  // @@protoc_insertion_point(destructor:tensorflow.serving.PredictionLog)
  SharedDtor();
}

void PredictionLog::SharedDtor() {
  if (GetArenaNoVirtual() != NULL) {
    return;
  }

  if (has_log_type()) {
    clear_log_type();
  }
  if (this != default_instance_) {
    delete log_metadata_;
  }
}

void PredictionLog::ArenaDtor(void* object) {
  PredictionLog* _this = reinterpret_cast< PredictionLog* >(object);
  (void)_this;
}
void PredictionLog::RegisterArenaDtor(::google::protobuf::Arena* arena) {
}
void PredictionLog::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ::google::protobuf::Descriptor* PredictionLog::descriptor() {
  protobuf_AssignDescriptorsOnce();
  return PredictionLog_descriptor_;
}

const PredictionLog& PredictionLog::default_instance() {
  if (default_instance_ == NULL) protobuf_AddDesc_tensorflow_5fserving_2fapis_2fprediction_5flog_2eproto();
  return *default_instance_;
}

PredictionLog* PredictionLog::default_instance_ = NULL;

PredictionLog* PredictionLog::New(::google::protobuf::Arena* arena) const {
  return ::google::protobuf::Arena::CreateMessage<PredictionLog>(arena);
}

void PredictionLog::clear_log_type() {
// @@protoc_insertion_point(one_of_clear_start:tensorflow.serving.PredictionLog)
  switch(log_type_case()) {
    case kClassifyLog: {
      if (GetArenaNoVirtual() == NULL) {
        delete log_type_.classify_log_;
      }
      break;
    }
    case kRegressLog: {
      if (GetArenaNoVirtual() == NULL) {
        delete log_type_.regress_log_;
      }
      break;
    }
    case kPredictLog: {
      if (GetArenaNoVirtual() == NULL) {
        delete log_type_.predict_log_;
      }
      break;
    }
    case kMultiInferenceLog: {
      if (GetArenaNoVirtual() == NULL) {
        delete log_type_.multi_inference_log_;
      }
      break;
    }
    case kSessionRunLog: {
      if (GetArenaNoVirtual() == NULL) {
        delete log_type_.session_run_log_;
      }
      break;
    }
    case LOG_TYPE_NOT_SET: {
      break;
    }
  }
  _oneof_case_[0] = LOG_TYPE_NOT_SET;
}


void PredictionLog::Clear() {
// @@protoc_insertion_point(message_clear_start:tensorflow.serving.PredictionLog)
  if (GetArenaNoVirtual() == NULL && log_metadata_ != NULL) delete log_metadata_;
  log_metadata_ = NULL;
  clear_log_type();
}

bool PredictionLog::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:tensorflow.serving.PredictionLog)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // optional .tensorflow.serving.LogMetadata log_metadata = 1;
      case 1: {
        if (tag == 10) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_log_metadata()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(18)) goto parse_classify_log;
        break;
      }

      // optional .tensorflow.serving.ClassifyLog classify_log = 2;
      case 2: {
        if (tag == 18) {
         parse_classify_log:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_classify_log()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(26)) goto parse_regress_log;
        break;
      }

      // optional .tensorflow.serving.RegressLog regress_log = 3;
      case 3: {
        if (tag == 26) {
         parse_regress_log:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_regress_log()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(34)) goto parse_multi_inference_log;
        break;
      }

      // optional .tensorflow.serving.MultiInferenceLog multi_inference_log = 4;
      case 4: {
        if (tag == 34) {
         parse_multi_inference_log:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_multi_inference_log()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(42)) goto parse_session_run_log;
        break;
      }

      // optional .tensorflow.serving.SessionRunLog session_run_log = 5;
      case 5: {
        if (tag == 42) {
         parse_session_run_log:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_session_run_log()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(50)) goto parse_predict_log;
        break;
      }

      // optional .tensorflow.serving.PredictLog predict_log = 6;
      case 6: {
        if (tag == 50) {
         parse_predict_log:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_predict_log()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:tensorflow.serving.PredictionLog)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:tensorflow.serving.PredictionLog)
  return false;
#undef DO_
}

void PredictionLog::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:tensorflow.serving.PredictionLog)
  // optional .tensorflow.serving.LogMetadata log_metadata = 1;
  if (this->has_log_metadata()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      1, *this->log_metadata_, output);
  }

  // optional .tensorflow.serving.ClassifyLog classify_log = 2;
  if (has_classify_log()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      2, *log_type_.classify_log_, output);
  }

  // optional .tensorflow.serving.RegressLog regress_log = 3;
  if (has_regress_log()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      3, *log_type_.regress_log_, output);
  }

  // optional .tensorflow.serving.MultiInferenceLog multi_inference_log = 4;
  if (has_multi_inference_log()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      4, *log_type_.multi_inference_log_, output);
  }

  // optional .tensorflow.serving.SessionRunLog session_run_log = 5;
  if (has_session_run_log()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      5, *log_type_.session_run_log_, output);
  }

  // optional .tensorflow.serving.PredictLog predict_log = 6;
  if (has_predict_log()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      6, *log_type_.predict_log_, output);
  }

  // @@protoc_insertion_point(serialize_end:tensorflow.serving.PredictionLog)
}

::google::protobuf::uint8* PredictionLog::InternalSerializeWithCachedSizesToArray(
    bool deterministic, ::google::protobuf::uint8* target) const {
  // @@protoc_insertion_point(serialize_to_array_start:tensorflow.serving.PredictionLog)
  // optional .tensorflow.serving.LogMetadata log_metadata = 1;
  if (this->has_log_metadata()) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageNoVirtualToArray(
        1, *this->log_metadata_, false, target);
  }

  // optional .tensorflow.serving.ClassifyLog classify_log = 2;
  if (has_classify_log()) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageNoVirtualToArray(
        2, *log_type_.classify_log_, false, target);
  }

  // optional .tensorflow.serving.RegressLog regress_log = 3;
  if (has_regress_log()) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageNoVirtualToArray(
        3, *log_type_.regress_log_, false, target);
  }

  // optional .tensorflow.serving.MultiInferenceLog multi_inference_log = 4;
  if (has_multi_inference_log()) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageNoVirtualToArray(
        4, *log_type_.multi_inference_log_, false, target);
  }

  // optional .tensorflow.serving.SessionRunLog session_run_log = 5;
  if (has_session_run_log()) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageNoVirtualToArray(
        5, *log_type_.session_run_log_, false, target);
  }

  // optional .tensorflow.serving.PredictLog predict_log = 6;
  if (has_predict_log()) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageNoVirtualToArray(
        6, *log_type_.predict_log_, false, target);
  }

  // @@protoc_insertion_point(serialize_to_array_end:tensorflow.serving.PredictionLog)
  return target;
}

int PredictionLog::ByteSize() const {
// @@protoc_insertion_point(message_byte_size_start:tensorflow.serving.PredictionLog)
  int total_size = 0;

  // optional .tensorflow.serving.LogMetadata log_metadata = 1;
  if (this->has_log_metadata()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->log_metadata_);
  }

  switch (log_type_case()) {
    // optional .tensorflow.serving.ClassifyLog classify_log = 2;
    case kClassifyLog: {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *log_type_.classify_log_);
      break;
    }
    // optional .tensorflow.serving.RegressLog regress_log = 3;
    case kRegressLog: {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *log_type_.regress_log_);
      break;
    }
    // optional .tensorflow.serving.PredictLog predict_log = 6;
    case kPredictLog: {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *log_type_.predict_log_);
      break;
    }
    // optional .tensorflow.serving.MultiInferenceLog multi_inference_log = 4;
    case kMultiInferenceLog: {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *log_type_.multi_inference_log_);
      break;
    }
    // optional .tensorflow.serving.SessionRunLog session_run_log = 5;
    case kSessionRunLog: {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
          *log_type_.session_run_log_);
      break;
    }
    case LOG_TYPE_NOT_SET: {
      break;
    }
  }
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = total_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void PredictionLog::MergeFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_merge_from_start:tensorflow.serving.PredictionLog)
  if (GOOGLE_PREDICT_FALSE(&from == this)) {
    ::google::protobuf::internal::MergeFromFail(__FILE__, __LINE__);
  }
  const PredictionLog* source = 
      ::google::protobuf::internal::DynamicCastToGenerated<const PredictionLog>(
          &from);
  if (source == NULL) {
  // @@protoc_insertion_point(generalized_merge_from_cast_fail:tensorflow.serving.PredictionLog)
    ::google::protobuf::internal::ReflectionOps::Merge(from, this);
  } else {
  // @@protoc_insertion_point(generalized_merge_from_cast_success:tensorflow.serving.PredictionLog)
    MergeFrom(*source);
  }
}

void PredictionLog::MergeFrom(const PredictionLog& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:tensorflow.serving.PredictionLog)
  if (GOOGLE_PREDICT_FALSE(&from == this)) {
    ::google::protobuf::internal::MergeFromFail(__FILE__, __LINE__);
  }
  switch (from.log_type_case()) {
    case kClassifyLog: {
      mutable_classify_log()->::tensorflow::serving::ClassifyLog::MergeFrom(from.classify_log());
      break;
    }
    case kRegressLog: {
      mutable_regress_log()->::tensorflow::serving::RegressLog::MergeFrom(from.regress_log());
      break;
    }
    case kPredictLog: {
      mutable_predict_log()->::tensorflow::serving::PredictLog::MergeFrom(from.predict_log());
      break;
    }
    case kMultiInferenceLog: {
      mutable_multi_inference_log()->::tensorflow::serving::MultiInferenceLog::MergeFrom(from.multi_inference_log());
      break;
    }
    case kSessionRunLog: {
      mutable_session_run_log()->::tensorflow::serving::SessionRunLog::MergeFrom(from.session_run_log());
      break;
    }
    case LOG_TYPE_NOT_SET: {
      break;
    }
  }
  if (from.has_log_metadata()) {
    mutable_log_metadata()->::tensorflow::serving::LogMetadata::MergeFrom(from.log_metadata());
  }
}

void PredictionLog::CopyFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_copy_from_start:tensorflow.serving.PredictionLog)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

void PredictionLog::CopyFrom(const PredictionLog& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:tensorflow.serving.PredictionLog)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool PredictionLog::IsInitialized() const {

  return true;
}

void PredictionLog::Swap(PredictionLog* other) {
  if (other == this) return;
  if (GetArenaNoVirtual() == other->GetArenaNoVirtual()) {
    InternalSwap(other);
  } else {
    PredictionLog temp;
    temp.MergeFrom(*this);
    CopyFrom(*other);
    other->CopyFrom(temp);
  }
}
void PredictionLog::UnsafeArenaSwap(PredictionLog* other) {
  if (other == this) return;
  GOOGLE_DCHECK(GetArenaNoVirtual() == other->GetArenaNoVirtual());
  InternalSwap(other);
}
void PredictionLog::InternalSwap(PredictionLog* other) {
  std::swap(log_metadata_, other->log_metadata_);
  std::swap(log_type_, other->log_type_);
  std::swap(_oneof_case_[0], other->_oneof_case_[0]);
  _internal_metadata_.Swap(&other->_internal_metadata_);
  std::swap(_cached_size_, other->_cached_size_);
}

::google::protobuf::Metadata PredictionLog::GetMetadata() const {
  protobuf_AssignDescriptorsOnce();
  ::google::protobuf::Metadata metadata;
  metadata.descriptor = PredictionLog_descriptor_;
  metadata.reflection = PredictionLog_reflection_;
  return metadata;
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// PredictionLog

// optional .tensorflow.serving.LogMetadata log_metadata = 1;
bool PredictionLog::has_log_metadata() const {
  return !_is_default_instance_ && log_metadata_ != NULL;
}
void PredictionLog::clear_log_metadata() {
  if (GetArenaNoVirtual() == NULL && log_metadata_ != NULL) delete log_metadata_;
  log_metadata_ = NULL;
}
const ::tensorflow::serving::LogMetadata& PredictionLog::log_metadata() const {
  // @@protoc_insertion_point(field_get:tensorflow.serving.PredictionLog.log_metadata)
  return log_metadata_ != NULL ? *log_metadata_ : *default_instance_->log_metadata_;
}
::tensorflow::serving::LogMetadata* PredictionLog::mutable_log_metadata() {
  
  if (log_metadata_ == NULL) {
    _slow_mutable_log_metadata();
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.serving.PredictionLog.log_metadata)
  return log_metadata_;
}
::tensorflow::serving::LogMetadata* PredictionLog::release_log_metadata() {
  // @@protoc_insertion_point(field_release:tensorflow.serving.PredictionLog.log_metadata)
  
  if (GetArenaNoVirtual() != NULL) {
    return _slow_release_log_metadata();
  } else {
    ::tensorflow::serving::LogMetadata* temp = log_metadata_;
    log_metadata_ = NULL;
    return temp;
  }
}
 void PredictionLog::set_allocated_log_metadata(::tensorflow::serving::LogMetadata* log_metadata) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete log_metadata_;
  }
  if (log_metadata != NULL) {
    _slow_set_allocated_log_metadata(message_arena, &log_metadata);
  }
  log_metadata_ = log_metadata;
  if (log_metadata) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:tensorflow.serving.PredictionLog.log_metadata)
}

// optional .tensorflow.serving.ClassifyLog classify_log = 2;
bool PredictionLog::has_classify_log() const {
  return log_type_case() == kClassifyLog;
}
void PredictionLog::set_has_classify_log() {
  _oneof_case_[0] = kClassifyLog;
}
void PredictionLog::clear_classify_log() {
  if (has_classify_log()) {
    if (GetArenaNoVirtual() == NULL) {
      delete log_type_.classify_log_;
    }
    clear_has_log_type();
  }
}
 const ::tensorflow::serving::ClassifyLog& PredictionLog::classify_log() const {
  // @@protoc_insertion_point(field_get:tensorflow.serving.PredictionLog.classify_log)
  return has_classify_log()
      ? *log_type_.classify_log_
      : ::tensorflow::serving::ClassifyLog::default_instance();
}
::tensorflow::serving::ClassifyLog* PredictionLog::mutable_classify_log() {
  if (!has_classify_log()) {
    clear_log_type();
    set_has_classify_log();
    log_type_.classify_log_ = 
      ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::ClassifyLog >(
      GetArenaNoVirtual());
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.serving.PredictionLog.classify_log)
  return log_type_.classify_log_;
}
::tensorflow::serving::ClassifyLog* PredictionLog::release_classify_log() {
  // @@protoc_insertion_point(field_release:tensorflow.serving.PredictionLog.classify_log)
  if (has_classify_log()) {
    clear_has_log_type();
    if (GetArenaNoVirtual() != NULL) {
      ::tensorflow::serving::ClassifyLog* temp = new ::tensorflow::serving::ClassifyLog;
      temp->MergeFrom(*log_type_.classify_log_);
      log_type_.classify_log_ = NULL;
      return temp;
    } else {
      ::tensorflow::serving::ClassifyLog* temp = log_type_.classify_log_;
      log_type_.classify_log_ = NULL;
      return temp;
    }
  } else {
    return NULL;
  }
}
void PredictionLog::set_allocated_classify_log(::tensorflow::serving::ClassifyLog* classify_log) {
  clear_log_type();
  if (classify_log) {
    if (GetArenaNoVirtual() != NULL &&
        ::google::protobuf::Arena::GetArena(classify_log) == NULL) {
      GetArenaNoVirtual()->Own(classify_log);
    } else if (GetArenaNoVirtual() !=
               ::google::protobuf::Arena::GetArena(classify_log)) {
      ::tensorflow::serving::ClassifyLog* new_classify_log = 
          ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::ClassifyLog >(
          GetArenaNoVirtual());
      new_classify_log->CopyFrom(*classify_log);
      classify_log = new_classify_log;
    }
    set_has_classify_log();
    log_type_.classify_log_ = classify_log;
  }
  // @@protoc_insertion_point(field_set_allocated:tensorflow.serving.PredictionLog.classify_log)
}
 ::tensorflow::serving::ClassifyLog* PredictionLog::unsafe_arena_release_classify_log() {
  // @@protoc_insertion_point(field_unsafe_arena_release:tensorflow.serving.PredictionLog.classify_log)
  if (has_classify_log()) {
    clear_has_log_type();
    ::tensorflow::serving::ClassifyLog* temp = log_type_.classify_log_;
    log_type_.classify_log_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
 void PredictionLog::unsafe_arena_set_allocated_classify_log(::tensorflow::serving::ClassifyLog* classify_log) {
  clear_log_type();
  if (classify_log) {
    set_has_classify_log();
    log_type_.classify_log_ = classify_log;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:tensorflow.serving.PredictionLog.classify_log)
}

// optional .tensorflow.serving.RegressLog regress_log = 3;
bool PredictionLog::has_regress_log() const {
  return log_type_case() == kRegressLog;
}
void PredictionLog::set_has_regress_log() {
  _oneof_case_[0] = kRegressLog;
}
void PredictionLog::clear_regress_log() {
  if (has_regress_log()) {
    if (GetArenaNoVirtual() == NULL) {
      delete log_type_.regress_log_;
    }
    clear_has_log_type();
  }
}
 const ::tensorflow::serving::RegressLog& PredictionLog::regress_log() const {
  // @@protoc_insertion_point(field_get:tensorflow.serving.PredictionLog.regress_log)
  return has_regress_log()
      ? *log_type_.regress_log_
      : ::tensorflow::serving::RegressLog::default_instance();
}
::tensorflow::serving::RegressLog* PredictionLog::mutable_regress_log() {
  if (!has_regress_log()) {
    clear_log_type();
    set_has_regress_log();
    log_type_.regress_log_ = 
      ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::RegressLog >(
      GetArenaNoVirtual());
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.serving.PredictionLog.regress_log)
  return log_type_.regress_log_;
}
::tensorflow::serving::RegressLog* PredictionLog::release_regress_log() {
  // @@protoc_insertion_point(field_release:tensorflow.serving.PredictionLog.regress_log)
  if (has_regress_log()) {
    clear_has_log_type();
    if (GetArenaNoVirtual() != NULL) {
      ::tensorflow::serving::RegressLog* temp = new ::tensorflow::serving::RegressLog;
      temp->MergeFrom(*log_type_.regress_log_);
      log_type_.regress_log_ = NULL;
      return temp;
    } else {
      ::tensorflow::serving::RegressLog* temp = log_type_.regress_log_;
      log_type_.regress_log_ = NULL;
      return temp;
    }
  } else {
    return NULL;
  }
}
void PredictionLog::set_allocated_regress_log(::tensorflow::serving::RegressLog* regress_log) {
  clear_log_type();
  if (regress_log) {
    if (GetArenaNoVirtual() != NULL &&
        ::google::protobuf::Arena::GetArena(regress_log) == NULL) {
      GetArenaNoVirtual()->Own(regress_log);
    } else if (GetArenaNoVirtual() !=
               ::google::protobuf::Arena::GetArena(regress_log)) {
      ::tensorflow::serving::RegressLog* new_regress_log = 
          ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::RegressLog >(
          GetArenaNoVirtual());
      new_regress_log->CopyFrom(*regress_log);
      regress_log = new_regress_log;
    }
    set_has_regress_log();
    log_type_.regress_log_ = regress_log;
  }
  // @@protoc_insertion_point(field_set_allocated:tensorflow.serving.PredictionLog.regress_log)
}
 ::tensorflow::serving::RegressLog* PredictionLog::unsafe_arena_release_regress_log() {
  // @@protoc_insertion_point(field_unsafe_arena_release:tensorflow.serving.PredictionLog.regress_log)
  if (has_regress_log()) {
    clear_has_log_type();
    ::tensorflow::serving::RegressLog* temp = log_type_.regress_log_;
    log_type_.regress_log_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
 void PredictionLog::unsafe_arena_set_allocated_regress_log(::tensorflow::serving::RegressLog* regress_log) {
  clear_log_type();
  if (regress_log) {
    set_has_regress_log();
    log_type_.regress_log_ = regress_log;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:tensorflow.serving.PredictionLog.regress_log)
}

// optional .tensorflow.serving.PredictLog predict_log = 6;
bool PredictionLog::has_predict_log() const {
  return log_type_case() == kPredictLog;
}
void PredictionLog::set_has_predict_log() {
  _oneof_case_[0] = kPredictLog;
}
void PredictionLog::clear_predict_log() {
  if (has_predict_log()) {
    if (GetArenaNoVirtual() == NULL) {
      delete log_type_.predict_log_;
    }
    clear_has_log_type();
  }
}
 const ::tensorflow::serving::PredictLog& PredictionLog::predict_log() const {
  // @@protoc_insertion_point(field_get:tensorflow.serving.PredictionLog.predict_log)
  return has_predict_log()
      ? *log_type_.predict_log_
      : ::tensorflow::serving::PredictLog::default_instance();
}
::tensorflow::serving::PredictLog* PredictionLog::mutable_predict_log() {
  if (!has_predict_log()) {
    clear_log_type();
    set_has_predict_log();
    log_type_.predict_log_ = 
      ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::PredictLog >(
      GetArenaNoVirtual());
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.serving.PredictionLog.predict_log)
  return log_type_.predict_log_;
}
::tensorflow::serving::PredictLog* PredictionLog::release_predict_log() {
  // @@protoc_insertion_point(field_release:tensorflow.serving.PredictionLog.predict_log)
  if (has_predict_log()) {
    clear_has_log_type();
    if (GetArenaNoVirtual() != NULL) {
      ::tensorflow::serving::PredictLog* temp = new ::tensorflow::serving::PredictLog;
      temp->MergeFrom(*log_type_.predict_log_);
      log_type_.predict_log_ = NULL;
      return temp;
    } else {
      ::tensorflow::serving::PredictLog* temp = log_type_.predict_log_;
      log_type_.predict_log_ = NULL;
      return temp;
    }
  } else {
    return NULL;
  }
}
void PredictionLog::set_allocated_predict_log(::tensorflow::serving::PredictLog* predict_log) {
  clear_log_type();
  if (predict_log) {
    if (GetArenaNoVirtual() != NULL &&
        ::google::protobuf::Arena::GetArena(predict_log) == NULL) {
      GetArenaNoVirtual()->Own(predict_log);
    } else if (GetArenaNoVirtual() !=
               ::google::protobuf::Arena::GetArena(predict_log)) {
      ::tensorflow::serving::PredictLog* new_predict_log = 
          ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::PredictLog >(
          GetArenaNoVirtual());
      new_predict_log->CopyFrom(*predict_log);
      predict_log = new_predict_log;
    }
    set_has_predict_log();
    log_type_.predict_log_ = predict_log;
  }
  // @@protoc_insertion_point(field_set_allocated:tensorflow.serving.PredictionLog.predict_log)
}
 ::tensorflow::serving::PredictLog* PredictionLog::unsafe_arena_release_predict_log() {
  // @@protoc_insertion_point(field_unsafe_arena_release:tensorflow.serving.PredictionLog.predict_log)
  if (has_predict_log()) {
    clear_has_log_type();
    ::tensorflow::serving::PredictLog* temp = log_type_.predict_log_;
    log_type_.predict_log_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
 void PredictionLog::unsafe_arena_set_allocated_predict_log(::tensorflow::serving::PredictLog* predict_log) {
  clear_log_type();
  if (predict_log) {
    set_has_predict_log();
    log_type_.predict_log_ = predict_log;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:tensorflow.serving.PredictionLog.predict_log)
}

// optional .tensorflow.serving.MultiInferenceLog multi_inference_log = 4;
bool PredictionLog::has_multi_inference_log() const {
  return log_type_case() == kMultiInferenceLog;
}
void PredictionLog::set_has_multi_inference_log() {
  _oneof_case_[0] = kMultiInferenceLog;
}
void PredictionLog::clear_multi_inference_log() {
  if (has_multi_inference_log()) {
    if (GetArenaNoVirtual() == NULL) {
      delete log_type_.multi_inference_log_;
    }
    clear_has_log_type();
  }
}
 const ::tensorflow::serving::MultiInferenceLog& PredictionLog::multi_inference_log() const {
  // @@protoc_insertion_point(field_get:tensorflow.serving.PredictionLog.multi_inference_log)
  return has_multi_inference_log()
      ? *log_type_.multi_inference_log_
      : ::tensorflow::serving::MultiInferenceLog::default_instance();
}
::tensorflow::serving::MultiInferenceLog* PredictionLog::mutable_multi_inference_log() {
  if (!has_multi_inference_log()) {
    clear_log_type();
    set_has_multi_inference_log();
    log_type_.multi_inference_log_ = 
      ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::MultiInferenceLog >(
      GetArenaNoVirtual());
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.serving.PredictionLog.multi_inference_log)
  return log_type_.multi_inference_log_;
}
::tensorflow::serving::MultiInferenceLog* PredictionLog::release_multi_inference_log() {
  // @@protoc_insertion_point(field_release:tensorflow.serving.PredictionLog.multi_inference_log)
  if (has_multi_inference_log()) {
    clear_has_log_type();
    if (GetArenaNoVirtual() != NULL) {
      ::tensorflow::serving::MultiInferenceLog* temp = new ::tensorflow::serving::MultiInferenceLog;
      temp->MergeFrom(*log_type_.multi_inference_log_);
      log_type_.multi_inference_log_ = NULL;
      return temp;
    } else {
      ::tensorflow::serving::MultiInferenceLog* temp = log_type_.multi_inference_log_;
      log_type_.multi_inference_log_ = NULL;
      return temp;
    }
  } else {
    return NULL;
  }
}
void PredictionLog::set_allocated_multi_inference_log(::tensorflow::serving::MultiInferenceLog* multi_inference_log) {
  clear_log_type();
  if (multi_inference_log) {
    if (GetArenaNoVirtual() != NULL &&
        ::google::protobuf::Arena::GetArena(multi_inference_log) == NULL) {
      GetArenaNoVirtual()->Own(multi_inference_log);
    } else if (GetArenaNoVirtual() !=
               ::google::protobuf::Arena::GetArena(multi_inference_log)) {
      ::tensorflow::serving::MultiInferenceLog* new_multi_inference_log = 
          ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::MultiInferenceLog >(
          GetArenaNoVirtual());
      new_multi_inference_log->CopyFrom(*multi_inference_log);
      multi_inference_log = new_multi_inference_log;
    }
    set_has_multi_inference_log();
    log_type_.multi_inference_log_ = multi_inference_log;
  }
  // @@protoc_insertion_point(field_set_allocated:tensorflow.serving.PredictionLog.multi_inference_log)
}
 ::tensorflow::serving::MultiInferenceLog* PredictionLog::unsafe_arena_release_multi_inference_log() {
  // @@protoc_insertion_point(field_unsafe_arena_release:tensorflow.serving.PredictionLog.multi_inference_log)
  if (has_multi_inference_log()) {
    clear_has_log_type();
    ::tensorflow::serving::MultiInferenceLog* temp = log_type_.multi_inference_log_;
    log_type_.multi_inference_log_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
 void PredictionLog::unsafe_arena_set_allocated_multi_inference_log(::tensorflow::serving::MultiInferenceLog* multi_inference_log) {
  clear_log_type();
  if (multi_inference_log) {
    set_has_multi_inference_log();
    log_type_.multi_inference_log_ = multi_inference_log;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:tensorflow.serving.PredictionLog.multi_inference_log)
}

// optional .tensorflow.serving.SessionRunLog session_run_log = 5;
bool PredictionLog::has_session_run_log() const {
  return log_type_case() == kSessionRunLog;
}
void PredictionLog::set_has_session_run_log() {
  _oneof_case_[0] = kSessionRunLog;
}
void PredictionLog::clear_session_run_log() {
  if (has_session_run_log()) {
    if (GetArenaNoVirtual() == NULL) {
      delete log_type_.session_run_log_;
    }
    clear_has_log_type();
  }
}
 const ::tensorflow::serving::SessionRunLog& PredictionLog::session_run_log() const {
  // @@protoc_insertion_point(field_get:tensorflow.serving.PredictionLog.session_run_log)
  return has_session_run_log()
      ? *log_type_.session_run_log_
      : ::tensorflow::serving::SessionRunLog::default_instance();
}
::tensorflow::serving::SessionRunLog* PredictionLog::mutable_session_run_log() {
  if (!has_session_run_log()) {
    clear_log_type();
    set_has_session_run_log();
    log_type_.session_run_log_ = 
      ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::SessionRunLog >(
      GetArenaNoVirtual());
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.serving.PredictionLog.session_run_log)
  return log_type_.session_run_log_;
}
::tensorflow::serving::SessionRunLog* PredictionLog::release_session_run_log() {
  // @@protoc_insertion_point(field_release:tensorflow.serving.PredictionLog.session_run_log)
  if (has_session_run_log()) {
    clear_has_log_type();
    if (GetArenaNoVirtual() != NULL) {
      ::tensorflow::serving::SessionRunLog* temp = new ::tensorflow::serving::SessionRunLog;
      temp->MergeFrom(*log_type_.session_run_log_);
      log_type_.session_run_log_ = NULL;
      return temp;
    } else {
      ::tensorflow::serving::SessionRunLog* temp = log_type_.session_run_log_;
      log_type_.session_run_log_ = NULL;
      return temp;
    }
  } else {
    return NULL;
  }
}
void PredictionLog::set_allocated_session_run_log(::tensorflow::serving::SessionRunLog* session_run_log) {
  clear_log_type();
  if (session_run_log) {
    if (GetArenaNoVirtual() != NULL &&
        ::google::protobuf::Arena::GetArena(session_run_log) == NULL) {
      GetArenaNoVirtual()->Own(session_run_log);
    } else if (GetArenaNoVirtual() !=
               ::google::protobuf::Arena::GetArena(session_run_log)) {
      ::tensorflow::serving::SessionRunLog* new_session_run_log = 
          ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::SessionRunLog >(
          GetArenaNoVirtual());
      new_session_run_log->CopyFrom(*session_run_log);
      session_run_log = new_session_run_log;
    }
    set_has_session_run_log();
    log_type_.session_run_log_ = session_run_log;
  }
  // @@protoc_insertion_point(field_set_allocated:tensorflow.serving.PredictionLog.session_run_log)
}
 ::tensorflow::serving::SessionRunLog* PredictionLog::unsafe_arena_release_session_run_log() {
  // @@protoc_insertion_point(field_unsafe_arena_release:tensorflow.serving.PredictionLog.session_run_log)
  if (has_session_run_log()) {
    clear_has_log_type();
    ::tensorflow::serving::SessionRunLog* temp = log_type_.session_run_log_;
    log_type_.session_run_log_ = NULL;
    return temp;
  } else {
    return NULL;
  }
}
 void PredictionLog::unsafe_arena_set_allocated_session_run_log(::tensorflow::serving::SessionRunLog* session_run_log) {
  clear_log_type();
  if (session_run_log) {
    set_has_session_run_log();
    log_type_.session_run_log_ = session_run_log;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:tensorflow.serving.PredictionLog.session_run_log)
}

bool PredictionLog::has_log_type() const {
  return log_type_case() != LOG_TYPE_NOT_SET;
}
void PredictionLog::clear_has_log_type() {
  _oneof_case_[0] = LOG_TYPE_NOT_SET;
}
PredictionLog::LogTypeCase PredictionLog::log_type_case() const {
  return PredictionLog::LogTypeCase(_oneof_case_[0]);
}
#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// @@protoc_insertion_point(namespace_scope)

}  // namespace serving
}  // namespace tensorflow

// @@protoc_insertion_point(global_scope)
