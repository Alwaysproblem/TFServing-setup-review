// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: tensorflow_serving/apis/regression.proto

#define INTERNAL_SUPPRESS_PROTOBUF_FIELD_DEPRECATION
#include "tensorflow_serving/apis/regression.pb.h"

#include <algorithm>

#include <google/protobuf/stubs/common.h>
#include <google/protobuf/stubs/port.h>
#include <google/protobuf/stubs/once.h>
#include <google/protobuf/io/coded_stream.h>
#include <google/protobuf/wire_format_lite_inl.h>
#include <google/protobuf/descriptor.h>
#include <google/protobuf/generated_message_reflection.h>
#include <google/protobuf/reflection_ops.h>
#include <google/protobuf/wire_format.h>
// @@protoc_insertion_point(includes)

namespace tensorflow {
namespace serving {

namespace {

const ::google::protobuf::Descriptor* Regression_descriptor_ = NULL;
const ::google::protobuf::internal::GeneratedMessageReflection*
  Regression_reflection_ = NULL;
const ::google::protobuf::Descriptor* RegressionResult_descriptor_ = NULL;
const ::google::protobuf::internal::GeneratedMessageReflection*
  RegressionResult_reflection_ = NULL;
const ::google::protobuf::Descriptor* RegressionRequest_descriptor_ = NULL;
const ::google::protobuf::internal::GeneratedMessageReflection*
  RegressionRequest_reflection_ = NULL;
const ::google::protobuf::Descriptor* RegressionResponse_descriptor_ = NULL;
const ::google::protobuf::internal::GeneratedMessageReflection*
  RegressionResponse_reflection_ = NULL;

}  // namespace


void protobuf_AssignDesc_tensorflow_5fserving_2fapis_2fregression_2eproto() GOOGLE_ATTRIBUTE_COLD;
void protobuf_AssignDesc_tensorflow_5fserving_2fapis_2fregression_2eproto() {
  protobuf_AddDesc_tensorflow_5fserving_2fapis_2fregression_2eproto();
  const ::google::protobuf::FileDescriptor* file =
    ::google::protobuf::DescriptorPool::generated_pool()->FindFileByName(
      "tensorflow_serving/apis/regression.proto");
  GOOGLE_CHECK(file != NULL);
  Regression_descriptor_ = file->message_type(0);
  static const int Regression_offsets_[1] = {
    GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(Regression, value_),
  };
  Regression_reflection_ =
    ::google::protobuf::internal::GeneratedMessageReflection::NewGeneratedMessageReflection(
      Regression_descriptor_,
      Regression::default_instance_,
      Regression_offsets_,
      -1,
      -1,
      -1,
      sizeof(Regression),
      GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(Regression, _internal_metadata_),
      GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(Regression, _is_default_instance_));
  RegressionResult_descriptor_ = file->message_type(1);
  static const int RegressionResult_offsets_[1] = {
    GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(RegressionResult, regressions_),
  };
  RegressionResult_reflection_ =
    ::google::protobuf::internal::GeneratedMessageReflection::NewGeneratedMessageReflection(
      RegressionResult_descriptor_,
      RegressionResult::default_instance_,
      RegressionResult_offsets_,
      -1,
      -1,
      -1,
      sizeof(RegressionResult),
      GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(RegressionResult, _internal_metadata_),
      GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(RegressionResult, _is_default_instance_));
  RegressionRequest_descriptor_ = file->message_type(2);
  static const int RegressionRequest_offsets_[2] = {
    GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(RegressionRequest, model_spec_),
    GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(RegressionRequest, input_),
  };
  RegressionRequest_reflection_ =
    ::google::protobuf::internal::GeneratedMessageReflection::NewGeneratedMessageReflection(
      RegressionRequest_descriptor_,
      RegressionRequest::default_instance_,
      RegressionRequest_offsets_,
      -1,
      -1,
      -1,
      sizeof(RegressionRequest),
      GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(RegressionRequest, _internal_metadata_),
      GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(RegressionRequest, _is_default_instance_));
  RegressionResponse_descriptor_ = file->message_type(3);
  static const int RegressionResponse_offsets_[2] = {
    GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(RegressionResponse, model_spec_),
    GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(RegressionResponse, result_),
  };
  RegressionResponse_reflection_ =
    ::google::protobuf::internal::GeneratedMessageReflection::NewGeneratedMessageReflection(
      RegressionResponse_descriptor_,
      RegressionResponse::default_instance_,
      RegressionResponse_offsets_,
      -1,
      -1,
      -1,
      sizeof(RegressionResponse),
      GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(RegressionResponse, _internal_metadata_),
      GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(RegressionResponse, _is_default_instance_));
}

namespace {

GOOGLE_PROTOBUF_DECLARE_ONCE(protobuf_AssignDescriptors_once_);
inline void protobuf_AssignDescriptorsOnce() {
  ::google::protobuf::GoogleOnceInit(&protobuf_AssignDescriptors_once_,
                 &protobuf_AssignDesc_tensorflow_5fserving_2fapis_2fregression_2eproto);
}

void protobuf_RegisterTypes(const ::std::string&) GOOGLE_ATTRIBUTE_COLD;
void protobuf_RegisterTypes(const ::std::string&) {
  protobuf_AssignDescriptorsOnce();
  ::google::protobuf::MessageFactory::InternalRegisterGeneratedMessage(
      Regression_descriptor_, &Regression::default_instance());
  ::google::protobuf::MessageFactory::InternalRegisterGeneratedMessage(
      RegressionResult_descriptor_, &RegressionResult::default_instance());
  ::google::protobuf::MessageFactory::InternalRegisterGeneratedMessage(
      RegressionRequest_descriptor_, &RegressionRequest::default_instance());
  ::google::protobuf::MessageFactory::InternalRegisterGeneratedMessage(
      RegressionResponse_descriptor_, &RegressionResponse::default_instance());
}

}  // namespace

void protobuf_ShutdownFile_tensorflow_5fserving_2fapis_2fregression_2eproto() {
  delete Regression::default_instance_;
  delete Regression_reflection_;
  delete RegressionResult::default_instance_;
  delete RegressionResult_reflection_;
  delete RegressionRequest::default_instance_;
  delete RegressionRequest_reflection_;
  delete RegressionResponse::default_instance_;
  delete RegressionResponse_reflection_;
}

void protobuf_AddDesc_tensorflow_5fserving_2fapis_2fregression_2eproto() GOOGLE_ATTRIBUTE_COLD;
void protobuf_AddDesc_tensorflow_5fserving_2fapis_2fregression_2eproto() {
  static bool already_here = false;
  if (already_here) return;
  already_here = true;
  GOOGLE_PROTOBUF_VERIFY_VERSION;

  ::tensorflow::serving::protobuf_AddDesc_tensorflow_5fserving_2fapis_2finput_2eproto();
  ::tensorflow::serving::protobuf_AddDesc_tensorflow_5fserving_2fapis_2fmodel_2eproto();
  ::google::protobuf::DescriptorPool::InternalAddGeneratedFile(
    "\n(tensorflow_serving/apis/regression.pro"
    "to\022\022tensorflow.serving\032#tensorflow_servi"
    "ng/apis/input.proto\032#tensorflow_serving/"
    "apis/model.proto\"\033\n\nRegression\022\r\n\005value\030"
    "\001 \001(\002\"G\n\020RegressionResult\0223\n\013regressions"
    "\030\001 \003(\0132\036.tensorflow.serving.Regression\"p"
    "\n\021RegressionRequest\0221\n\nmodel_spec\030\001 \001(\0132"
    "\035.tensorflow.serving.ModelSpec\022(\n\005input\030"
    "\002 \001(\0132\031.tensorflow.serving.Input\"}\n\022Regr"
    "essionResponse\0221\n\nmodel_spec\030\002 \001(\0132\035.ten"
    "sorflow.serving.ModelSpec\0224\n\006result\030\001 \001("
    "\0132$.tensorflow.serving.RegressionResultB"
    "\003\370\001\001b\006proto3", 492);
  ::google::protobuf::MessageFactory::InternalRegisterGeneratedFile(
    "tensorflow_serving/apis/regression.proto", &protobuf_RegisterTypes);
  Regression::default_instance_ = new Regression();
  RegressionResult::default_instance_ = new RegressionResult();
  RegressionRequest::default_instance_ = new RegressionRequest();
  RegressionResponse::default_instance_ = new RegressionResponse();
  Regression::default_instance_->InitAsDefaultInstance();
  RegressionResult::default_instance_->InitAsDefaultInstance();
  RegressionRequest::default_instance_->InitAsDefaultInstance();
  RegressionResponse::default_instance_->InitAsDefaultInstance();
  ::google::protobuf::internal::OnShutdown(&protobuf_ShutdownFile_tensorflow_5fserving_2fapis_2fregression_2eproto);
}

// Force AddDescriptors() to be called at static initialization time.
struct StaticDescriptorInitializer_tensorflow_5fserving_2fapis_2fregression_2eproto {
  StaticDescriptorInitializer_tensorflow_5fserving_2fapis_2fregression_2eproto() {
    protobuf_AddDesc_tensorflow_5fserving_2fapis_2fregression_2eproto();
  }
} static_descriptor_initializer_tensorflow_5fserving_2fapis_2fregression_2eproto_;

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int Regression::kValueFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

Regression::Regression()
  : ::google::protobuf::Message(), _internal_metadata_(NULL) {
  SharedCtor();
  // @@protoc_insertion_point(constructor:tensorflow.serving.Regression)
}

Regression::Regression(::google::protobuf::Arena* arena)
  : ::google::protobuf::Message(),
  _internal_metadata_(arena) {
  SharedCtor();
  RegisterArenaDtor(arena);
  // @@protoc_insertion_point(arena_constructor:tensorflow.serving.Regression)
}

void Regression::InitAsDefaultInstance() {
  _is_default_instance_ = true;
}

Regression::Regression(const Regression& from)
  : ::google::protobuf::Message(),
    _internal_metadata_(NULL) {
  SharedCtor();
  MergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:tensorflow.serving.Regression)
}

void Regression::SharedCtor() {
    _is_default_instance_ = false;
  _cached_size_ = 0;
  value_ = 0;
}

Regression::~Regression() {
  // @@protoc_insertion_point(destructor:tensorflow.serving.Regression)
  SharedDtor();
}

void Regression::SharedDtor() {
  if (GetArenaNoVirtual() != NULL) {
    return;
  }

  if (this != default_instance_) {
  }
}

void Regression::ArenaDtor(void* object) {
  Regression* _this = reinterpret_cast< Regression* >(object);
  (void)_this;
}
void Regression::RegisterArenaDtor(::google::protobuf::Arena* arena) {
}
void Regression::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ::google::protobuf::Descriptor* Regression::descriptor() {
  protobuf_AssignDescriptorsOnce();
  return Regression_descriptor_;
}

const Regression& Regression::default_instance() {
  if (default_instance_ == NULL) protobuf_AddDesc_tensorflow_5fserving_2fapis_2fregression_2eproto();
  return *default_instance_;
}

Regression* Regression::default_instance_ = NULL;

Regression* Regression::New(::google::protobuf::Arena* arena) const {
  return ::google::protobuf::Arena::CreateMessage<Regression>(arena);
}

void Regression::Clear() {
// @@protoc_insertion_point(message_clear_start:tensorflow.serving.Regression)
  value_ = 0;
}

bool Regression::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:tensorflow.serving.Regression)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // optional float value = 1;
      case 1: {
        if (tag == 13) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   float, ::google::protobuf::internal::WireFormatLite::TYPE_FLOAT>(
                 input, &value_)));

        } else {
          goto handle_unusual;
        }
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:tensorflow.serving.Regression)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:tensorflow.serving.Regression)
  return false;
#undef DO_
}

void Regression::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:tensorflow.serving.Regression)
  // optional float value = 1;
  if (this->value() != 0) {
    ::google::protobuf::internal::WireFormatLite::WriteFloat(1, this->value(), output);
  }

  // @@protoc_insertion_point(serialize_end:tensorflow.serving.Regression)
}

::google::protobuf::uint8* Regression::InternalSerializeWithCachedSizesToArray(
    bool deterministic, ::google::protobuf::uint8* target) const {
  // @@protoc_insertion_point(serialize_to_array_start:tensorflow.serving.Regression)
  // optional float value = 1;
  if (this->value() != 0) {
    target = ::google::protobuf::internal::WireFormatLite::WriteFloatToArray(1, this->value(), target);
  }

  // @@protoc_insertion_point(serialize_to_array_end:tensorflow.serving.Regression)
  return target;
}

int Regression::ByteSize() const {
// @@protoc_insertion_point(message_byte_size_start:tensorflow.serving.Regression)
  int total_size = 0;

  // optional float value = 1;
  if (this->value() != 0) {
    total_size += 1 + 4;
  }

  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = total_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void Regression::MergeFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_merge_from_start:tensorflow.serving.Regression)
  if (GOOGLE_PREDICT_FALSE(&from == this)) {
    ::google::protobuf::internal::MergeFromFail(__FILE__, __LINE__);
  }
  const Regression* source = 
      ::google::protobuf::internal::DynamicCastToGenerated<const Regression>(
          &from);
  if (source == NULL) {
  // @@protoc_insertion_point(generalized_merge_from_cast_fail:tensorflow.serving.Regression)
    ::google::protobuf::internal::ReflectionOps::Merge(from, this);
  } else {
  // @@protoc_insertion_point(generalized_merge_from_cast_success:tensorflow.serving.Regression)
    MergeFrom(*source);
  }
}

void Regression::MergeFrom(const Regression& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:tensorflow.serving.Regression)
  if (GOOGLE_PREDICT_FALSE(&from == this)) {
    ::google::protobuf::internal::MergeFromFail(__FILE__, __LINE__);
  }
  if (from.value() != 0) {
    set_value(from.value());
  }
}

void Regression::CopyFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_copy_from_start:tensorflow.serving.Regression)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

void Regression::CopyFrom(const Regression& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:tensorflow.serving.Regression)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool Regression::IsInitialized() const {

  return true;
}

void Regression::Swap(Regression* other) {
  if (other == this) return;
  if (GetArenaNoVirtual() == other->GetArenaNoVirtual()) {
    InternalSwap(other);
  } else {
    Regression temp;
    temp.MergeFrom(*this);
    CopyFrom(*other);
    other->CopyFrom(temp);
  }
}
void Regression::UnsafeArenaSwap(Regression* other) {
  if (other == this) return;
  GOOGLE_DCHECK(GetArenaNoVirtual() == other->GetArenaNoVirtual());
  InternalSwap(other);
}
void Regression::InternalSwap(Regression* other) {
  std::swap(value_, other->value_);
  _internal_metadata_.Swap(&other->_internal_metadata_);
  std::swap(_cached_size_, other->_cached_size_);
}

::google::protobuf::Metadata Regression::GetMetadata() const {
  protobuf_AssignDescriptorsOnce();
  ::google::protobuf::Metadata metadata;
  metadata.descriptor = Regression_descriptor_;
  metadata.reflection = Regression_reflection_;
  return metadata;
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// Regression

// optional float value = 1;
void Regression::clear_value() {
  value_ = 0;
}
 float Regression::value() const {
  // @@protoc_insertion_point(field_get:tensorflow.serving.Regression.value)
  return value_;
}
 void Regression::set_value(float value) {
  
  value_ = value;
  // @@protoc_insertion_point(field_set:tensorflow.serving.Regression.value)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int RegressionResult::kRegressionsFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

RegressionResult::RegressionResult()
  : ::google::protobuf::Message(), _internal_metadata_(NULL) {
  SharedCtor();
  // @@protoc_insertion_point(constructor:tensorflow.serving.RegressionResult)
}

RegressionResult::RegressionResult(::google::protobuf::Arena* arena)
  : ::google::protobuf::Message(),
  _internal_metadata_(arena),
  regressions_(arena) {
  SharedCtor();
  RegisterArenaDtor(arena);
  // @@protoc_insertion_point(arena_constructor:tensorflow.serving.RegressionResult)
}

void RegressionResult::InitAsDefaultInstance() {
  _is_default_instance_ = true;
}

RegressionResult::RegressionResult(const RegressionResult& from)
  : ::google::protobuf::Message(),
    _internal_metadata_(NULL) {
  SharedCtor();
  MergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:tensorflow.serving.RegressionResult)
}

void RegressionResult::SharedCtor() {
    _is_default_instance_ = false;
  _cached_size_ = 0;
}

RegressionResult::~RegressionResult() {
  // @@protoc_insertion_point(destructor:tensorflow.serving.RegressionResult)
  SharedDtor();
}

void RegressionResult::SharedDtor() {
  if (GetArenaNoVirtual() != NULL) {
    return;
  }

  if (this != default_instance_) {
  }
}

void RegressionResult::ArenaDtor(void* object) {
  RegressionResult* _this = reinterpret_cast< RegressionResult* >(object);
  (void)_this;
}
void RegressionResult::RegisterArenaDtor(::google::protobuf::Arena* arena) {
}
void RegressionResult::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ::google::protobuf::Descriptor* RegressionResult::descriptor() {
  protobuf_AssignDescriptorsOnce();
  return RegressionResult_descriptor_;
}

const RegressionResult& RegressionResult::default_instance() {
  if (default_instance_ == NULL) protobuf_AddDesc_tensorflow_5fserving_2fapis_2fregression_2eproto();
  return *default_instance_;
}

RegressionResult* RegressionResult::default_instance_ = NULL;

RegressionResult* RegressionResult::New(::google::protobuf::Arena* arena) const {
  return ::google::protobuf::Arena::CreateMessage<RegressionResult>(arena);
}

void RegressionResult::Clear() {
// @@protoc_insertion_point(message_clear_start:tensorflow.serving.RegressionResult)
  regressions_.Clear();
}

bool RegressionResult::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:tensorflow.serving.RegressionResult)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated .tensorflow.serving.Regression regressions = 1;
      case 1: {
        if (tag == 10) {
          DO_(input->IncrementRecursionDepth());
         parse_loop_regressions:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtualNoRecursionDepth(
                input, add_regressions()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(10)) goto parse_loop_regressions;
        input->UnsafeDecrementRecursionDepth();
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:tensorflow.serving.RegressionResult)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:tensorflow.serving.RegressionResult)
  return false;
#undef DO_
}

void RegressionResult::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:tensorflow.serving.RegressionResult)
  // repeated .tensorflow.serving.Regression regressions = 1;
  for (unsigned int i = 0, n = this->regressions_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      1, this->regressions(i), output);
  }

  // @@protoc_insertion_point(serialize_end:tensorflow.serving.RegressionResult)
}

::google::protobuf::uint8* RegressionResult::InternalSerializeWithCachedSizesToArray(
    bool deterministic, ::google::protobuf::uint8* target) const {
  // @@protoc_insertion_point(serialize_to_array_start:tensorflow.serving.RegressionResult)
  // repeated .tensorflow.serving.Regression regressions = 1;
  for (unsigned int i = 0, n = this->regressions_size(); i < n; i++) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageNoVirtualToArray(
        1, this->regressions(i), false, target);
  }

  // @@protoc_insertion_point(serialize_to_array_end:tensorflow.serving.RegressionResult)
  return target;
}

int RegressionResult::ByteSize() const {
// @@protoc_insertion_point(message_byte_size_start:tensorflow.serving.RegressionResult)
  int total_size = 0;

  // repeated .tensorflow.serving.Regression regressions = 1;
  total_size += 1 * this->regressions_size();
  for (int i = 0; i < this->regressions_size(); i++) {
    total_size +=
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        this->regressions(i));
  }

  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = total_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void RegressionResult::MergeFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_merge_from_start:tensorflow.serving.RegressionResult)
  if (GOOGLE_PREDICT_FALSE(&from == this)) {
    ::google::protobuf::internal::MergeFromFail(__FILE__, __LINE__);
  }
  const RegressionResult* source = 
      ::google::protobuf::internal::DynamicCastToGenerated<const RegressionResult>(
          &from);
  if (source == NULL) {
  // @@protoc_insertion_point(generalized_merge_from_cast_fail:tensorflow.serving.RegressionResult)
    ::google::protobuf::internal::ReflectionOps::Merge(from, this);
  } else {
  // @@protoc_insertion_point(generalized_merge_from_cast_success:tensorflow.serving.RegressionResult)
    MergeFrom(*source);
  }
}

void RegressionResult::MergeFrom(const RegressionResult& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:tensorflow.serving.RegressionResult)
  if (GOOGLE_PREDICT_FALSE(&from == this)) {
    ::google::protobuf::internal::MergeFromFail(__FILE__, __LINE__);
  }
  regressions_.MergeFrom(from.regressions_);
}

void RegressionResult::CopyFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_copy_from_start:tensorflow.serving.RegressionResult)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

void RegressionResult::CopyFrom(const RegressionResult& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:tensorflow.serving.RegressionResult)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool RegressionResult::IsInitialized() const {

  return true;
}

void RegressionResult::Swap(RegressionResult* other) {
  if (other == this) return;
  if (GetArenaNoVirtual() == other->GetArenaNoVirtual()) {
    InternalSwap(other);
  } else {
    RegressionResult temp;
    temp.MergeFrom(*this);
    CopyFrom(*other);
    other->CopyFrom(temp);
  }
}
void RegressionResult::UnsafeArenaSwap(RegressionResult* other) {
  if (other == this) return;
  GOOGLE_DCHECK(GetArenaNoVirtual() == other->GetArenaNoVirtual());
  InternalSwap(other);
}
void RegressionResult::InternalSwap(RegressionResult* other) {
  regressions_.UnsafeArenaSwap(&other->regressions_);
  _internal_metadata_.Swap(&other->_internal_metadata_);
  std::swap(_cached_size_, other->_cached_size_);
}

::google::protobuf::Metadata RegressionResult::GetMetadata() const {
  protobuf_AssignDescriptorsOnce();
  ::google::protobuf::Metadata metadata;
  metadata.descriptor = RegressionResult_descriptor_;
  metadata.reflection = RegressionResult_reflection_;
  return metadata;
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// RegressionResult

// repeated .tensorflow.serving.Regression regressions = 1;
int RegressionResult::regressions_size() const {
  return regressions_.size();
}
void RegressionResult::clear_regressions() {
  regressions_.Clear();
}
const ::tensorflow::serving::Regression& RegressionResult::regressions(int index) const {
  // @@protoc_insertion_point(field_get:tensorflow.serving.RegressionResult.regressions)
  return regressions_.Get(index);
}
::tensorflow::serving::Regression* RegressionResult::mutable_regressions(int index) {
  // @@protoc_insertion_point(field_mutable:tensorflow.serving.RegressionResult.regressions)
  return regressions_.Mutable(index);
}
::tensorflow::serving::Regression* RegressionResult::add_regressions() {
  // @@protoc_insertion_point(field_add:tensorflow.serving.RegressionResult.regressions)
  return regressions_.Add();
}
::google::protobuf::RepeatedPtrField< ::tensorflow::serving::Regression >*
RegressionResult::mutable_regressions() {
  // @@protoc_insertion_point(field_mutable_list:tensorflow.serving.RegressionResult.regressions)
  return &regressions_;
}
const ::google::protobuf::RepeatedPtrField< ::tensorflow::serving::Regression >&
RegressionResult::regressions() const {
  // @@protoc_insertion_point(field_list:tensorflow.serving.RegressionResult.regressions)
  return regressions_;
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

void RegressionRequest::_slow_mutable_model_spec() {
  model_spec_ = ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::ModelSpec >(
      GetArenaNoVirtual());
}
::tensorflow::serving::ModelSpec* RegressionRequest::_slow_release_model_spec() {
  if (model_spec_ == NULL) {
    return NULL;
  } else {
    ::tensorflow::serving::ModelSpec* temp = new ::tensorflow::serving::ModelSpec;
    temp->MergeFrom(*model_spec_);
    model_spec_ = NULL;
    return temp;
  }
}
::tensorflow::serving::ModelSpec* RegressionRequest::unsafe_arena_release_model_spec() {
  // @@protoc_insertion_point(field_unsafe_arena_release:tensorflow.serving.RegressionRequest.model_spec)
  
  ::tensorflow::serving::ModelSpec* temp = model_spec_;
  model_spec_ = NULL;
  return temp;
}
void RegressionRequest::_slow_set_allocated_model_spec(
    ::google::protobuf::Arena* message_arena, ::tensorflow::serving::ModelSpec** model_spec) {
    if (message_arena != NULL && 
        ::google::protobuf::Arena::GetArena(*model_spec) == NULL) {
      message_arena->Own(*model_spec);
    } else if (message_arena !=
               ::google::protobuf::Arena::GetArena(*model_spec)) {
      ::tensorflow::serving::ModelSpec* new_model_spec = 
            ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::ModelSpec >(
            message_arena);
      new_model_spec->CopyFrom(**model_spec);
      *model_spec = new_model_spec;
    }
}
void RegressionRequest::unsafe_arena_set_allocated_model_spec(
    ::tensorflow::serving::ModelSpec* model_spec) {
  if (GetArenaNoVirtual() == NULL) {
    delete model_spec_;
  }
  model_spec_ = model_spec;
  if (model_spec) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:tensorflow.serving.RegressionRequest.model_spec)
}
void RegressionRequest::_slow_mutable_input() {
  input_ = ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::Input >(
      GetArenaNoVirtual());
}
::tensorflow::serving::Input* RegressionRequest::_slow_release_input() {
  if (input_ == NULL) {
    return NULL;
  } else {
    ::tensorflow::serving::Input* temp = new ::tensorflow::serving::Input;
    temp->MergeFrom(*input_);
    input_ = NULL;
    return temp;
  }
}
::tensorflow::serving::Input* RegressionRequest::unsafe_arena_release_input() {
  // @@protoc_insertion_point(field_unsafe_arena_release:tensorflow.serving.RegressionRequest.input)
  
  ::tensorflow::serving::Input* temp = input_;
  input_ = NULL;
  return temp;
}
void RegressionRequest::_slow_set_allocated_input(
    ::google::protobuf::Arena* message_arena, ::tensorflow::serving::Input** input) {
    if (message_arena != NULL && 
        ::google::protobuf::Arena::GetArena(*input) == NULL) {
      message_arena->Own(*input);
    } else if (message_arena !=
               ::google::protobuf::Arena::GetArena(*input)) {
      ::tensorflow::serving::Input* new_input = 
            ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::Input >(
            message_arena);
      new_input->CopyFrom(**input);
      *input = new_input;
    }
}
void RegressionRequest::unsafe_arena_set_allocated_input(
    ::tensorflow::serving::Input* input) {
  if (GetArenaNoVirtual() == NULL) {
    delete input_;
  }
  input_ = input;
  if (input) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:tensorflow.serving.RegressionRequest.input)
}
#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int RegressionRequest::kModelSpecFieldNumber;
const int RegressionRequest::kInputFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

RegressionRequest::RegressionRequest()
  : ::google::protobuf::Message(), _internal_metadata_(NULL) {
  SharedCtor();
  // @@protoc_insertion_point(constructor:tensorflow.serving.RegressionRequest)
}

RegressionRequest::RegressionRequest(::google::protobuf::Arena* arena)
  : ::google::protobuf::Message(),
  _internal_metadata_(arena) {
  SharedCtor();
  RegisterArenaDtor(arena);
  // @@protoc_insertion_point(arena_constructor:tensorflow.serving.RegressionRequest)
}

void RegressionRequest::InitAsDefaultInstance() {
  _is_default_instance_ = true;
  model_spec_ = const_cast< ::tensorflow::serving::ModelSpec*>(&::tensorflow::serving::ModelSpec::default_instance());
  input_ = const_cast< ::tensorflow::serving::Input*>(&::tensorflow::serving::Input::default_instance());
}

RegressionRequest::RegressionRequest(const RegressionRequest& from)
  : ::google::protobuf::Message(),
    _internal_metadata_(NULL) {
  SharedCtor();
  MergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:tensorflow.serving.RegressionRequest)
}

void RegressionRequest::SharedCtor() {
    _is_default_instance_ = false;
  _cached_size_ = 0;
  model_spec_ = NULL;
  input_ = NULL;
}

RegressionRequest::~RegressionRequest() {
  // @@protoc_insertion_point(destructor:tensorflow.serving.RegressionRequest)
  SharedDtor();
}

void RegressionRequest::SharedDtor() {
  if (GetArenaNoVirtual() != NULL) {
    return;
  }

  if (this != default_instance_) {
    delete model_spec_;
    delete input_;
  }
}

void RegressionRequest::ArenaDtor(void* object) {
  RegressionRequest* _this = reinterpret_cast< RegressionRequest* >(object);
  (void)_this;
}
void RegressionRequest::RegisterArenaDtor(::google::protobuf::Arena* arena) {
}
void RegressionRequest::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ::google::protobuf::Descriptor* RegressionRequest::descriptor() {
  protobuf_AssignDescriptorsOnce();
  return RegressionRequest_descriptor_;
}

const RegressionRequest& RegressionRequest::default_instance() {
  if (default_instance_ == NULL) protobuf_AddDesc_tensorflow_5fserving_2fapis_2fregression_2eproto();
  return *default_instance_;
}

RegressionRequest* RegressionRequest::default_instance_ = NULL;

RegressionRequest* RegressionRequest::New(::google::protobuf::Arena* arena) const {
  return ::google::protobuf::Arena::CreateMessage<RegressionRequest>(arena);
}

void RegressionRequest::Clear() {
// @@protoc_insertion_point(message_clear_start:tensorflow.serving.RegressionRequest)
  if (GetArenaNoVirtual() == NULL && model_spec_ != NULL) delete model_spec_;
  model_spec_ = NULL;
  if (GetArenaNoVirtual() == NULL && input_ != NULL) delete input_;
  input_ = NULL;
}

bool RegressionRequest::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:tensorflow.serving.RegressionRequest)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // optional .tensorflow.serving.ModelSpec model_spec = 1;
      case 1: {
        if (tag == 10) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_model_spec()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(18)) goto parse_input;
        break;
      }

      // optional .tensorflow.serving.Input input = 2;
      case 2: {
        if (tag == 18) {
         parse_input:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_input()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:tensorflow.serving.RegressionRequest)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:tensorflow.serving.RegressionRequest)
  return false;
#undef DO_
}

void RegressionRequest::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:tensorflow.serving.RegressionRequest)
  // optional .tensorflow.serving.ModelSpec model_spec = 1;
  if (this->has_model_spec()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      1, *this->model_spec_, output);
  }

  // optional .tensorflow.serving.Input input = 2;
  if (this->has_input()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      2, *this->input_, output);
  }

  // @@protoc_insertion_point(serialize_end:tensorflow.serving.RegressionRequest)
}

::google::protobuf::uint8* RegressionRequest::InternalSerializeWithCachedSizesToArray(
    bool deterministic, ::google::protobuf::uint8* target) const {
  // @@protoc_insertion_point(serialize_to_array_start:tensorflow.serving.RegressionRequest)
  // optional .tensorflow.serving.ModelSpec model_spec = 1;
  if (this->has_model_spec()) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageNoVirtualToArray(
        1, *this->model_spec_, false, target);
  }

  // optional .tensorflow.serving.Input input = 2;
  if (this->has_input()) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageNoVirtualToArray(
        2, *this->input_, false, target);
  }

  // @@protoc_insertion_point(serialize_to_array_end:tensorflow.serving.RegressionRequest)
  return target;
}

int RegressionRequest::ByteSize() const {
// @@protoc_insertion_point(message_byte_size_start:tensorflow.serving.RegressionRequest)
  int total_size = 0;

  // optional .tensorflow.serving.ModelSpec model_spec = 1;
  if (this->has_model_spec()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->model_spec_);
  }

  // optional .tensorflow.serving.Input input = 2;
  if (this->has_input()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->input_);
  }

  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = total_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void RegressionRequest::MergeFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_merge_from_start:tensorflow.serving.RegressionRequest)
  if (GOOGLE_PREDICT_FALSE(&from == this)) {
    ::google::protobuf::internal::MergeFromFail(__FILE__, __LINE__);
  }
  const RegressionRequest* source = 
      ::google::protobuf::internal::DynamicCastToGenerated<const RegressionRequest>(
          &from);
  if (source == NULL) {
  // @@protoc_insertion_point(generalized_merge_from_cast_fail:tensorflow.serving.RegressionRequest)
    ::google::protobuf::internal::ReflectionOps::Merge(from, this);
  } else {
  // @@protoc_insertion_point(generalized_merge_from_cast_success:tensorflow.serving.RegressionRequest)
    MergeFrom(*source);
  }
}

void RegressionRequest::MergeFrom(const RegressionRequest& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:tensorflow.serving.RegressionRequest)
  if (GOOGLE_PREDICT_FALSE(&from == this)) {
    ::google::protobuf::internal::MergeFromFail(__FILE__, __LINE__);
  }
  if (from.has_model_spec()) {
    mutable_model_spec()->::tensorflow::serving::ModelSpec::MergeFrom(from.model_spec());
  }
  if (from.has_input()) {
    mutable_input()->::tensorflow::serving::Input::MergeFrom(from.input());
  }
}

void RegressionRequest::CopyFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_copy_from_start:tensorflow.serving.RegressionRequest)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

void RegressionRequest::CopyFrom(const RegressionRequest& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:tensorflow.serving.RegressionRequest)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool RegressionRequest::IsInitialized() const {

  return true;
}

void RegressionRequest::Swap(RegressionRequest* other) {
  if (other == this) return;
  if (GetArenaNoVirtual() == other->GetArenaNoVirtual()) {
    InternalSwap(other);
  } else {
    RegressionRequest temp;
    temp.MergeFrom(*this);
    CopyFrom(*other);
    other->CopyFrom(temp);
  }
}
void RegressionRequest::UnsafeArenaSwap(RegressionRequest* other) {
  if (other == this) return;
  GOOGLE_DCHECK(GetArenaNoVirtual() == other->GetArenaNoVirtual());
  InternalSwap(other);
}
void RegressionRequest::InternalSwap(RegressionRequest* other) {
  std::swap(model_spec_, other->model_spec_);
  std::swap(input_, other->input_);
  _internal_metadata_.Swap(&other->_internal_metadata_);
  std::swap(_cached_size_, other->_cached_size_);
}

::google::protobuf::Metadata RegressionRequest::GetMetadata() const {
  protobuf_AssignDescriptorsOnce();
  ::google::protobuf::Metadata metadata;
  metadata.descriptor = RegressionRequest_descriptor_;
  metadata.reflection = RegressionRequest_reflection_;
  return metadata;
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// RegressionRequest

// optional .tensorflow.serving.ModelSpec model_spec = 1;
bool RegressionRequest::has_model_spec() const {
  return !_is_default_instance_ && model_spec_ != NULL;
}
void RegressionRequest::clear_model_spec() {
  if (GetArenaNoVirtual() == NULL && model_spec_ != NULL) delete model_spec_;
  model_spec_ = NULL;
}
const ::tensorflow::serving::ModelSpec& RegressionRequest::model_spec() const {
  // @@protoc_insertion_point(field_get:tensorflow.serving.RegressionRequest.model_spec)
  return model_spec_ != NULL ? *model_spec_ : *default_instance_->model_spec_;
}
::tensorflow::serving::ModelSpec* RegressionRequest::mutable_model_spec() {
  
  if (model_spec_ == NULL) {
    _slow_mutable_model_spec();
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.serving.RegressionRequest.model_spec)
  return model_spec_;
}
::tensorflow::serving::ModelSpec* RegressionRequest::release_model_spec() {
  // @@protoc_insertion_point(field_release:tensorflow.serving.RegressionRequest.model_spec)
  
  if (GetArenaNoVirtual() != NULL) {
    return _slow_release_model_spec();
  } else {
    ::tensorflow::serving::ModelSpec* temp = model_spec_;
    model_spec_ = NULL;
    return temp;
  }
}
 void RegressionRequest::set_allocated_model_spec(::tensorflow::serving::ModelSpec* model_spec) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete model_spec_;
  }
  if (model_spec != NULL) {
    _slow_set_allocated_model_spec(message_arena, &model_spec);
  }
  model_spec_ = model_spec;
  if (model_spec) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:tensorflow.serving.RegressionRequest.model_spec)
}

// optional .tensorflow.serving.Input input = 2;
bool RegressionRequest::has_input() const {
  return !_is_default_instance_ && input_ != NULL;
}
void RegressionRequest::clear_input() {
  if (GetArenaNoVirtual() == NULL && input_ != NULL) delete input_;
  input_ = NULL;
}
const ::tensorflow::serving::Input& RegressionRequest::input() const {
  // @@protoc_insertion_point(field_get:tensorflow.serving.RegressionRequest.input)
  return input_ != NULL ? *input_ : *default_instance_->input_;
}
::tensorflow::serving::Input* RegressionRequest::mutable_input() {
  
  if (input_ == NULL) {
    _slow_mutable_input();
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.serving.RegressionRequest.input)
  return input_;
}
::tensorflow::serving::Input* RegressionRequest::release_input() {
  // @@protoc_insertion_point(field_release:tensorflow.serving.RegressionRequest.input)
  
  if (GetArenaNoVirtual() != NULL) {
    return _slow_release_input();
  } else {
    ::tensorflow::serving::Input* temp = input_;
    input_ = NULL;
    return temp;
  }
}
 void RegressionRequest::set_allocated_input(::tensorflow::serving::Input* input) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete input_;
  }
  if (input != NULL) {
    _slow_set_allocated_input(message_arena, &input);
  }
  input_ = input;
  if (input) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:tensorflow.serving.RegressionRequest.input)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// ===================================================================

void RegressionResponse::_slow_mutable_model_spec() {
  model_spec_ = ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::ModelSpec >(
      GetArenaNoVirtual());
}
::tensorflow::serving::ModelSpec* RegressionResponse::_slow_release_model_spec() {
  if (model_spec_ == NULL) {
    return NULL;
  } else {
    ::tensorflow::serving::ModelSpec* temp = new ::tensorflow::serving::ModelSpec;
    temp->MergeFrom(*model_spec_);
    model_spec_ = NULL;
    return temp;
  }
}
::tensorflow::serving::ModelSpec* RegressionResponse::unsafe_arena_release_model_spec() {
  // @@protoc_insertion_point(field_unsafe_arena_release:tensorflow.serving.RegressionResponse.model_spec)
  
  ::tensorflow::serving::ModelSpec* temp = model_spec_;
  model_spec_ = NULL;
  return temp;
}
void RegressionResponse::_slow_set_allocated_model_spec(
    ::google::protobuf::Arena* message_arena, ::tensorflow::serving::ModelSpec** model_spec) {
    if (message_arena != NULL && 
        ::google::protobuf::Arena::GetArena(*model_spec) == NULL) {
      message_arena->Own(*model_spec);
    } else if (message_arena !=
               ::google::protobuf::Arena::GetArena(*model_spec)) {
      ::tensorflow::serving::ModelSpec* new_model_spec = 
            ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::ModelSpec >(
            message_arena);
      new_model_spec->CopyFrom(**model_spec);
      *model_spec = new_model_spec;
    }
}
void RegressionResponse::unsafe_arena_set_allocated_model_spec(
    ::tensorflow::serving::ModelSpec* model_spec) {
  if (GetArenaNoVirtual() == NULL) {
    delete model_spec_;
  }
  model_spec_ = model_spec;
  if (model_spec) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:tensorflow.serving.RegressionResponse.model_spec)
}
void RegressionResponse::_slow_mutable_result() {
  result_ = ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::RegressionResult >(
      GetArenaNoVirtual());
}
::tensorflow::serving::RegressionResult* RegressionResponse::_slow_release_result() {
  if (result_ == NULL) {
    return NULL;
  } else {
    ::tensorflow::serving::RegressionResult* temp = new ::tensorflow::serving::RegressionResult;
    temp->MergeFrom(*result_);
    result_ = NULL;
    return temp;
  }
}
::tensorflow::serving::RegressionResult* RegressionResponse::unsafe_arena_release_result() {
  // @@protoc_insertion_point(field_unsafe_arena_release:tensorflow.serving.RegressionResponse.result)
  
  ::tensorflow::serving::RegressionResult* temp = result_;
  result_ = NULL;
  return temp;
}
void RegressionResponse::_slow_set_allocated_result(
    ::google::protobuf::Arena* message_arena, ::tensorflow::serving::RegressionResult** result) {
    if (message_arena != NULL && 
        ::google::protobuf::Arena::GetArena(*result) == NULL) {
      message_arena->Own(*result);
    } else if (message_arena !=
               ::google::protobuf::Arena::GetArena(*result)) {
      ::tensorflow::serving::RegressionResult* new_result = 
            ::google::protobuf::Arena::CreateMessage< ::tensorflow::serving::RegressionResult >(
            message_arena);
      new_result->CopyFrom(**result);
      *result = new_result;
    }
}
void RegressionResponse::unsafe_arena_set_allocated_result(
    ::tensorflow::serving::RegressionResult* result) {
  if (GetArenaNoVirtual() == NULL) {
    delete result_;
  }
  result_ = result;
  if (result) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:tensorflow.serving.RegressionResponse.result)
}
#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int RegressionResponse::kModelSpecFieldNumber;
const int RegressionResponse::kResultFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

RegressionResponse::RegressionResponse()
  : ::google::protobuf::Message(), _internal_metadata_(NULL) {
  SharedCtor();
  // @@protoc_insertion_point(constructor:tensorflow.serving.RegressionResponse)
}

RegressionResponse::RegressionResponse(::google::protobuf::Arena* arena)
  : ::google::protobuf::Message(),
  _internal_metadata_(arena) {
  SharedCtor();
  RegisterArenaDtor(arena);
  // @@protoc_insertion_point(arena_constructor:tensorflow.serving.RegressionResponse)
}

void RegressionResponse::InitAsDefaultInstance() {
  _is_default_instance_ = true;
  model_spec_ = const_cast< ::tensorflow::serving::ModelSpec*>(&::tensorflow::serving::ModelSpec::default_instance());
  result_ = const_cast< ::tensorflow::serving::RegressionResult*>(&::tensorflow::serving::RegressionResult::default_instance());
}

RegressionResponse::RegressionResponse(const RegressionResponse& from)
  : ::google::protobuf::Message(),
    _internal_metadata_(NULL) {
  SharedCtor();
  MergeFrom(from);
  // @@protoc_insertion_point(copy_constructor:tensorflow.serving.RegressionResponse)
}

void RegressionResponse::SharedCtor() {
    _is_default_instance_ = false;
  _cached_size_ = 0;
  model_spec_ = NULL;
  result_ = NULL;
}

RegressionResponse::~RegressionResponse() {
  // @@protoc_insertion_point(destructor:tensorflow.serving.RegressionResponse)
  SharedDtor();
}

void RegressionResponse::SharedDtor() {
  if (GetArenaNoVirtual() != NULL) {
    return;
  }

  if (this != default_instance_) {
    delete model_spec_;
    delete result_;
  }
}

void RegressionResponse::ArenaDtor(void* object) {
  RegressionResponse* _this = reinterpret_cast< RegressionResponse* >(object);
  (void)_this;
}
void RegressionResponse::RegisterArenaDtor(::google::protobuf::Arena* arena) {
}
void RegressionResponse::SetCachedSize(int size) const {
  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
}
const ::google::protobuf::Descriptor* RegressionResponse::descriptor() {
  protobuf_AssignDescriptorsOnce();
  return RegressionResponse_descriptor_;
}

const RegressionResponse& RegressionResponse::default_instance() {
  if (default_instance_ == NULL) protobuf_AddDesc_tensorflow_5fserving_2fapis_2fregression_2eproto();
  return *default_instance_;
}

RegressionResponse* RegressionResponse::default_instance_ = NULL;

RegressionResponse* RegressionResponse::New(::google::protobuf::Arena* arena) const {
  return ::google::protobuf::Arena::CreateMessage<RegressionResponse>(arena);
}

void RegressionResponse::Clear() {
// @@protoc_insertion_point(message_clear_start:tensorflow.serving.RegressionResponse)
  if (GetArenaNoVirtual() == NULL && model_spec_ != NULL) delete model_spec_;
  model_spec_ = NULL;
  if (GetArenaNoVirtual() == NULL && result_ != NULL) delete result_;
  result_ = NULL;
}

bool RegressionResponse::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:tensorflow.serving.RegressionResponse)
  for (;;) {
    ::std::pair< ::google::protobuf::uint32, bool> p = input->ReadTagWithCutoff(127);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // optional .tensorflow.serving.RegressionResult result = 1;
      case 1: {
        if (tag == 10) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_result()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectTag(18)) goto parse_model_spec;
        break;
      }

      // optional .tensorflow.serving.ModelSpec model_spec = 2;
      case 2: {
        if (tag == 18) {
         parse_model_spec:
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessageNoVirtual(
               input, mutable_model_spec()));
        } else {
          goto handle_unusual;
        }
        if (input->ExpectAtEnd()) goto success;
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0 ||
            ::google::protobuf::internal::WireFormatLite::GetTagWireType(tag) ==
            ::google::protobuf::internal::WireFormatLite::WIRETYPE_END_GROUP) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormatLite::SkipField(input, tag));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:tensorflow.serving.RegressionResponse)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:tensorflow.serving.RegressionResponse)
  return false;
#undef DO_
}

void RegressionResponse::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:tensorflow.serving.RegressionResponse)
  // optional .tensorflow.serving.RegressionResult result = 1;
  if (this->has_result()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      1, *this->result_, output);
  }

  // optional .tensorflow.serving.ModelSpec model_spec = 2;
  if (this->has_model_spec()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      2, *this->model_spec_, output);
  }

  // @@protoc_insertion_point(serialize_end:tensorflow.serving.RegressionResponse)
}

::google::protobuf::uint8* RegressionResponse::InternalSerializeWithCachedSizesToArray(
    bool deterministic, ::google::protobuf::uint8* target) const {
  // @@protoc_insertion_point(serialize_to_array_start:tensorflow.serving.RegressionResponse)
  // optional .tensorflow.serving.RegressionResult result = 1;
  if (this->has_result()) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageNoVirtualToArray(
        1, *this->result_, false, target);
  }

  // optional .tensorflow.serving.ModelSpec model_spec = 2;
  if (this->has_model_spec()) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageNoVirtualToArray(
        2, *this->model_spec_, false, target);
  }

  // @@protoc_insertion_point(serialize_to_array_end:tensorflow.serving.RegressionResponse)
  return target;
}

int RegressionResponse::ByteSize() const {
// @@protoc_insertion_point(message_byte_size_start:tensorflow.serving.RegressionResponse)
  int total_size = 0;

  // optional .tensorflow.serving.ModelSpec model_spec = 2;
  if (this->has_model_spec()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->model_spec_);
  }

  // optional .tensorflow.serving.RegressionResult result = 1;
  if (this->has_result()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSizeNoVirtual(
        *this->result_);
  }

  GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
  _cached_size_ = total_size;
  GOOGLE_SAFE_CONCURRENT_WRITES_END();
  return total_size;
}

void RegressionResponse::MergeFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_merge_from_start:tensorflow.serving.RegressionResponse)
  if (GOOGLE_PREDICT_FALSE(&from == this)) {
    ::google::protobuf::internal::MergeFromFail(__FILE__, __LINE__);
  }
  const RegressionResponse* source = 
      ::google::protobuf::internal::DynamicCastToGenerated<const RegressionResponse>(
          &from);
  if (source == NULL) {
  // @@protoc_insertion_point(generalized_merge_from_cast_fail:tensorflow.serving.RegressionResponse)
    ::google::protobuf::internal::ReflectionOps::Merge(from, this);
  } else {
  // @@protoc_insertion_point(generalized_merge_from_cast_success:tensorflow.serving.RegressionResponse)
    MergeFrom(*source);
  }
}

void RegressionResponse::MergeFrom(const RegressionResponse& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:tensorflow.serving.RegressionResponse)
  if (GOOGLE_PREDICT_FALSE(&from == this)) {
    ::google::protobuf::internal::MergeFromFail(__FILE__, __LINE__);
  }
  if (from.has_model_spec()) {
    mutable_model_spec()->::tensorflow::serving::ModelSpec::MergeFrom(from.model_spec());
  }
  if (from.has_result()) {
    mutable_result()->::tensorflow::serving::RegressionResult::MergeFrom(from.result());
  }
}

void RegressionResponse::CopyFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_copy_from_start:tensorflow.serving.RegressionResponse)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

void RegressionResponse::CopyFrom(const RegressionResponse& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:tensorflow.serving.RegressionResponse)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool RegressionResponse::IsInitialized() const {

  return true;
}

void RegressionResponse::Swap(RegressionResponse* other) {
  if (other == this) return;
  if (GetArenaNoVirtual() == other->GetArenaNoVirtual()) {
    InternalSwap(other);
  } else {
    RegressionResponse temp;
    temp.MergeFrom(*this);
    CopyFrom(*other);
    other->CopyFrom(temp);
  }
}
void RegressionResponse::UnsafeArenaSwap(RegressionResponse* other) {
  if (other == this) return;
  GOOGLE_DCHECK(GetArenaNoVirtual() == other->GetArenaNoVirtual());
  InternalSwap(other);
}
void RegressionResponse::InternalSwap(RegressionResponse* other) {
  std::swap(model_spec_, other->model_spec_);
  std::swap(result_, other->result_);
  _internal_metadata_.Swap(&other->_internal_metadata_);
  std::swap(_cached_size_, other->_cached_size_);
}

::google::protobuf::Metadata RegressionResponse::GetMetadata() const {
  protobuf_AssignDescriptorsOnce();
  ::google::protobuf::Metadata metadata;
  metadata.descriptor = RegressionResponse_descriptor_;
  metadata.reflection = RegressionResponse_reflection_;
  return metadata;
}

#if PROTOBUF_INLINE_NOT_IN_HEADERS
// RegressionResponse

// optional .tensorflow.serving.ModelSpec model_spec = 2;
bool RegressionResponse::has_model_spec() const {
  return !_is_default_instance_ && model_spec_ != NULL;
}
void RegressionResponse::clear_model_spec() {
  if (GetArenaNoVirtual() == NULL && model_spec_ != NULL) delete model_spec_;
  model_spec_ = NULL;
}
const ::tensorflow::serving::ModelSpec& RegressionResponse::model_spec() const {
  // @@protoc_insertion_point(field_get:tensorflow.serving.RegressionResponse.model_spec)
  return model_spec_ != NULL ? *model_spec_ : *default_instance_->model_spec_;
}
::tensorflow::serving::ModelSpec* RegressionResponse::mutable_model_spec() {
  
  if (model_spec_ == NULL) {
    _slow_mutable_model_spec();
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.serving.RegressionResponse.model_spec)
  return model_spec_;
}
::tensorflow::serving::ModelSpec* RegressionResponse::release_model_spec() {
  // @@protoc_insertion_point(field_release:tensorflow.serving.RegressionResponse.model_spec)
  
  if (GetArenaNoVirtual() != NULL) {
    return _slow_release_model_spec();
  } else {
    ::tensorflow::serving::ModelSpec* temp = model_spec_;
    model_spec_ = NULL;
    return temp;
  }
}
 void RegressionResponse::set_allocated_model_spec(::tensorflow::serving::ModelSpec* model_spec) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete model_spec_;
  }
  if (model_spec != NULL) {
    _slow_set_allocated_model_spec(message_arena, &model_spec);
  }
  model_spec_ = model_spec;
  if (model_spec) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:tensorflow.serving.RegressionResponse.model_spec)
}

// optional .tensorflow.serving.RegressionResult result = 1;
bool RegressionResponse::has_result() const {
  return !_is_default_instance_ && result_ != NULL;
}
void RegressionResponse::clear_result() {
  if (GetArenaNoVirtual() == NULL && result_ != NULL) delete result_;
  result_ = NULL;
}
const ::tensorflow::serving::RegressionResult& RegressionResponse::result() const {
  // @@protoc_insertion_point(field_get:tensorflow.serving.RegressionResponse.result)
  return result_ != NULL ? *result_ : *default_instance_->result_;
}
::tensorflow::serving::RegressionResult* RegressionResponse::mutable_result() {
  
  if (result_ == NULL) {
    _slow_mutable_result();
  }
  // @@protoc_insertion_point(field_mutable:tensorflow.serving.RegressionResponse.result)
  return result_;
}
::tensorflow::serving::RegressionResult* RegressionResponse::release_result() {
  // @@protoc_insertion_point(field_release:tensorflow.serving.RegressionResponse.result)
  
  if (GetArenaNoVirtual() != NULL) {
    return _slow_release_result();
  } else {
    ::tensorflow::serving::RegressionResult* temp = result_;
    result_ = NULL;
    return temp;
  }
}
 void RegressionResponse::set_allocated_result(::tensorflow::serving::RegressionResult* result) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete result_;
  }
  if (result != NULL) {
    _slow_set_allocated_result(message_arena, &result);
  }
  result_ = result;
  if (result) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_set_allocated:tensorflow.serving.RegressionResponse.result)
}

#endif  // PROTOBUF_INLINE_NOT_IN_HEADERS

// @@protoc_insertion_point(namespace_scope)

}  // namespace serving
}  // namespace tensorflow

// @@protoc_insertion_point(global_scope)
